[{"content":"PC gamers often use Discord for its superior audio quality compared to in-game chat options. However, cross-platform gaming presents a challenge, limiting users to less desirable options like in-game chat or the Xbox Companion app.\nThis guide explains how to set up audio forwarding from an Xbox party directly to Discord, and vice versa, using a Windows 10 virtual machine (VM) on Unraid. This allows for the use of Discord\u0026rsquo;s high-quality audio even when gaming with friends on Xbox.\nUsing a Virtual Audio Cable The Virtual Audio Cable (VAC) is the key to this setup. While it requires a $30 purchase, it offers a trial period with a voice reminder every 30 minutes.\nSetting it up involves creating two cables and clicking \u0026ldquo;Set,\u0026rdquo; resulting in two audio interfaces for routing audio within your operating system.\nConfiguring the Xbox Console Companion App While Microsoft is promoting their newer Xbox PC app, this setup utilizes the Xbox Console Companion App. It involves creating a dedicated Xbox Live user for hosting parties with your Xbox friends.\nWithin the app settings, set the Speaker source to \u0026ldquo;Line 1 (Virtual Audio Cable)\u0026rdquo; and the Microphone to \u0026ldquo;Line 2 (Virtual Audio Cable).\u0026rdquo;\nSetting up Discord Create a new Discord user specifically for this setup and add it to your desired server.\nIn the Voice \u0026amp; Video settings, set the Input device to \u0026ldquo;Line 1 (Virtual Audio Cable)\u0026rdquo; and the Output device to \u0026ldquo;Line 2 (Virtual Audio Cable).\u0026rdquo;\nConnecting with Friends Join your Discord server with the new user. Have your Xbox friends join the party hosted by your dedicated Xbox Live user. You should now be able to communicate with your Xbox friends through Discord.\n","date":"2024-07-11T18:12:23Z","image":"https://raw.githubusercontent.com/Nexus-Security/image-1/main/xbox2discord-how-to-forward-audio-from-xbox-live-to-discord.jpg","permalink":"https://Nexus-Security.github.io/forwarding-audio-from-xbox-live-to-discord-a-guide/","title":"Forwarding audio from Xbox Live to Discord: A guide"},{"content":" Introduction Logging and traffic monitoring are crucial for information security. Stored logs that can be searched offer valuable information about security breaches.\nFor instance, logs from individual computers can reveal an attacker\u0026rsquo;s movements within a network. Authentication logs from Active Directory can provide a more detailed view of these movements and establish a timeline. Firewall logs can show an attacker\u0026rsquo;s initial point of entry or the first time a specific command or control domain was used. NetFlow logs offer information about a user\u0026rsquo;s interactions with other devices on the network.\nThese examples illustrate how logging is essential for providing critical data, especially during or after a security incident.\nThis is not intended to be an exhaustive analysis of logging and monitoring. Further research into the theoretical and practical reasons for logging in a secure systems environment is strongly recommended.\nGraylog Graylog is a robust open-source log management system that is ideal for homelab environments and beyond. It is a powerful tool suitable even for enterprise environments and has enterprise-friendly options available.\nHowever, for our needs on Unraid, the open-source version will be more than enough.\nGraylog has been challenging to set up on Unraid in the past. Docker Compose simplifies the installation process, so we\u0026rsquo;ll use it on Unraid to overcome these challenges and get Graylog working.\nPrepping Unraid This setup requires the Docker Compose Manager plugin. You can install it from the Community Applications page in Unraid. Just search for \u0026ldquo;Docker Compose Manager\u0026rdquo; and install the package from \u0026ldquo;dcflachs\u0026rdquo;. More information can be found in the Docker Compose forum post. See the image below for reference.\n_Note: As of this writing, Unraid\u0026rsquo;s Docker Compose implementation is in beta. However, I have been using it without issues since its initial release. It\u0026rsquo;s always recommended to have backups and parity set up before using beta software. _ Once installed, we can proceed to the next steps.\nConfiguring a Docker Compose Stack on Unraid This step requires Docker to be enabled on your Unraid server. Refer to Unraid\u0026rsquo;s Wiki for information on Docker in Unraid and how to enable it.\nOnce enabled, navigate to \u0026ldquo;Docker\u0026rdquo; in the menu. If the installation was successful, you should see a \u0026ldquo;Compose\u0026rdquo; section at the bottom of the page.\nClick ADD NEW STACK and name the stack something clear like \u0026ldquo;graylog\u0026rdquo;, as shown in the image. Next, click the gear icon next to the \u0026ldquo;graylog\u0026rdquo; stack and select EDIT STACK. A large text edit box should appear, displaying something similar to \u0026ldquo;Editing /boot/config/plugins/compose.manager/projects/graylog/compose.yml\u0026rdquo;. Copy and paste the following code into the box:\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 version: \u0026#34;3.8\u0026#34; services: mongodb: image: \u0026#34;mongo:5.0\u0026#34; volumes: - \u0026#34;/your/unraid/path/here:/data/db\u0026#34; restart: \u0026#34;on-failure\u0026#34; elasticsearch: environment: ES_JAVA_OPTS: \u0026#34;-Xms1g -Xmx1g -Dlog4j2.formatMsgNoLookups=true\u0026#34; bootstrap.memory_lock: \u0026#34;true\u0026#34; discovery.type: \u0026#34;single-node\u0026#34; http.host: \u0026#34;0.0.0.0\u0026#34; action.auto_create_index: \u0026#34;false\u0026#34; image: \u0026#34;docker.elastic.co/elasticsearch/elasticsearch-oss:7.10.2\u0026#34; ulimits: memlock: hard: -1 soft: -1 volumes: - \u0026#34;/your/unraid/path/here:/usr/share/elasticsearch/data\u0026#34; restart: \u0026#34;on-failure\u0026#34; graylog: image: \u0026#34;graylog/graylog:4.2\u0026#34; depends_on: elasticsearch: condition: \u0026#34;service_started\u0026#34; mongodb: condition: \u0026#34;service_started\u0026#34; entrypoint: \u0026#34;/usr/bin/tini -- wait-for-it elasticsearch:9200 -- /docker-entrypoint.sh\u0026#34; environment: GRAYLOG_TIMEZONE: \u0026#34;America/New_York\u0026#34; TZ: \u0026#34;America/New_York\u0026#34; GRAYLOG_NODE_ID_FILE: \u0026#34;/usr/share/graylog/data/config/node-id\u0026#34; GRAYLOG_PASSWORD_SECRET: \u0026#34;enter secret here\u0026#34; GRAYLOG_ROOT_PASSWORD_SHA2: \u0026#34;enter SHA2 of secret here\u0026#34; GRAYLOG_HTTP_BIND_ADDRESS: \u0026#34;0.0.0.0:9000\u0026#34; GRAYLOG_HTTP_EXTERNAL_URI: \u0026#34;http://localhost:9000/\u0026#34; GRAYLOG_ELASTICSEARCH_HOSTS: \u0026#34;http://elasticsearch:9200\u0026#34; GRAYLOG_MONGODB_URI: \u0026#34;mongodb://mongodb:27017/graylog\u0026#34; ports: - \u0026#34;5044:5044/tcp\u0026#34; # Beats - \u0026#34;5140:5140/udp\u0026#34; # Syslog - \u0026#34;5140:5140/tcp\u0026#34; # Syslog - \u0026#34;5555:5555/tcp\u0026#34; # RAW TCP - \u0026#34;5555:5555/udp\u0026#34; # RAW TCP - \u0026#34;9000:9000/tcp\u0026#34; # Server API - \u0026#34;12201:12201/tcp\u0026#34; # GELF TCP - \u0026#34;12201:12201/udp\u0026#34; # GELF UDP - \u0026#34;10000:10000/tcp\u0026#34; # Custom TCP port - \u0026#34;10000:10000/udp\u0026#34; # Custom UDP port - \u0026#34;13301:13301/tcp\u0026#34; # Forwarder data - \u0026#34;13302:13302/tcp\u0026#34; # Forwarder config volumes: - \u0026#34;/your/unraid/path/here:/usr/share/graylog/data/data\u0026#34; - \u0026#34;/your/unraid/path/here:/usr/share/graylog/data/journal\u0026#34; restart: \u0026#34;on-failure\u0026#34; volumes: mongodb_data: es_data: graylog_data: graylog_journal: Your Unraid interface should resemble the image: Before saving, you\u0026rsquo;ll need to modify Lines 7, 23, 59, and 60. Replace /your/unraid/path/here with the desired directory path for Graylog files. For example, I used /mnt/disk7/graylog/mongodb_data for Line 7, /mnt/disk7/graylog/es_data for Line 23, /mnt/disk7/graylog/graylog_data for Line 59, and /mnt/disk7/graylog/graylog_journal for Line 60.\nRemember to keep the \u0026quot; : \u0026quot; and everything after it unchanged. For example, Line 7 in my case would be \u0026quot;/mnt/disk7/graylog/mongodb_data:/data/db\u0026quot;.\nI opted to store Graylog data on a dedicated disk, separate from my cache drive, for long-term storage and to avoid consuming Docker image or cache storage space.\nImportant Note: DO NOT place all the folders within the same subdirectory. I used /mnt/disk7/graylog as the root directory for Graylog data, with each volume in the Docker Compose file having its own subdirectory. Putting all Graylog volumes in the same subdirectory is not recommended and may cause issues, potentially preventing the application from running.\nNext, before saving, generate a password and its SHA2 hash. These values will be used on Lines 38 and 39 respectively. I utilized another Docker container on Unraid called CyberChef (mpepping/cyberchef). You can also use Unraid\u0026rsquo;s terminal. Click the Terminal icon in the top right corner of Unraid\u0026rsquo;s menu. Copy and paste the following command:\n1 echo -n \u0026#34;Enter Password: \u0026#34; \u0026amp;\u0026amp; head -1 \u0026lt;/dev/stdin | tr -d \u0026#39;\\n\u0026#39; | sha256sum | cut -d\u0026#34; \u0026#34; -f1 Press Enter. You will be prompted to enter your desired Graylog password (for Line 38). After entering your password, press Enter again. The terminal should output the corresponding SHA2 hash. Copy this hash and paste it into Line 39 of the docker-compose.yml file on Unraid. See the image below for reference. Finally, click SAVE CHANGES.\nStart the Stack - Compose Up Now, click COMPOSE UP under Commands associated with your graylog stack. This is equivalent to the standard Docker Compose command:\n1 docker-compose up -d A popup titled \u0026ldquo;Stack graylog Up\u0026rdquo; should appear, and you should see Docker Compose searching for and downloading any missing images. Once all necessary files are downloaded, the final output should look like this:\n1 2 3 Container graylog-mongodb-1 Running Container graylog-elasticsearch-1 Running Container graylog-graylog-1 Running This output confirms that your Graylog containers, including network settings and dependencies, have been successfully launched on your Unraid server.\nYou should now see three new containers at the top of the running containers list in Unraid\u0026rsquo;s Docker menu. See the image below for reference: You can now access your Graylog instance by going to your Unraid server\u0026rsquo;s IP address on port 9000 (e.g., 172.16.1.10:9000). Use the username admin and yourpassword from Line 38 to log in (not your SHA2 hash). Bonus: Send Other Unraid Container Syslogs to Graylog To send syslog data to Graylog, enable Syslog TCP incoming connections in Graylog. Go to System / Inputs in the Graylog interface and select Inputs. Choose Syslog TCP from the Select input dropdown. Click Launch new input.\nGive your input a descriptive title and change the incoming port from 514 to 5140. Leave the remaining settings at their defaults unless you have specific requirements. Scroll down and click Save. Back in Unraid, navigate to a running container that you wish to configure or add a new container as needed.\nUnder \u0026ldquo;Extra Parameters\u0026rdquo;, add the following code snippet to the Extra Parameters field, placing it after any existing parameters.\n1 --log-driver=syslog --log-opt tag=\u0026#34;varken\u0026#34; --log-opt syslog-address=tcp://YOUR_GRAYLOG_IP:5140 Replace varken with a descriptive name for the container you are configuring and YOUR_GRAYLOG_IP with the IP address you used to access Graylog.\nScroll to the bottom of the container configuration and click SAVE. If your network settings are correct and communication is successful, your container should start, and its syslog data should be forwarded to your Graylog instance! ","date":"2024-06-20T08:20:10Z","image":"https://raw.githubusercontent.com/Nexus-Security/image-1/main/graylog-and-unraid.png","permalink":"https://Nexus-Security.github.io/launch-graylog-using-docker-compose-on-unraid/","title":"Launch Graylog using Docker Compose on Unraid"},{"content":"Overview This post explains how to set up a Matrix Server using Docker-Compose, turn it into a hub for encrypted notifications, and make it accessible on the internet via a Cloudflare tunnel using Cloudflare\u0026rsquo;s Zero Trust platform.\nShort Cybersecurity Note Matrix Servers can enhance homelab or business cybersecurity, particularly through encryption. Encryption is crucial for anyone studying for the CISSP exam.\nEncryption safeguards confidentiality, one of the core principles of the CIA triad. It transforms readable data (plaintext) into unreadable ciphertext using an algorithm and a secret key. Decryption, or making the data readable again, requires this key, ensuring that only authorized individuals with the key can access the original information. Encryption is a cornerstone of security for email, online banking, file sharing, and more.\nWhat is a Matrix Server? Matrix is a communication protocol designed for decentralized and federated chat and real-time communication applications. It offers a secure and versatile messaging platform that allows users to connect irrespective of their app, device, or service provider.\nMatrix servers form a decentralized network, enabling communication between users on different servers regardless of their chosen applications or providers. This interconnected system is called \u0026ldquo;federation.\u0026rdquo;\nSecurity is paramount: Matrix servers employ end-to-end encryption, encrypting messages from sender to recipient. This means that not even server administrators can read the content, guaranteeing user privacy.\nUsers can choose between self-hosting their Matrix server or utilizing a third-party provider. The Matrix protocol enjoys broad support from various applications, including Element, the author\u0026rsquo;s preferred choice.\nWhat is a \u0026ldquo;Cloudflared\u0026rdquo; Tunnel? A Cloudflared tunnel, a Cloudflare-developed command-line tool, allows users to securely expose HTTP and HTTPS services running on their servers to the internet. It achieves this by establishing a secure, encrypted tunnel between the server and Cloudflare\u0026rsquo;s edge network, which acts as a reverse proxy.\nUsers can access services on private networks through a public URL, eliminating the need to directly expose the service to the internet. This approach enhances security by channeling all access through the encrypted tunnel, shielding the service from direct exposure.\nWhat is Cloudflare\u0026rsquo;s Zero Trust Interface? Cloudflare\u0026rsquo;s Zero Trust solution is a security framework that assumes all network traffic, users, and devices are untrustworthy. It revolves around a Zero Trust Access (ZTA) interface, providing a secure method for managing and controlling access to organizational resources.\nThe ZTA interface ensures that only verified and authorized users, devices, and applications can interact with resources. It accomplishes this through stringent authentication and customizable access control policies tailored to each organization\u0026rsquo;s specific requirements.\nThe ZTA interface operates by establishing a secure, encrypted, and authenticated tunnel between the user\u0026rsquo;s device and the organization\u0026rsquo;s network, permitting traffic flow only between approved endpoints. This safeguards data even in the event of a compromised device or intercepted traffic.\nFurthermore, Cloudflare\u0026rsquo;s ZTA interface boasts sophisticated monitoring and analytics tools. These tools empower administrators to track user behavior, identify potential threats, and respond promptly to security incidents, minimizing potential damage.\nSetting Up the Environment This project utilizes an overprovisioned (4 core/4GB) Ubuntu 22.10 virtual machine running on an ESXi server. The Matrix server stack comprises several Docker containers: Synapse (the Matrix server), Postgres DB (for enhanced performance), Element (the user interface), Webhooks (for receiving webhooks), Maubot (for GitHub and RSS integration), another Postgres DB (for Maubot), Nginx Proxy Manager (and its database), and Postfix (for sending emails through GMail SMTP).\nWhile only the Synapse container and a user interface like Element are strictly necessary, the author includes additional containers for a customized setup. Note that these configurations are specific to the author\u0026rsquo;s stack and may require adjustments for different setups.\nRequirements An Ubuntu Host Docker and Docker-Compose A domain name A Cloudflare instance A working Cloudflare tunnel For convenience, all required files and a secrets directory are available on GitHub. Download both the files and the secrets directory to your Ubuntu host.\nRemember to generate and use secure passwords throughout the tutorial. A password manager is highly recommended.\nModify the docker-compose.yml The downloaded docker-compose.yml file requires modifications. Initially, the Maubot, Maubot Postgres, and Webhooks containers are commented out and are best configured after the Matrix server is up and running.\nStart by editing the Synapse container section. Replace YOURSERVERNAME (Lines 16 and 30) with your domain name and YOURSECRETPASSWORD (Line 22) with a secure database password.\nNext, in the postgres section below synapse, replace YOURSECRETPASSWORD (Line 39) with the same database password used above.\nIf using Maubot, replace YOURSECRETPASSWORD (Line 108) with a new secure password, which you\u0026rsquo;ll need later for the maubot/config.yaml file.\nFor those using postfix for email relay, adjust Lines 120-124 to match your SMTP relay service. The author uses GMail with an application-specific password. If you don\u0026rsquo;t have an SMTP relay, consider a containerized SMTP server.\nThe Webhooks container configuration can be skipped for now as it requires channel IDs.\nComment out or remove any unwanted containers, save the docker-compose.yml file, and exit.\nModify the element-config.json Replace YOURDOMAIN.COM on Lines 4 and 5 with your domain name, save, and exit.\nModify the homeserver.yaml This file is the most intricate and allows for extensive customization. This guide will cover the author\u0026rsquo;s specific settings.\nReplace all instances of YOURDOMAIN.COM with your actual domain name.\nOn Line 33, substitute SAME-PASSWORD-DEFINED-IN-DOCKERCOMPOSE with the password used for the postgres database in the docker-compose.yml file.\nGenerate strong, unique passwords for Lines 61, 70, and 71.\nIf not using postfix, adjust Lines 88-103 according to your email configuration.\nSave and close the file.\nModify secrets/matrix_admin_pass.txt Within the secrets directory, open matrix_admin_pass.txt and replace YOUR_VERY_SECURE_PASSWORD with a strong, secure password.\nModify secrets/synapse_secrets.env In the synapse_secrets.env file within the secrets directory, replace YOURDOMAIN.COM with your domain name.\nModify maubot/config.yaml In maubot/config.yaml, replace THEPOSTGRESDB_PASSWORD_FROM_DOCKERCOMPOSE on Line 6 with the Maubot Postgres database password you saved earlier.\nSet strong passwords for Lines 63, 72, and 75.\nReplace YOURSERVERNAME.TLD on Lines 53 and 71 with your domain. Save and close the file.\nStart the Matrix Server Navigate to the directory with the docker-compose.yml file in your terminal and run:\n1 docker-compose up -d Configure Cloudflare The Cloudflared Tunnel This section assumes you have a working Cloudflare tunnel. In the Cloudflare Zero Trust Dashboard, navigate to Access \u0026gt; Tunnels. Choose your tunnel and click Configure.\nAdd your matrix.yourdomain.tld to the tunnel. When adding the hostname, use \u0026ldquo;matrix\u0026rdquo; for the subdomain and select your domain. Input the following path:\n1 (/_matrix/|/_synapse/client/).* Choose HTTP as the Service Type and enter your Matrix Server\u0026rsquo;s local IP address with port 8008 (e.g., 10.99.100.50:8008). Save the hostname.\nDuplicate this hostname entry, but this time, use your root domain (e.g., yourdomain.tld) without any subdomain.\nRepeat the process for element.yourdomain.tld, and if applicable, maubot.yourdomain.tld and webhooks.yourdomain.tld. Note that these don\u0026rsquo;t require the specific path mentioned earlier. The correct ports for each service are:\n1 2 3 4 5 Element: http://IP:8080 Maubot: http://IP:29316 Webhooks: http://IP:80000 The Cloudflare Worker To enable federation with the Cloudflared tunnel, a Cloudflare Worker is necessary to serve the required .well-known file.\nIn the Cloudflare Dashboard, go to Workers and create a new service (e.g., matrix). Use the quick edit function and replace the default code with the following, replacing matrix.yourdomain.tld with your domain:\n1 2 3 4 5 6 7 addEventListener(\u0026#39;fetch\u0026#39;, event =\u0026gt; { event.respondWith(handleRequest(event.request)) }) async function handleRequest(request) { return new Response(`{\u0026#34;m.server\u0026#34;:\u0026#34;matrix.yourdomain.tld:443\u0026#34;}`, {status: 200}) } Save and deploy the worker. Return to the worker overview, go to \u0026ldquo;Custom Domains,\u0026rdquo; and add a custom domain like federation.yourdomain.tld.\nUnder \u0026ldquo;Routes,\u0026rdquo; add the following routes, making sure they point to the zone corresponding to your domain:\n1 2 3 4 yourdomain.tld/.well-known/matrix/server yourdomain.tld/.well-known/matrix/server\\ *yourdomain.tld/.well-known/matrix/* yourdomain.tld/.well-known/matrix/matrix (Optional) Nginx Proxy Manager (NPM) Configuration NPM configuration is optional if using split-brain DNS for local access. If you already have NPM, configure matrix.yourdomain.tld and element.yourdomain.tld to point to it.\nRunning a New NPM Instance If you need NPM, uncomment the relevant lines in the docker-compose.yml and restart the stack with docker-compose up -d. Refer to the author\u0026rsquo;s previous post on NPM for setup instructions.\nConfiguring NPM In NPM, create proxy hosts for matrix.yourdomain.tld, element.yourdomain.tld, and yourdomain.tld. Add hosts for webhooks.yourdomain.tld and maubot.yourdomain.tld if you are using those services.\nFor each host, configure it to forward to the corresponding local IP address and port:\n1 2 3 4 5 6 7 8 9 yourdomain.tld : http://IP:8008 matrix.yourdomain.tld : http://IP:8008 element.yourdomain.tld : http://IP:8080 maubot.yourdomain.tld : http://IP:29316 webhooks.yourdomain.tld : http://IP:80000 For SSL, request a new certificate from Let\u0026rsquo;s Encrypt.\nAccessing Your Server You should now be able to access your Matrix server by visiting element.yourdomain.tld. Click Edit next to the auto-filled server name and input matrix.yourdomain.tld.\nFinally, sign in with the admin username (default: admin) from the docker-compose.yml and the corresponding password from /synapse-secrets/matrix_admin_pass.txt.\nYou should now have access to your new Matrix server through Element.\n","date":"2024-06-18T19:43:50Z","image":"https://raw.githubusercontent.com/Nexus-Security/image-1/main/splash.jpg","permalink":"https://Nexus-Security.github.io/my-matrix-server-secured-with-docker-compose/","title":"My Matrix Server secured with Docker-Compose"},{"content":" Introduction My previous post detailed my migration from WordPress to Hugo for static site generation. It covered local hosting within a Docker container and using a reverse proxy for public access. This post will guide you through hosting your Hugo site on Cloudflare\u0026rsquo;s free platform. This approach eliminates the need for self-hosting, port exposure, and SSL certificate management. Cloudflare automates these tasks and even offers automatic site builds from Git repositories.\nThis guide assumes you have a Git repository containing your functional Hugo static site and a Cloudflare account with DNS management capabilities for your custom domain. Technically, a custom domain isn\u0026rsquo;t mandatory as Cloudflare can generate a development URL.\nCloudflare Setup The setup process is straightforward, particularly compared to setting up Cloudflare tunnels.\nWithin your Cloudflare account, navigate to Pages in the left panel and choose Create a project. Select the option to Connect to Git. Log in to your preferred Git host (e.g., GitHub or GitLab) through Cloudflare and grant the requested account permissions. Refresh the page to see your Git user and repositories. Choose the relevant repository and proceed.\nSelect the primary branch (usually \u0026ldquo;master\u0026rdquo; or \u0026ldquo;main\u0026rdquo;) of your repository in the Production branch dropdown. Under Build settings, select Hugo as the Framework preset. This automatically populates the Build command and Build output directory fields. Click Save and Deploy; your site deployment will begin immediately.\nA failed build might indicate that Cloudflare is using an outdated Hugo version. You can rectify this by going back to Pages, selecting your site, and clicking Settings. Navigate to Environment variables, click Edit variables, and choose Add variable. Input HUGO_VERSION as the Variable name and specify the desired version (e.g., 0.101.0) as the Value. Save your changes.\nReturn to the Deployments tab, locate your failed deployment, and select Retry deployment.\nOnce the deployment succeeds, your site will be accessible at the provided link under Deployment.\nTo set up a custom domain, go to Custom domains at the top, click Set up a custom domain, and enter your desired root domain or subdomain. You\u0026rsquo;ll be prompted to add a DNS record directing your domain to the Cloudflare deployment URL. Cloudflare might offer to add this record automatically. After adding the DNS record, verify it and allow time for propagation.\n","date":"2024-06-15T16:53:15Z","image":"https://raw.githubusercontent.com/Nexus-Security/image-1/main/logo.png","permalink":"https://Nexus-Security.github.io/host-your-static-website-with-cloudflare/","title":"Host your static website with Cloudflare"},{"content":"I recently upgraded two domain controllers to Windows Server 2022. The primary controller was running Windows Server 2016, while the secondary controller was running Windows Server 2019. Both in-place upgrades were successful.\nBoth servers were running Hyper-V, Bitlocker, and Active Directory Domain Services. The 2019 server also hosted an SMTP server. However, Microsoft has deprecated the SMTP stack and its management tools, including those within IIS. It\u0026rsquo;s highly recommended to research any specific functionalities used on your Windows Servers before upgrading, as other software or tools may also be deprecated.\nInstructions Back up your server. I opted for Veeam. Locate the Windows Server Setup media for the desired Windows Server upgrade image and run setup.exe. Click Yes to initiate the setup. For devices with internet connectivity, choose the Download updates, drivers and optional features [recommended] option and then select Next. Allow the setup to verify your device configuration and then click Next. Depending on how you obtained your Windows Server media (e.g., Retail, Volume License, OEM, ODM, etc.), you might be asked for a licensing key to proceed. Choose the Windows Server edition you wish to install and select Next. Select Accept to agree to the terms of your licensing agreement, which varies based on your distribution channel (e.g., Retail, Volume License, OEM, ODM, etc.). Select Keep personal files and apps to perform an in-place upgrade and then click Next. Once the Setup analyzes your device, you\u0026rsquo;ll see a prompt to start the upgrade by selecting Install.\nThe in-place upgrade will begin, displaying the Upgrading Windows screen with a progress bar. Your server will restart after the upgrade is finished.\nConclusion Upgrading production machines crucial to your team or business can feel risky, but Microsoft has streamlined this particular upgrade path. As always, back up your server before starting any upgrade, though you likely won\u0026rsquo;t need it.\nCongratulations if you\u0026rsquo;ve reached this point and everything is functioning correctly! I hope this tutorial proves helpful in your quest to upgrade your servers to Windows Server 2022.\n","date":"2024-04-28T19:03:18Z","image":"https://raw.githubusercontent.com/Nexus-Security/image-1/main/how-to-do-an-in-place-upgrade-to-windows-server-2022.jpg","permalink":"https://Nexus-Security.github.io/upgrading-to-windows-server-2022-a-step-by-step-guide/","title":"Upgrading to Windows Server 2022: A Step-by-Step Guide"},{"content":" Introduction OSSIM is a robust, open-source security information and event management (SIEM) operating system. It\u0026rsquo;s the open-source counterpart to AlienVault, a product offered by AT\u0026amp;T.\nI\u0026rsquo;ve used OSSIM professionally and currently utilize it for vulnerability scanning, asset management, and security alerts. Setting up and installing OSSIM can be quite complex, and troubleshooting resources are limited. I\u0026rsquo;ve picked up some useful tips through trial and error over the years.\nInterestingly, OSSIM leans towards a Windows-centric approach, despite being built on Debian. Consequently, deploying the host intrusion detection system (HIDS) on Linux endpoints can be a bit challenging.\nThis post will guide you through installing the OSSEC HIDS necessary for an OSSIM deployment and configuring Linux syslog forwarding to OSSIM. If your Linux distribution isn\u0026rsquo;t supported on the OSSEC site, I\u0026rsquo;ll also explain how to compile the HIDS agent and configure it via the command-line interface.\nAdding an Agent to OSSIM This guide assumes you have a working OSSIM installation (either virtualized or on bare metal) and can access the web interface. If you\u0026rsquo;re not at this stage yet, I suggest referring to AT\u0026amp;T\u0026rsquo;s official documentation. I previously wrote a post on installing OSSIM as a VM on Unraid if you prefer that platform.\nStart by navigating to your OSSIM web interface. Go to ENIORONMENT \u0026gt; DETECTION \u0026gt; AGENTS and click ADD AGENT. Locate your Linux endpoint under the \u0026ldquo;All Assets\u0026rdquo; section (this requires you to have previously added assets to OSSIM, either manually or through a scan). Upon selecting your endpoint, the IP/CIDR field should auto-populate with its IP address. Click SAVE. See the screenshot below: Note: If your endpoint doesn\u0026rsquo;t have a static IP address, I highly recommend assigning one if feasible. While HIDS might function with a dynamic IP (by checking the \u0026ldquo;This is a dynamic IP address (DHCP)\u0026rdquo; checkbox), static IPs tend to be more reliable and have caused fewer issues in my experience.\nAfter saving, you should see your endpoint listed as shown in the image below. The status in my image is already \u0026ldquo;Active,\u0026rdquo; which is how yours will appear once we complete this tutorial. Take note of this location, as you\u0026rsquo;ll need the Base64 encoded key (accessible by clicking the key icon next to your endpoint) later on for your Linux OS.\nInstalling OSSEC HIDS The installation process has been simplified and is now fairly straightforward for most Linux distributions. Looking at the OSSEC.net downloads, we can see that all supported Linux distributions can utilize the same wget command to add the source. The necessary wget command, followed by the appropriate package manager commands for agent installation, is provided below. Note that the commands for Ubuntu and Debian are identical, as are those for Centos/RedHat, Fedora, and Amazon Linux. The configuration process will also be the same across all distributions.\nUbuntu \u0026amp; Debian For Ubuntu and Debian, execute the following to install the agent:\n1 2 3 wget -q -O - https://updates.atomicorp.com/installers/atomic | sudo bash sudo apt update \u0026amp;\u0026amp; sudo apt install ossec-hids-agent Centos/RedHat, Fedora, \u0026amp; Amazon Linux For Centos/RedHat, Fedora, \u0026amp; Amazon Linux, run the following to install the agent:\n1 2 3 wget -q -O - https://updates.atomicorp.com/installers/atomic | sudo bash sudo yum install ossec-hids-agent From Source Compiling from source is another installation method. I\u0026rsquo;ve personally only tested this on Ubuntu and two Raspberry Pis. Your results may differ, and you\u0026rsquo;ll need to install the following:\n1 2 # Install build tools with APT - works on Distros with APT sudo apt install build-essential libevent-dev libpcre2-dev libz-dev libssl-dev The next set of steps covers the actual installation:\n1 2 3 4 wget https://github.com/ossec/ossec-hids/archive/3.6.0.tar.gz /tmp/3.6.0.tar.gz sudo tar xzf /tmp/3.6.0.tar.gz cd /tmp/ossec-hids-3.6.0/ sudo ./install.sh Upon running the final script, you\u0026rsquo;ll be prompted to answer a few straightforward questions. I\u0026rsquo;ve listed my responses, corresponding to the numbered steps in the script, below:\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 # (en/br/cn/de/el/es/fr/hu/it/jp/nl/pl/ru/sr/tr) [en]: en OR leave blank # 1- What kind of installation do you want (server, agent, local, hybrid or help)? a OR agent # 2- Setting up the installation environment. - Choose where to install the OSSEC HIDS [/var/ossec]: leave blank # 3- Configuring the OSSEC HIDS. 3.1 - What\u0026#39;s the IP Address or hostname of the OSSEC HIDS server?: YOUR OSSIM SERVER IP (e.g., 172.16.7.100) # 3.2- Do you want to run the integrity check daemon? (y/n) [y]: y OR blank # 3.3- Do you want to run the rootkit detection engine? (y/n) [y]: y OR blank # 3.4 - Do you want to enable active response? (y/n) [y]: y y OR blank Once the script finishes execution, assuming no errors are encountered, the HIDS agent should be installed on your endpoint. While troubleshooting this process is beyond the scope of this post, I\u0026rsquo;ve generally had success with it.\nConfiguring OSSEC HIDS As previously mentioned, the configuration process for OSSEC HIDS should be consistent across different Linux distributions. All screenshots provided here are from SSH connections to Ubuntu endpoints using RoyalTS.\nEditing OSSEC.conf First, we\u0026rsquo;ll need to modify the ossec.conf file, which is an XML configuration file generated during installation. Open /var/ossec/etc/ossec.conf with your preferred text editor on your Linux endpoint. (Replace nano in the command below with your editor of choice)\n1 sudo nano /var/ossec/etc/ossec.conf Within the file, locate the client section, typically at the beginning. Replace the placeholder with your OSSIM server\u0026rsquo;s IP address.\n1 2 3 4 #example \u0026lt;client\u0026gt; \u0026lt;server-ip\u0026gt;172.16.7.100\u0026lt;/server-ip\u0026gt; \u0026lt;/client\u0026gt; Still in the same file, insert a new localfile node under the ossec_config node. You\u0026rsquo;ll notice several existing localfile nodes; I suggest adding the new one near the bottom for easy identification later on if needed. Use the following code:\n1 2 3 4 \u0026lt;localfile\u0026gt; \u0026lt;log_format\u0026gt;syslog\u0026lt;/log_format\u0026gt; \u0026lt;location\u0026gt;/var/log/vault.log\u0026lt;/location\u0026gt; \u0026lt;/localfile\u0026gt; Save your changes and close the file (CTRL+X if using nano).\nConnect Endpoint to OSSIM Server Once the agent is configured, we can connect it to the OSSIM server. Go back to the OSSIM web interface and return to where you initially added the endpoint. Click the Extract Key icon next to your endpoint and copy the key, ensuring you capture all characters, including any trailing \u0026ldquo;=\u0026rdquo;, and avoid copying extra spaces.\nOn the endpoint, execute the following command:\n1 sudo /var/ossec/bin/manage_agents Follow the prompts to import the key. Press \u0026ldquo;I\u0026rdquo; to initiate the import, then paste the key you copied from the server. Confirm with \u0026ldquo;y\u0026rdquo;, then press \u0026ldquo;Enter\u0026rdquo; and \u0026ldquo;Q\u0026rdquo; to quit. This sequence should resemble the following: To finalize the configuration, run:\n1 sudo /var/ossec/bin/ossec-control restart Your endpoint is now successfully configured.\nWrapping Up Congratulations on reaching this point! Give it a few minutes, and your endpoint will establish a connection with your server. This connection will be reflected in the web interface, with the endpoint\u0026rsquo;s status changing to \u0026ldquo;Active,\u0026rdquo; as shown in the screenshot earlier. You\u0026rsquo;ll also start seeing events populated on the dashboard and in the event logs.\nOSSIM is a powerful yet potentially complex tool. If you\u0026rsquo;re new to OSSIM, I recommend conducting thorough research on SIEM tools in general and specifically on OSSIM configuration best practices. While it might be overkill for a home lab, it\u0026rsquo;s a valuable tool for any security professional to be familiar with.\n","date":"2024-03-26T14:00:41Z","image":"https://raw.githubusercontent.com/Nexus-Security/image-1/main/ossim.png","permalink":"https://Nexus-Security.github.io/alienvault-ossim-allows-you-to-effectively-manage-linux-logs/","title":"AlienVault OSSIM allows you to effectively manage Linux logs"},{"content":"This guide explains how to route Docker container traffic through a VPN on Unraid for enhanced privacy, anonymity, and security.\nWhile a VPN provider that prioritizes privacy is recommended, this guide does not endorse illegal activities or piracy.\nSetting up the VPN Container Begin by choosing a suitable VPN container from Unraid, such as binhex/arch-privoxyvpn, and set it up. Consider your specific needs and VPN provider when making this decision.\nEstablishing a Dedicated VPN Network Create a Docker network specifically for VPN traffic using the Unraid terminal. For instance, if your VPN container is named \u0026ldquo;privoxyvpn,\u0026rdquo; the command would be:\n1 docker network create container:privoxyvpn Routing Container Traffic Through the VPN To route a container\u0026rsquo;s traffic through the VPN, modify its network settings. Select the newly created VPN network (e.g., privoxyvpn) under \u0026ldquo;Network Type.\u0026rdquo;\nImportantly, remove all existing port mappings for the container to ensure traffic flows through the VPN. Note these port mappings for later use.\nConfiguring VPN Container for GUI Access To access the container\u0026rsquo;s interface, configure port forwarding on the VPN container. Add the desired ports to the \u0026ldquo;VPN_OUTPUT_PORTS\u0026rdquo; option (or its equivalent for your VPN container).\nMap each port to the corresponding container port. For example, to access a container\u0026rsquo;s port 8080 on port 8082, set \u0026ldquo;Container Port\u0026rdquo; to 8080 and \u0026ldquo;Host Port\u0026rdquo; to 8082.\nConclusion After saving the settings, your container\u0026rsquo;s traffic should be routed through the VPN, enhancing its privacy and security. You should now have secure access to your container\u0026rsquo;s interface.\n","date":"2024-02-15T05:22:54Z","image":"https://raw.githubusercontent.com/Nexus-Security/image-1/main/how-to-route-any-docker-container-through-vpn-in-unraid.png","permalink":"https://Nexus-Security.github.io/how-to-direct-any-docker-container-through-vpn-on-unraid/","title":"How to Direct Any Docker Container Through VPN on Unraid"},{"content":"Reverse proxies are frequently used to direct client traffic to a designated server. Unlike a forward proxy, a reverse proxy operates on the server-side, receiving client requests and forwarding them to the appropriate server. This guide outlines setting up a reverse proxy using Docker, along with port forwarding configuration, and demonstrates traffic forwarding to various servers on a network. Specifically, the example focuses on a WordPress setup, although the applications are vast.\nThe tutorial uses Nginx Reverse Proxy managed through its GUI frontend, Nginx Proxy Manager. While managing Nginx without a GUI is possible, this guide aims for a simpler approach. A dedicated device or VM to host the reverse proxy is recommended. The guide assumes docker-compose is installed on a Ubuntu system for demonstration, utilizing a bare-metal machine behind a Unifi Dream Machine Pro (UDMP). Users of different operating systems can adapt the commands accordingly. For those seeking guidance on self-hosted WordPress, a separate tutorial is linked for reference. It\u0026rsquo;s suggested to run the WordPress site and the reverse proxy on separate devices or VMs for clarity.\nThe installation process is simplified with a provided \u0026lsquo;docker-compose.yml\u0026rsquo; file available for download from a GitHub repository. Building the containers is achieved by executing the \u0026lsquo;docker-compose up -d\u0026rsquo; command. Upon successful execution, both Nginx Proxy Manager and MariaDB should be running.\nConfiguring Nginx Proxy Manager involves accessing its frontend through a web browser and changing the default login credentials for security. The guide provides a step-by-step example of configuring a proxy host for a WordPress site, including forwarding ports 80 and 443 to the reverse proxy host. Screenshots illustrating these configurations within the Unifi Controller interface are included.\nAdding a new proxy host within Nginx Proxy Manager involves specifying domain names, selecting the appropriate scheme (https is recommended), entering the target server IP and port, and enabling caching, exploit blocking, and websocket support. Additional configurations, such as adding a custom location for \u0026lsquo;/wp-admin\u0026rsquo; and obtaining an SSL certificate using Let\u0026rsquo;s Encrypt are also covered.\nThe guide concludes by highlighting the flexibility of this reverse proxy setup, using the example of adding SSL to a UDMP connection. The author encourages readers to reach out with questions and welcomes feedback on practical applications of the setup.\n","date":"2024-01-13T10:56:20Z","image":"https://raw.githubusercontent.com/Nexus-Security/image-1/main/run-a-reverse-proxy-using-docker.png","permalink":"https://Nexus-Security.github.io/running-a-reverse-proxy-using-docker-made-simple/","title":"Running a reverse proxy using Docker made simple"},{"content":"Introduction In today\u0026rsquo;s fast-paced business world, digital transformation is no longer optional—it\u0026rsquo;s crucial for success. Companies globally are utilizing technology to optimize their operations, improve customer satisfaction, and maintain a competitive edge. In this digital era, IT services are essential partners, guiding and supporting organizations throughout their transformation journey. Let\u0026rsquo;s delve into the pivotal role of IT services in this essential shift.\n1. Technology Evaluation and Roadmap IT service providers conduct a thorough assessment of your current IT infrastructure, systems, and processes. This evaluation helps them identify weaknesses, uncover hidden opportunities, and create a clear roadmap for your digital transformation journey.\n2. Choosing the Right Technology Selecting the most suitable technologies and solutions is paramount for successful transformation. IT service providers utilize their knowledge to align your technology investments with your core business goals, encompassing hardware, software, cloud solutions, and cutting-edge technologies like the Internet of Things (IoT) and Artificial Intelligence (AI).\n3. Implementation and Seamless Integration Seamless integration of new technologies into your existing framework is critical. IT service providers manage this complex task, ensuring that these advancements coexist smoothly with your legacy systems, preventing interruptions and minimizing downtime.\n4. Tailored Development Off-the-shelf solutions aren\u0026rsquo;t always sufficient. IT service providers step in to create customized software applications that address your specific requirements. These bespoke solutions enable you to overcome unique obstacles and distinguish yourself in the digital landscape.\n5. Transitioning to the Cloud Cloud migration is a central aspect of many organizations\u0026rsquo; digital transformation strategies. IT services are instrumental in planning and executing these migrations, ensuring scalability, cost efficiency, and robust security within the cloud environment.\n6. Data Management and Insights Data is the cornerstone of the digital age. IT service providers assist you in efficiently managing data, from storage to retrieval, and harnessing the power of analytics. This empowers data-driven decision-making and enhances customer satisfaction.\n7. Cybersecurity Fortification Security is non-negotiable in digital transformation. IT service providers strengthen your digital environment with robust cybersecurity measures, regular security checks, and strict compliance with industry standards and regulations.\n8. Automation for Efficiency Optimizing operations through automation is a key characteristic of successful transformation. IT service providers pinpoint areas for process automation, reducing manual tasks, increasing efficiency, and delivering significant cost reductions.\n9. Mobile and Omnichannel Solutions Improving customer engagement across multiple platforms is a strategic necessity. IT service providers facilitate the development of mobile apps and omnichannel solutions, ensuring a smooth and engaging user experience across all touchpoints.\n10. Empowering Your Workforce New technologies demand new proficiencies. IT service providers offer training and assistance to ensure your employees are adept at using these tools efficiently, ultimately increasing productivity and adoption rates.\n11. Building for Scalability Digital transformation is an ongoing process. IT service providers design solutions with scalability in mind, allowing your organization to adapt to changing business needs and accommodate growth seamlessly.\n12. Proactive Monitoring and Support Downtime is detrimental to productivity. IT service providers offer continuous monitoring and support to guarantee your systems are operational and performing optimally, minimizing disruptions and maintaining high service availability.\n13. Feedback-Driven Improvement The digital world is constantly changing, making feedback essential. IT service providers collect user feedback to drive continuous enhancements in digital solutions, keeping your organization agile and competitive.\n14. Compliance and Risk Mitigation Navigating regulatory complexities is challenging. IT service providers assist in ensuring compliance with regulations and industry standards, mitigating legal and reputational risks associated with digital operations.\n15. Strategic Partnership IT service providers go beyond technical assistance to become strategic advisors. They help you align technology investments with long-term growth strategies, ensuring every digital step contributes to your overall business goals.\nConclusion In conclusion, IT services are the backbone of successful digital transformation. They offer technical expertise, strategic insights, and the ability to effectively implement and sustain initiatives. By leveraging IT services, organizations can confidently navigate the intricate world of digital transformation and unlock the full potential of technology to drive innovation and growth.\n","date":"2023-12-02T12:05:25Z","image":"https://raw.githubusercontent.com/Nexus-Security/image-1/main/main.jpg","permalink":"https://Nexus-Security.github.io/the-crucial-role-of-it-services-in-unlocking-digital-transformation/","title":"The Crucial Role of IT Services in Unlocking Digital Transformation"},{"content":"Introduction When I started posting on this site, I wanted to use Docker for containerization. My only experience was with WordPress, so I aimed to self-host it with Docker.\nMy first public post explains how I used docker-compose to deploy my blog using containers for WordPress and Traefik.\nWhy I Left WordPress While the WordPress container setup worked well initially, as my readership grew, so did the amount of spam. It became unmanageable despite numerous spam blocker plugins.\nMy reasons for leaving are common: Speed, Security, and Spam.\nSpeed WordPress processes requests through PHP, querying the database for files to render HTML for each request, which is then sent to your browser. Hugo, however, uses pre-rendered HTML, resulting in a significant speed difference.\nSecurity While WordPress is generally safe, I learned about a cross-site scripting (XSS) vulnerability in a reputable plugin I was using. Although I removed it, I was concerned enough to research WordPress plugin vulnerabilities.\nSpam Even with various spam blocker plugins, I received over 1000 spam messages daily, making it hard to find genuine reader interactions.\nMigrating from WordPress to Markdown First, export your WordPress site to XML, including all content. WordPress\u0026rsquo;s official documentation explains this process.\nI used wordpress-export-to-markdown to convert my content. You\u0026rsquo;ll need NodeJS and your WordPress export file to utilize this tool. Instructions can be found in the project\u0026rsquo;s GitHub repository.\nInstall Hugo Refer to the Hugo Install Guide for detailed installation instructions for various operating systems, including using Docker.\nSet Up Site Structure and Choose a Theme Create a \u0026ldquo;hugo\u0026rdquo; folder with a subfolder called \u0026ldquo;content.\u0026rdquo; Place your Markdown files in \u0026ldquo;hugo/content.\u0026rdquo; Hugo will automatically generate posts from these files.\nMy GitHub repository for WhiteMatter.tech provides an example site structure. Explore themes on themes.gohugo.io and configure your config.toml or config.yml according to the theme\u0026rsquo;s instructions. My customized theme, a fork of PaperMod, is available on GitHub.\nRun a Development Build Use the following command to generate a local development site:\n1 hugo server -D Access the site in your web browser at http://localhost:1313.\nBuild Your Static Site Once satisfied with your site, build the static files in the \u0026ldquo;hugo/public\u0026rdquo; folder with this command:\n1 hugo Serve Your Site You have various options to serve your site. GitHub Pages is a popular choice for hosting static sites directly from a GitHub repository.\nI use a local Docker image to host my site. You can find details on my previous WordPress/Traefik setup. The key is to serve the content of your \u0026ldquo;hugo/public\u0026rdquo; folder.\nOptional: Serving with Docker You can use curl or wget to download my docker-compose.yml file:\nCurl:\n1 curl -LJO https://raw.githubusercontent.com/RobertDWhite/hugo-webserver-docker/main/docker-compose.yml wget:\n1 wget --no-check-certificate --content-disposition https://raw.githubusercontent.com/RobertDWhite/hugo-webserver-docker/main/docker-compose.yml Alternatively, create a docker-compose.yml file with the following content, making sure to update the path to your \u0026ldquo;hugo/public\u0026rdquo; folder:\n1 2 3 4 5 6 7 8 version: \u0026#34;3\u0026#34; services: nginx: image: nginx:latest ports: - 80:80 volumes: - /path/to/hugo/public:/usr/share/nginx/html Run the following command to start the Docker container:\n1 docker-compose up -d Forward port 80 to your host machine to make your site publicly accessible.\nConsider setting up a reverse proxy with Docker and implementing security measures to protect your network. I have a post on running a reverse proxy with Docker and another on hardening your network for web hosting.\nI prefer this method because it allows me to rebuild and update my site automatically whenever I make changes.\nSelf-Hosting Security If you self-host, read my previous posts on hardening your network security for web hosting.\nWrapping Up I hope this guide helps you migrate from WordPress to Hugo. Good luck with your new static site!\n","date":"2023-11-29T22:26:18Z","image":"https://raw.githubusercontent.com/Nexus-Security/image-1/main/wordpress_to_hugo.png","permalink":"https://Nexus-Security.github.io/switching-from-wordpress-to-hugo/","title":"Switching from Wordpress to Hugo"},{"content":" If you own a Unifi device, such as a Dream Machine, Dream Machine Pro, UNVR, CloudKey, or a similar model, you\u0026rsquo;ve likely encountered the infamous red triangle accompanied by the frustrating message \u0026ldquo;Your connection is not private.\u0026rdquo;\nAs of now, there isn\u0026rsquo;t an official or integrated method for generating a security certificate for your device. However, some intriguing GitHub projects suggest potential solutions if you\u0026rsquo;re willing to experiment. While my approach might not be as elegant as directly generating certificates using LetsEncrypt on the server-side like the linked GitHub project, it offers a reliable solution.\nMy solution involves using an NGINX reverse proxy. This setup forwards traffic from a subdomain linked to my primary domain (e.g., unifi.whitematter.tech, protect.whitematter.tech) to my Dream Machine Pro or UNVR, depending on my needs. For guidance on creating a reverse proxy with Docker, you can refer to my previous post here.\nAfter configuring your reverse proxy and adding a subdomain to your DNS records (e.g., unifi.whitematter.tech) to point to your public IP address, you\u0026rsquo;re ready for the next step.\nThis configuration assumes you\u0026rsquo;ve directed your subdomain (e.g., unifi.whitematter.tech) to your public IP. If your public IP address isn\u0026rsquo;t static, consider using a free dynamic DNS service like DuckDNS. This service automatically updates your DNS records to reflect your IP address if your ISP changes it. My private domain is linked to a DuckDNS hostname for this reason.\nDisclosure: As an Amazon Associate, I earn from qualifying purchases. Your support through these links helps maintain this blog, and the product pricing remains the same for you regardless.\nSetting Up the Reverse Proxy for Unifi Devices Configuring the reverse proxy is fairly straightforward. The image below demonstrates the correct setup. In the \u0026ldquo;Domain Names\u0026rdquo; field, enter your domain (e.g., unifi.whitematter.tech). Choose \u0026ldquo;https\u0026rdquo; for the \u0026ldquo;Scheme\u0026rdquo; and specify your Unifi Dream Machine\u0026rsquo;s local IP or other Unifi device\u0026rsquo;s IP address (e.g., 192.168.1.1, 10.0.0.1) in the \u0026ldquo;Forward Hostname / IP\u0026rdquo; field. Set the \u0026ldquo;Forward Port\u0026rdquo; to 443. I generally enable \u0026ldquo;Cache Assets.\u0026rdquo; Given that Unifi devices are secured with a Unifi account, I leave the \u0026ldquo;Access List\u0026rdquo; as public. While you can add username/password protection, I suggest using a strong Unifi password for enhanced security.\nNext, navigate to \u0026ldquo;SSL\u0026rdquo; in the top menu, as illustrated in the subsequent image. Select \u0026ldquo;Request a new SSL Certificate\u0026rdquo; and enable the options for \u0026ldquo;Force SSL,\u0026rdquo; \u0026ldquo;HTTP/2 Support,\u0026rdquo; and \u0026ldquo;HSTS Enabled.\u0026rdquo; Input your email address, which will be used for Let\u0026rsquo;s Encrypt certificate generation, and after reviewing, agree to the Let\u0026rsquo;s Encrypt terms. Save your settings by clicking \u0026ldquo;Save,\u0026rdquo; and you\u0026rsquo;re all set!\nConclusion If you\u0026rsquo;ve followed these instructions carefully and your DNS records are accurate, you should be able to access your Unifi device using the Let\u0026rsquo;s Encrypt-generated certificate by going to unifi.yourdomain.com. The bothersome error message should no longer appear when connecting. Additionally, when you examine the certificate details linked to your Unifi domain, you\u0026rsquo;ll be redirected to https://letsencrypt.org/documents/isrg-cps-v4.1/.\nI appreciate this solution because it simplifies access by requiring only your subdomain instead of an IP address. It also allows you to connect to your device without relying on Unifi\u0026rsquo;s cloud-based connection when you\u0026rsquo;re outside your local network.\nRemember that whenever you make services publicly accessible, it\u0026rsquo;s crucial to prioritize security with strong passwords. Password managers, such as 1Password, and hardware security keys like Yubikey, can significantly enhance your security practices.\n","date":"2023-11-01T23:30:24Z","image":"https://raw.githubusercontent.com/Nexus-Security/image-1/main/header_how-to-connect-to-your-unifi-dream-machine-or-unvr-with-ssl-from-lets-encrypt.png","permalink":"https://Nexus-Security.github.io/connecting-to-your-unifi-dream-machine-or-unvr-using-ssl-from-lets-encrypt/","title":"Connecting to your Unifi Dream Machine or UNVR using SSL from Let's Encrypt"},{"content":" Introduction This guide explains how to set up a Cloudflared container within your Docker environment, creating a secure tunnel from Cloudflare to your internal network resources. Once set up, this tunnel eliminates the need to expose ports 80 and 443.\nThis guide presumes you have an operational internal network with a configured reverse proxy like Nginx Proxy Manager. It also assumes you possess a Cloudflare account with DNS management capabilities for your domain.\nUPDATE 04/17/2023 There\u0026rsquo;s now a simplified method for running Cloudflared containers that streamlines the process described below. Cloudflare allows you to store the configuration in the cloud, eliminating the need for local credential files and simplifying the setup. All you need is a tunnel token from the Cloudflare Zero Trust dashboard.\nYou can now build the entire tunnel locally using docker-compose. Here\u0026rsquo;s an updated docker-compose.yml file that simplifies the process. Just replace YOUR_TOKEN_FROM_CLOUDFLARE with your actual tunnel token from the Cloudflare dashboard:\n1 2 3 4 5 6 7 8 9 10 11 version: \u0026#39;3\u0026#39; services: cloudflared-tunnel: image: cloudflare/cloudflared:latest container_name: cloudflared environment: - TZ=America/New_York - TUNNEL_TOKEN=YOUR_TOKEN_FROM_CLOUDFLARE restart: always command: tunnel run network_mode: \u0026#34;host\u0026#34; If you prefer to set up the tunnel manually, continue reading the original guide below. A more comprehensive tutorial for the cloud-based method will be available in the future.\nThe Cloudflared Project This project utilizes Docker-Compose and Docker on a Ubuntu VM. Initially, this was set up on an Unraid system using the _aeleos/cloudflared_ container, instructions for which can be found at https://github.com/aeleos/cloudflared. The setup has since migrated to Ubuntu for hosting and site independence, but the Unraid instructions remain relevant for those using Unraid.\nFor simplicity, the necessary files will be stored at _/docker-host/cloudflared-whitemattertech/_. If you have multiple domains requiring tunneling, create separate folders like _/docker-host/cloudflared-domain2/_ and _/docker-host/cloudflared-domain3/_. Each step needs to be performed individually for each domain.\nBegin by logging into Cloudflare from your terminal:\n1 docker run --rm -v /.cloudflared:/home/nonroot/.cloudflared/ cloudflare/cloudflared:latest tunnel login After running the command, you\u0026rsquo;ll see output similar to the image below: Use the generated URL from the terminal output to log into your Cloudflare account. Upon successful login, the required certificate will be saved to your /docker-host/cloudflared-whitemattertech/.cloudflared directory.\nNext, create a new tunnel using this terminal command, replacing TUNNELNAMEHERE with your desired name:\n1 docker run --rm -v /docker-host/cloudflared-whitemattertech/.cloudflared:/home/nonroot/.cloudflared/ cloudflare/cloudflared:latest tunnel create TUNNELNAMEHERE Verify the tunnel\u0026rsquo;s status by running:\n1 docker run --rm -v /docker-host/cloudflared-whitemattertech/.cloudflared:/home/nonroot/.cloudflared/ cloudflare/cloudflared:latest tunnel list Make note of the tunnel ID from the output for later use.\nNow, create and edit a configuration file named _config.yaml_:\n1 2 touch /docker-host/cloudflared-whitemattertech/.cloudflared/config.yaml nano /docker-host/cloudflared-whitemattertech/.cloudflared/config.yaml Paste the following content into config.yaml, replacing the _xxx\u0026hellip;_ with your copied tunnel ID:\n1 2 3 4 5 6 7 tunnel: xxxxxxxxxxxxxxxxxxxxxxxxxx credentials-file: /home/nonroot/.cloudflared/xxxxxxxxxxxxxxxxxxxxxxxx.json ingress: - service: https://localhost:443 originRequest: originServerName: whitematter.tech Under \u0026ldquo;ingress\u0026rdquo;, service refers to your reverse proxy host. If different from the example, replace localhost:443 with your reverse proxy IP. originServerName should be your root domain. If you encounter issues starting the container, try using a subdomain like auth.whitematter.tech as the originServerName. A single subdomain is sufficient, regardless of how many you plan to use with the tunnel.\nNext, create the docker-compose.yml file:\n1 2 touch /docker-host/cloudflared-whitemattertech/docker-compose.yml nano /docker-host/cloudflared-whitemattertech/docker-compose.yml For your convenience, the example below includes commented-out configurations for additional domains. If you\u0026rsquo;re using more than one domain, uncomment and update those lines with your specific paths.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 version: \u0026#34;3\u0026#34; services: cloudflared-whitematter: image: cloudflare/cloudflared:latest container_name: cloudflared-whitematter environment: - TZ=America/New_York volumes: - /docker-host/cloudflared-whitematter/:/home/nonroot/.cloudflared/ restart: always command: tunnel run network_mode: \u0026#34;host\u0026#34; # cloudflared-domain1: # image: cloudflare/cloudflared:latest # container_name: cloudflared-domain1 # environment: # - TZ=America/New_York # volumes: # - /docker-host/cloudflared-domain1/:/home/nonroot/.cloudflared/ # restart: always # command: tunnel run # network_mode: \u0026#34;host\u0026#34; # cloudflared-domain2: # image: cloudflare/cloudflared:latest # container_name: cloudflared-domain2 # environment: # - TZ=America/New_York # volumes: # - /docker-host/cloudflared-domain2/:/home/nonroot/.cloudflared/ # restart: always # command: tunnel run # network_mode: \u0026#34;host\u0026#34; # cloudflared-domain3: # image: cloudflare/cloudflared:latest # container_name: cloudflared-domain3 # environment: # - TZ=America/New_York # volumes: # - /docker-host/cloudflared-domain3/:/home/nonroot/.cloudflared/ # restart: always # command: tunnel run # network_mode: \u0026#34;host\u0026#34; Finally, start your container(s) with this command:\n1 docker-compose up -d Cloudflare DNS Modifications In your Cloudflare DNS settings, replace any existing A records with CNAME records. The Name field should point to your root domain (@), and the Value field should be set to TUNID.cfargotunnel.com, replacing TUNID with your actual tunnel ID.\nFor subdomains, create a CNAME record for each. Set the Name field to the subdomain (e.g., auth, plex, unraid) and the Value field to \u0026ldquo;_@_\u0026rdquo;. After saving, Cloudflare will automatically replace \u0026ldquo;_@_\u0026rdquo; with your root domain name. For reference, see the image below: Once these changes are saved, your internal applications should be accessible through the Cloudflare tunnel. You can now disable port forwarding for ports 80 and 443 to your reverse proxy. Performing a _dig_ command on your domain or subdomain will confirm that your services no longer point to your external IP, enhancing the privacy of your self-hosted applications.\nWrapping Up Hopefully, this guide has provided you with the necessary steps to successfully set up your own Cloudflare tunnel.\n","date":"2023-09-28T11:56:17Z","image":"https://raw.githubusercontent.com/Nexus-Security/image-1/main/logo.png","permalink":"https://Nexus-Security.github.io/using-cloudflared-for-tunneling-to-internal-resources/","title":"Using Cloudflared for Tunneling to Internal Resources"},{"content":"Introduction This guide demonstrates setting up AlienVault OSSIM as a virtual machine within Unraid. OSSIM, a robust open-source SIEM (Security Information and Event Management) system, offers free network monitoring and protection. Its applications include network-wide vulnerability assessments and intrusion detection on endpoint devices.\nThe integrated HIDS within OSSIM originates from OSSEC. Moreover, OSSIM seamlessly integrates with Open Threat Exchange (OTX), a platform available for Windows, Mac, and Linux systems. This integration provides an updated, open-source vulnerability scanning tool. Deploying the OTX installer is made possible through platforms like Mosyle (MDM for MacOS) and Intune (MDM for Windows).\nWhile a custom-built Unraid server serves as the primary system, getting OSSIM to run on it as a VM was a challenge previously. Past attempts resulted in the VM failing to boot after installation. This guide outlines the successful configuration settings for installing and running OSSIM on Unraid, potentially filling a gap in available resources.\nDownload OSSIM The OSSIM installation ISO is freely downloadable from: https://cybersecurity.att.com/products/ossim/download. As an open-source solution, it\u0026rsquo;s free to use.\nAlternatively, you can use Unraid\u0026rsquo;s terminal or SSH to download the ISO directly. Ensure you\u0026rsquo;re in the desired directory (e.g., /mnt/user/iso/) and use the command:\n1 wget https://dlcdn.alienvault.com/AlienVault_OSSIM_64bits.iso Prepare Unraid If downloaded locally, transfer the ISO file to your Unraid server\u0026rsquo;s file share. If using \u0026lsquo;wget,\u0026rsquo; the ISO is ready for use.\nCreate a New VM The key to running OSSIM within Unraid lies in selecting \u0026lsquo;Debian\u0026rsquo; as the VM host. Under the \u0026lsquo;VMs\u0026rsquo; tab in Unraid, choose \u0026lsquo;ADD VM\u0026rsquo; and select \u0026lsquo;Debian\u0026rsquo; under the \u0026lsquo;Linux\u0026rsquo; section.\nConfigure the VM Maintain the default \u0026lsquo;Host Passthrough\u0026rsquo; CPU mode. Allocate desired cores/threads (e.g., 8 cores for a Ryzen 9 3950x). Set both Initial and Max Memory to 8192MB.\nSelect the following:\nMachine: Q35-6.0 BIOS: SeaBIOS USB Controller: 3.0 (nec XHCI) OS Install ISO: Path to the downloaded OSSIM ISO (e.g., /mnt/user/iso/AlienVault_OSSIM_64bits.iso) Choose \u0026lsquo;SATA\u0026rsquo; for \u0026lsquo;OS Install CDRom Bus.\u0026rsquo; Specify the location and size of the OSSIM VM\u0026rsquo;s virtual disk under \u0026lsquo;Primary vDisk Location,\u0026rsquo; opting for \u0026lsquo;SATA\u0026rsquo; again for \u0026lsquo;Primary vDisk Bus.\u0026rsquo;\nLeave remaining settings until \u0026lsquo;Network MAC\u0026rsquo; untouched. Add a second virtual network interface by clicking the \u0026lsquo;+\u0026rsquo; in the network pane\u0026rsquo;s bottom left corner. For the new \u0026lsquo;2nd Network MAC,\u0026rsquo; duplicate the original MAC address and modify the last character. Keep the rest as default.\nStart the VM Save the VM configuration and start the machine. Follow the standard OSSIM installation process. The designated vDisk will be automatically chosen for installation. When prompted to install the GRUB bootloader, skip this step.\nAfter installation and upon restarting the VM, you\u0026rsquo;ll be able to set up the OSSIM administrator credentials.\nWrapping Up OSSIM\u0026rsquo;s capabilities come with complexity. For newcomers, extensive research on SIEM tools and OSSIM-specific configuration is advised. While potentially exceeding typical home lab needs, mastering OSSIM is valuable for any security professional.\n","date":"2023-09-25T19:02:09Z","image":"https://raw.githubusercontent.com/Nexus-Security/image-1/main/ossimunraid.png","permalink":"https://Nexus-Security.github.io/running-alienvault-ossim-as-a-virtual-machine-on-unraid/","title":"Running AlienVault OSSIM as a virtual machine on Unraid"},{"content":"Working from home during the COVID-19 pandemic has been a mixed bag for many. While I enjoyed the increased time at home, the closure of coffee shops was a definite downside. As someone who thrives on freshly brewed coffee, I missed the aroma and the experience of enjoying a meticulously crafted beverage.\nTo satisfy my caffeine cravings, I invested in high-quality coffee equipment. Not only can I experiment with various coffee drinks, but I\u0026rsquo;ve also saved a considerable amount of money. In this post, I\u0026rsquo;ll share my favorite coffee gear. Stay tuned for future posts where I\u0026rsquo;ll detail the specific drinks I create.\nDisclosure: As an Amazon Associate, I earn from qualifying purchases. Your support helps maintain this blog. Rest assured, prices remain the same for you whether you use my links or not. Thank you for your support!\nEspresso My espresso machine, a Breville Barista Express, was chosen for its balance of manual control, built-in grinder, and affordability. This machine allows me to grind beans, pull shots, steam milk, and craft delicious espresso drinks with ease.\n[Image of Breville Barista Express Espresso Machine with link to purchase on Amazon]\nTo maintain a clean workspace while using the espresso machine, I rely on a rubber tamper mat. This simple addition simplifies cleanup and protects my counter.\n[Image of rubber tamper mat with link to purchase on Amazon]\nDid you know spent espresso grounds make excellent fertilizer? I utilize mine to nourish my plants, reducing the need for store-bought fertilizers. A coffee knock box helps collect the used grounds for easy transfer to my garden.\n[Image of coffee knock box with link to purchase on Amazon]\nCoffee Pour Over My preferred pour-over setup consists of a Chemex coffee maker and Chemex filters, known for producing a clean and flavorful cup.\n[Image of Chemex coffee maker with link to purchase on Amazon]\nFor precise temperature control during pour-over brewing, I use an electric kettle. I recommend the OVALWARE Electric Kettle for its granular temperature settings.\n[Image of OVALWARE Electric Kettle with link to purchase on Amazon]\nAnother excellent electric kettle option, particularly if you enjoy tea, is the COSORI Electric Gooseneck Kettle. Its pre-set temperature options simplify the brewing process for various beverages.\n[Image of COSORI Electric Gooseneck Kettle with link to purchase on Amazon]\nAccurate measurements are crucial for pour-over coffee. I use a reliable and affordable scale recommended by a barista friend.\n[Image of coffee scale with link to purchase on Amazon]\nVietnamese Coffee For Vietnamese coffee, specifically the iced variety known as cafe sua da, I utilize a traditional Vietnamese coffee filter.\n[Image of Vietnamese coffee filter with link to purchase on Amazon]\nI use the same electric kettles mentioned earlier for Vietnamese coffee preparation.\nDrip Coffee Finding the perfect drip coffee maker was challenging. I desired a machine with a quality built-in grinder for fresh grounds, versatile brewing options, from single servings to full carafes, and programmable scheduling for automated morning coffee. The Breville BDC650BSS Grind Control fulfilled all my requirements.\n[Image of Breville BDC650BSS Grind Control Coffee Maker with link to purchase on Amazon]\nCold Brew My cold brew consumption outpaced my previous setup, so I upgraded to a gallon-sized mason jar with a dispenser.\n[Image of gallon mason jar with dispenser and link to purchase on Amazon]\nNitro Cold Brew To infuse my cold brew with nitrogen, I utilize a Royal Nitro Coffee Maker. This system creates smooth and creamy nitro cold brew at home.\n[Image of Royal Nitro Coffee Maker with link to purchase on Amazon]\nNitrous oxide canisters, commonly used for whipped cream dispensers, are necessary for infusing the coffee.\n[Image of nitrous oxide canisters with link to purchase on Amazon]\nSiphon Coffee While more involved, siphon coffee offers a visually impressive and flavorful brewing experience. My current model is the Hario Technica 5-Cup Syphon Coffee Maker.\n[Image of Hario Technica 5-Cup Syphon Coffee Maker with link to purchase on Amazon]\nThe Belgian Family Balance Siphon, although pricier, boasts an even more captivating design. Perhaps one day I\u0026rsquo;ll have the pleasure of trying it.\nWrapping Up There\u0026rsquo;s a world of coffee brewing possibilities to explore. I encourage you to try these methods and discover your favorites. Feel free to share your experiences or alternative product recommendations. I\u0026rsquo;m always seeking new coffee adventures!\n","date":"2023-07-21T16:57:39Z","image":"https://raw.githubusercontent.com/Nexus-Security/image-1/main/header_coffee-gear.jpg","permalink":"https://Nexus-Security.github.io/the-essential-coffee-equipment-checklist-for-working-from-home/","title":"The Essential Coffee Equipment Checklist for Working from Home"},{"content":"This guide explains how to set up a Docker Registry GUI using Harbor on a Ubuntu virtual machine. The registry will operate within a Docker container. The tutorial uses docker-compose to create the necessary containers.\nDownloading and Unpacking the Harbor Installer You\u0026rsquo;ll begin by downloading the Harbor installer, extracting its contents, and navigating to the Harbor directory:\n1 2 3 4 5 wget https://github.com/goharbor/harbor/releases/download/v2.3.1/harbor-offline-installer-v2.3.1.tgz tar -xzf harbor-offline-installer-v2.3.1.tgz cd harbor/ Generating SSL Certificates for Internal Use This section focuses on generating SSL certificates for internal registry use. For external access, obtain certificates from a trusted Certificate Authority. These steps are Linux-specific.\nOpen a terminal and execute the commands below, replacing the placeholder text with your organization\u0026rsquo;s details. In the cat command where [alt_names] appears, substitute DNS.1, DNS.2, and DNS.3 with your host machine\u0026rsquo;s local IP or FQDN. If your machine\u0026rsquo;s name is \u0026ldquo;hostname\u0026rdquo; and your network domain is \u0026ldquo;.local\u0026rdquo;, use \u0026ldquo;hostname.local\u0026rdquo; and \u0026ldquo;hostname\u0026rdquo; as your DNS entries. You can include the static IP as well. Remove any unnecessary DNS entries. If you encounter issues with the cat command, download a pre-created file V3.ext.\nAfter creating the certificates, store them securely for adding as \u0026ldquo;trusted\u0026rdquo; on machines accessing the registry.\nPreparing for Installation A pre-made docker-compose.yml file is available for deploying the registry and Harbor GitHub. This file should be placed in the Harbor directory created earlier. Retrieve the file using either curl or wget:\n1 curl -LJO [https://raw.githubusercontent.com/Nexus-Security/Harbor-Registry/main/docker-compose.yml](http://curl -LJO https://raw.githubusercontent.com/Nexus-Security/Harbor-Registry/main/docker-compose.yml \u0026#34;https://raw.githubusercontent.com/Nexus-Security/nginx-proxy-manager/main/docker-compose.yml\u0026#34;) OR\n1 wget --no-check-certificate --content-disposition [https://raw.githubusercontent.com/Nexus-Security/Harbor-Registry/main/docker-compose.yml](https://raw.githubusercontent.com/Nexus-Security/Harbor-Registry/main/docker-compose.yml) Alternatively, you can create your own docker-compose.yml file by taking the content from the provided GitHub link.\nInstalling the Registry and Harbor With everything in place, run the following command to start the installation process:\n1 docker-compose up -d This might take several minutes to complete. Upon successful completion, the containers for your Docker Registry and Harbor (the GUI) will be operational. You can then access the Harbor GUI via a web browser using https://hostname, https://hostname.local, or https://IP_OF_HOST assuming the SSL certificates are trusted.\nManaging Harbor Harbor\u0026rsquo;s official documentation at https://goharbor.io/docs/2.3.0/administration/ offers comprehensive information about Harbor administration.\nConclusion After completing these steps, you should have a functional Docker Registry and Harbor setup. Remember to explore Docker image and container tagging for effective registry management.\n","date":"2023-07-20T20:35:49Z","image":"https://raw.githubusercontent.com/Nexus-Security/image-1/main/harbor.png","permalink":"https://Nexus-Security.github.io/running-a-locally-hosted-docker-registry-gui-with-harbor/","title":"Running a Locally Hosted Docker Registry GUI with Harbor"},{"content":"After sharing my guide on routing Docker containers through a VPN on Unraid, I received many questions about accessing Twitter anonymously. This post explains my method for anonymously accessing Twitter feeds without an account.\nThis guide assumes you understand how to route Docker containers through a VPN on Unraid. If not, please refer to my previous post on the topic. While I use Unraid, these steps can be adapted for Docker on other operating systems.\nNitter Nitter (zedeus/nitter) is a third-party Docker container that allows anonymous Twitter feed access. It acts as an intermediary, fetching data for you so your device never interacts directly with Twitter. This prevents Twitter from tracking your IP address or browser fingerprint. Behind a VPN, Nitter becomes a private, self-hosted Twitter reader that doesn\u0026rsquo;t require an account or track your activity.\nPublic Nitter instances are available if you prefer not to host your own. However, you can use a reverse proxy to connect a custom domain like \u0026ldquo;twitter.your-domain.com\u0026rdquo; to your own Nitter instance.\nNitter\u0026rsquo;s interface lets you search for users and view their feeds. Clicking the RSS icon on a user\u0026rsquo;s profile provides an RSS feed URL, which we\u0026rsquo;ll use in the next step.\nFreshRSS FreshRSS (linuxserver/freshrss) is a Docker container that manages RSS feeds. I recommend using it alongside your Nitter container, even with a public instance, and routing it through a VPN for maximum privacy and security.\nAfter setting up the container, add your desired Nitter RSS feeds. Create categories to organize your feeds within FreshRSS. When adding a feed, you can also enhance your privacy by using a SOCKS5 proxy.\nFor convenient access, use a reverse proxy to link your FreshRSS instance to a custom domain.\nI highly recommend setting up strong HTTP authentication if you make either Nitter or FreshRSS publicly accessible. A password manager can help generate and securely store strong passwords.\nConclusion You\u0026rsquo;ve successfully set up an anonymous way to access Twitter feeds while enhancing your privacy and minimizing data tracking. Congratulations!\n","date":"2023-07-12T22:48:50Z","image":"https://raw.githubusercontent.com/Nexus-Security/image-1/main/header_how-to-access-twitter-without-an-account-anonymously.jpg","permalink":"https://Nexus-Security.github.io/accessing-twitter-without-an-account-anonymously/","title":"Accessing Twitter without an account, anonymously"},{"content":"I\u0026rsquo;ve switched from using Traefik and WordPress for my website. Now, I use an internal Nginx proxy within Docker and my site is built with Hugo. To learn more about this change, you can read my post: Migrating from WordPress to Hugo\nThis first post will guide you on how to establish your own self-hosted Wordpress website using Docker, mirroring the setup of this very site. For this example, I\u0026rsquo;m using a physical Ubuntu machine connected to a Unifi Dream Machine Pro. You have the flexibility to employ a VPS or any bare-metal operating system that supports Docker. While this tutorial focuses on Ubuntu-specific tools, adjustments can be made for other environments if you\u0026rsquo;re comfortable with Docker. For code examples and further assistance, you can visit this project\u0026rsquo;s GitHub page.\nDisclosure: As an Amazon Associate, I earn from qualifying purchases. Your support through these links helps maintain this blog, but rest assured, the pricing remains the same for you whether you use my links or not. Thanks for your support!\nThis guide presumes you have Ubuntu installed, access to a user account with root privileges, Docker and Docker-Compose set up, a domain name under your ownership (consider Google Domains or Hover if you need one), the ability to manage your domain\u0026rsquo;s DNS records (CloudFlare is recommended), and the capability to forward ports on your network (unless using a VPS or externally hosted OS).\nUbuntu\u0026rsquo;s environment file system is particularly useful for securely storing sensitive variables. This simplifies sharing docker-compose files without exposing confidential information on platforms like GitHub. While there might be other methods for managing secrets within Docker, they won\u0026rsquo;t be covered in this tutorial. Lastly, prioritize security when hosting a website on your network. Additionally, if you\u0026rsquo;re running multiple externally accessible web servers, you might find this post on reverse proxies helpful: https://nexus-security.github.io/running-a-reverse-proxy-using-docker-made-simple/.\nPrepare Your Domain For your domain (e.g., whitematter.tech), add DNS records that direct it to your server\u0026rsquo;s public IP address. For home networks, this is the public IP provided by your ISP, which can be found at whatismyip.com. Personally, I prefer dynamic DNS solutions like duckdns.org, allowing automatic updates to my public IP in the absence of a static IP from my ISP. For a better understanding of how to integrate duckdns, refer to this video. The following image illustrates how CloudFlare is configured for my domain, pointing to my duckdns address (without duckdns, \u0026ldquo;Content\u0026rdquo; would be your public IP, such as 177.99.88.10).\nSet Environment Variables In Ubuntu, access the Environment file with the command:\n1 sudo nano /etc/environment Below the PATH variable, paste the following, replacing the generic values in red with your specific information. Do not modify the PATH variable\u0026rsquo;s content.\nFor your time zone (TZ), refer to this list.\nReplace “username” in the USERDIR with your Ubuntu username. It\u0026rsquo;s strongly recommended to use a password manager like 1Password to generate and securely store passwords for MySQL and your WordPress databases.\n1 2 3 4 5 6 7 8 9 PATH=\u0026#34;/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin:/usr/games:/usr/local/games\u0026#34; TZ=America/New_York USERDIR=/home/username/docker MYSQL_ROOT_PASSWORD=dbrootpassword DATABASE_NAME=dbname DATABASE_USER=dbuser DATABASE_PASSWORD=dbpassword DOMAIN=yourdomain.com Save the file (“Ctrl” + “o” followed by “Enter”) and exit (“Ctrl” + “x”).\nA logout/login or a system reboot is needed for the changes to take effect:\n1 sudo reboot Preparing Directories and Permissions Execute the following commands. You can run them all at once or individually. ${USERDIR} will be populated from the environment file.\n1 2 3 4 5 6 7 8 sudo mkdir ${USERDIR} sudo mkdir ${USERDIR}/apps sudo mkdir ${USERDIR}/wordpress sudo mkdir ${USERDIR}/wordpress/wp-data sudo mkdir ${USERDIR}/data sudo mkdir ${USERDIR}/data/configurations sudo touch ${USERDIR}/data/acme.json sudo chmod 600 ${USERDIR}/data/acme.json Create Traefik static configuration file. Use the following command to create the Traefik static configuration file (${USERDIR} will be taken from your environment file):\n1 sudo nano ${USERDIR}/data/traefik.yml Paste the following content into the file, ensuring you replace the placeholder email address in red with your own.\nTwo caServer addresses are provided at the end of the file. The one containing \u0026ldquo;staging\u0026rdquo; is intended for testing purposes. While using the staging server certificates is beneficial during setup, be aware that browsers may display security warnings. The LetsEncrypt server has limitations on the number of certificates issued within a specific timeframe. To disable an address, simply add \u0026ldquo;#\u0026rdquo; at the beginning of the line – Ubuntu will ignore lines prefixed with \u0026ldquo;#.\u0026rdquo;\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 api: dashboard: true entryPoints: http: address: \u0026#34;:80\u0026#34; http: redirections: entryPoint: to: https https: address: \u0026#34;:443\u0026#34; http: middlewares: - secureHeaders@file - page-ratelimit@file tls: certResolver: letsencrypt providers: docker: endpoint: \u0026#34;unix:///var/run/docker.sock\u0026#34; exposedByDefault: false file: filename: /configurations/dynamic.yml certificatesResolvers: letsencrypt: acme: email: you@yourdomain.com storage: acme.json keyType: EC384 httpChallenge: entryPoint: http #caServer: https://acme-staging-v02.api.letsencrypt.org/directory caServer: https://acme-v02.api.letsencrypt.org/directory Save the file (“Ctrl” + “o” and “Enter”) and exit (“Ctrl” + “x”).\nPrepare Traefik and Create Traefik Dynamic Config For security, your Traefik password needs to be hashed using MD5, SHA1, or BCrypt. BCrypt is recommended for its stronger encryption.\nOnline Htpassword generators can be used for this purpose. For instance, if your username is \u0026ldquo;admin\u0026rdquo; and password is \u0026ldquo;password1234,\u0026rdquo; the BCrypt hash would resemble this: \u0026ldquo;admin:$2y$10$d0yk7WE.XqhF5bT1DdJhduRFOM5JSabTiSFCTnbC2.JgMolypHgS2.\u0026rdquo; Utilize a password manager (1Password is recommended) to generate a strong password for this step.\nOnce your username, password, and its hashed version are ready, proceed with the following:\nCreate the Traefik dynamic configuration file:\n1 sudo nano ${USERDIR}/data/configurations/dynamic.yml Paste the content below into the dynamic.yml file, replacing the placeholder username and password (in red) with your generated hashed credentials.\nThis configuration utilizes the page-ratelimit middleware. The provided values permit an average of 50 requests per second with a burst limit of 50. Feel free to adjust these values as needed.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 http: middlewares: secureHeaders: headers: frameDeny: true sslRedirect: true browserXssFilter: true contentTypeNosniff: true forceSTSHeader: true stsIncludeSubdomains: true stsPreload: true stsSeconds: 31536000 customFrameOptionsValue: SAMEORIGIN user-auth: basicAuth: users: - \u0026#34;admin:$2y$10$d0yk7WE.XqhF5bT1DdJhduRFOM5JSabTiSFCTnbC2.JgMolypHgS2\u0026#34; page-ratelimit: rateLimit: average: 50 burst: 50 tls: options: default: cipherSuites: - TLS_ECDHE_ECDSA_WITH_AES_256_GCM_SHA384 - TLS_ECDHE_RSA_WITH_AES_256_GCM_SHA384 - TLS_ECDHE_ECDSA_WITH_AES_128_GCM_SHA256 - TLS_ECDHE_RSA_WITH_AES_128_GCM_SHA256 - TLS_ECDHE_ECDSA_WITH_CHACHA20_POLY1305 - TLS_ECDHE_RSA_WITH_CHACHA20_POLY1305 minVersion: VersionTLS12 Save (“Ctrl” + “o” and “Enter”) and exit (“Ctrl” + “x”).\nCreate the Traefik Docker-Compose File Create the Traefik docker-compose file:\n1 sudo nano ${USERDIR}/docker-compose.yml Copy the content below into the file, noting that fields enclosed in ${ } will be filled in from the environment file.\nThis configuration uses the latest version of Traefik (currently v2.2.6). If you prefer to stick to the latest release within the 2.2.X series, specify the image tag as \u0026ldquo;traefik:chevrotin.\u0026rdquo; Be mindful that using the \u0026ldquo;latest\u0026rdquo; tag might lead to compatibility issues if future Traefik versions (v3, etc.) introduce breaking changes. Check dockerhub for available release tags.\nThis setup routes ports 80 (http) and 443 (https) to the Traefik container.\nYou can find a copy of this docker-compose.yml file on GitHub.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 version: \u0026#39;3.3\u0026#39; services: traefik: image: traefik:latest container_name: traefik restart: always security_opt: - no-new-privileges:true ports: - 80:80 - 443:443 volumes: - /var/run/docker.sock:/var/run/docker.sock:ro - ${USERDIR}/data/traefik.yml:/traefik.yml:ro - ${USERDIR}/data/configurations:/configurations - ${USERDIR}/data/acme.json:/acme.json environment: TZ: ${TZ} networks: - proxy labels: - \u0026#34;traefik.enable=true\u0026#34; - \u0026#34;traefik.docker.network=proxy\u0026#34; - \u0026#34;traefik.http.routers.traefik-secure.entrypoints=https\u0026#34; - \u0026#34;traefik.http.routers.traefik-secure.rule=Host(`traefik.${DOMAIN}`)\u0026#34; - \u0026#34;traefik.http.routers.traefik-secure.service=api@internal\u0026#34; - \u0026#34;traefik.http.routers.traefik-secure.middlewares=user-auth@file\u0026#34; networks: proxy: external: true Save (“Ctrl” + “o” and “Enter”) and exit (“Ctrl” + “x”).\nCreate the WordPress Docker-Compose File Create the WordPress docker-compose file:\n1 sudo nano ${USERDIR}/apps/docker-compose.yml Copy the content below, noting that fields enclosed in ${ } will be populated from the environment file.\ndocker-compose.yml\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 version: \u0026#39;3.7\u0026#39; services: db: image: mariadb:latest container_name: wp-db volumes: - db-data:/var/lib/mysql networks: - default security_opt: - no-new-privileges:true restart: unless-stopped environment: MYSQL_ROOT_PASSWORD: ${MYSQL_ROOT_PASSWORD} MYSQL_DATABASE: ${DATABASE_NAME} MYSQL_USER: ${DATABASE_USER} MYSQL_PASSWORD: ${DATABASE_PASSWORD} TZ: ${TZ} wordpress: depends_on: - db image: wordpress:latest container_name: wordpress environment: WORDPRESS_DB_HOST: db:3306 WORDPRESS_DB_NAME: ${DATABASE_NAME} WORDPRESS_DB_USER: ${DATABASE_USER} WORDPRESS_DB_PASSWORD: ${DATABASE_PASSWORD} TZ: ${TZ} volumes: - ${USERDIR}/wordpress/wp-data:/var/www/html networks: - proxy - default security_opt: - no-new-privileges:true restart: unless-stopped labels: - \u0026#34;traefik.enable=true\u0026#34; - \u0026#34;traefik.docker.network=proxy\u0026#34; - \u0026#34;traefik.http.routers.wordpress-secure.entrypoints=https\u0026#34; - \u0026#34;traefik.http.routers.wordpress-secure.rule=Host(`${DOMAIN}`)\u0026#34; portainer: container_name: portainer image: portainer/portainer:latest restart: unless-stopped command: -H unix:///var/run/docker.sock networks: - proxy - default security_opt: - no-new-privileges:true volumes: - /var/run/docker.sock:/var/run/docker.sock:ro environment: TZ: ${TZ} labels: - \u0026#34;traefik.enable=true\u0026#34; - \u0026#34;traefik.docker.network=proxy\u0026#34; - \u0026#34;traefik.http.routers.portainer-secure.entrypoints=https\u0026#34; - \u0026#34;traefik.http.routers.portainer-secure.rule=Host(`portainer.${DOMAIN}`)\u0026#34; volumes: db-data: networks: proxy: external: true Create the Proxy Network and Start Traefik With all files in place, create the proxy network and start Traefik.\nCreate the proxy network:\n1 docker network create proxy Start the Traefik container:\n1 docker-compose -f ${USERDIR}/docker-compose.yml up -d Allow a few minutes for startup, then try to access the Traefik web admin page at https://traefik.yourdomain.com (replace \u0026ldquo;yourdomain.com\u0026rdquo; with your domain, matching the one in the environment file). If successful, log in using the non-hashed username and password. You should see the following:\nIf you can\u0026rsquo;t access the page, you might need to forward ports 80 and 443 to your Ubuntu server (see the example screenshot for forwarding to Ubuntu at 10.100.0.15). This step is not required for VPS or externally hosted options. This post provides recommendations for securing your network. Once ports are forwarded, you can proceed.\nStart the Remaining Containers Important: Remember to log in to WordPress and Portainer immediately after starting them to set passwords and secure access.\nStart the remaining containers:\n1 docker-compose -f ${USERDIR}/apps/docker-compose.yml up -d Give the containers a few minutes to start, and then try to access WordPress and Portainer through your browser.\nWordPress:\nAccess WordPress at “https://yourdomain.com\u0026quot; (replace \u0026ldquo;yourdomain.com\u0026rdquo; with your domain).\nYou should land on the initial WordPress setup screen. Create a strong password for your WordPress user account. A password manager like 1Password can help with this.\nA success message will indicate a successful WordPress installation.\nPortainer:\nAccess Portainer at “https://portainer.yourdomain.com\u0026quot; (replace \u0026ldquo;yourdomain.com\u0026rdquo; with the domain from Section 1).\nYou\u0026rsquo;ll be directed to the Portainer setup screen. Create a strong password for your Portainer user account. Again, using a password manager like 1Password is recommended.\nAfter setting your Portainer password, log in. In the left-hand menu, click \u0026ldquo;containers.\u0026rdquo; This view lists all running containers. From here, you can stop, start, restart, remove, and otherwise manage installed containers.\nGetting Started with WordPress It\u0026rsquo;s a good practice to increase the PHP memory limit and maximum file upload size for new WordPress installations. One approach is to modify the .htaccess file. Access this file in the terminal (${USERDIR} will be taken from the environment file; the file path below assumes you\u0026rsquo;ve kept the WordPress volume as defined in the docker-compose file).\n1 sudo nano ${USERDIR}/wordpress/wp-data/.htaccess Add the following lines to the file:\n1 2 3 4 5 php_value memory_limit 256M php_value upload_max_filesize 128M php_value post_max_size 128M php_value max_execution_time 300 php_value max_input_time 1000 Save (“Ctrl” + “o” and “Enter”) and exit (“Ctrl” + “x”).\nRestart the WordPress container for the changes to take effect. You can either do this through Portainer or restart all containers:\n1 docker restart $(docker ps -q) WordPress Admin Page:\nYou can access your WordPress admin area at \u0026ldquo;https://yourdomain.com/wp-admin\" (replace \u0026ldquo;yourdomain.com\u0026rdquo; with your domain). Use the credentials you set up in Section 14 to log in.\nChange the WordPress Theme:\nYou can easily change your blog or website’s theme.\nClick “Add New Theme,” search for your desired theme, hover over it, and click “Install.” After installation, click “Activate.”\nCreate a Blog Post:\nTo add a new blog post, click “Posts,” then “Add New.\u0026rdquo; This will generate a blank post. The default WordPress block editor is a user-friendly option for creating content. If you prefer, you can install page builder plugins. Here’s what the default block editor looks like:\nAfter crafting your post, click “Publish,” and then click it again to make it live. You can view your post by navigating to the address displayed under “Post address.” Your blog post will resemble this. Customization can be achieved through settings and plugins.\n","date":"2023-06-29T05:46:49Z","image":"https://raw.githubusercontent.com/Nexus-Security/image-1/main/header_hosting-your-own-site-with-traefik-and-wordpress_main.jpg","permalink":"https://Nexus-Security.github.io/create-your-own-wordpress-website-for-free-using-traefik-and-docker/","title":"Create your own Wordpress website for free using Traefik and Docker"},{"content":" #UPDATE 11/05/2022 Original post date: 2023-07-29T15:38:58Z\nThis update details the configuration process for utilizing 4096-bit RSA keys, the SHA256 Auth digest algorithm, and AES256 encryption. The initial post used the default PIA key length of 2048, SHA1, and allowed for AES128.\nUpdated information will be tagged with #Update throughout this post.\nIntroduction This post will guide you on how to use pfSense within a Unifi network situated behind a Unifi Gateway, specifically a Unifi Dream Machine Pro (UDMP). I will outline my network configuration, encompassing subnets, VLANs, and wireless SSIDs, that are crucial for a successful setup. The ultimate goal is to enable the addition of a client to a specific VLAN on my Unifi network, either wirelessly by connecting to a designated SSID or by tagging the client\u0026rsquo;s physical port to that VLAN. This VLAN will correspond to a subnet that directs traffic through the pfSense machine, which functions as a VPN client (using Private Internet Access). This approach allows the UDMP to maintain its role as the DHCP server for these clients while pfSense handles the anonymization and encryption of their data.\nDisclosure: As an Amazon Associate, I earn from qualifying purchases. Your support helps maintain this blog. Please note that the pricing remains the same for you, regardless of whether you use my links.\nPrerequisites for this setup include: a Unifi Gateway device (e.g., UDMP, Unifi Security Gateway), a pfSense machine/VM, Unifi wireless APs (necessary for adding wireless devices to the VPN), and Unifi switches (required for tagging specific switch ports to the VPN). It is also assumed that you have an active VPN service subscription. This guide primarily focuses on PIA but can potentially be adapted for other VPN providers.\nWe will begin by configuring pfSense and the VPN connection, followed by the Unifi setup.\npfSense Setup and Configuration My custom pfSense machine consists of the following components: an Intel Core i5-8500 CPU, a GIGABYTE B365M DS3H motherboard, 16GB of Corsair Vengeance LPX RAM, a Thermaltake Smart 500W Power Supply, and a 4-Port PCI-E Network Interface Card.\nSelect a PIA Server Start by choosing an optimal server location, typically based on your country and geographic region. Using your PIA account credentials, access the complete server list at: https://www.privateinternetaccess.com/pages/ovpn-config-generator\nTo import the required certificate, select the 1198 port option and click \u0026ldquo;Generate.\u0026rdquo;\n#UPDATE For enhanced security with a 4096 RSA cert, download the .CRT file from: https://www.privateinternetaccess.com/openvpn/ca.rsa.4096.crt\nOpen the downloaded file in a text editor (e.g., Atom, Notepad++) and copy the entire content from -\u0026mdash;-BEGIN CERTIFICATE\u0026mdash;\u0026ndash; to -\u0026mdash;-END CERTIFICATE\u0026mdash;\u0026ndash;, as illustrated in the image below.\nCreate a Certificate Authority in pfSense Navigate to System \u0026gt; Cert Manager in pfSense and click on the \u0026ldquo;+ ADD\u0026rdquo; button. Select \u0026ldquo;Import an existing certificate authority\u0026rdquo; as the \u0026ldquo;Method\u0026rdquo; and paste the copied certificate text into the provided field, as shown below.\nSave the settings.\nThe imported certificate should now be visible in the list.\nConfigure OpenVPN Client With the certificate imported, go to VPN \u0026gt; OpenVPN, then Clients, and click ADD.\nConfigure the OpenVPN client settings according to the following images, replacing the Server Host, PIA Username, and Password with your own details. The server host information can be found in the .ovpn file downloaded earlier from PIA. Copy and paste the following into the \u0026ldquo;Custom options\u0026rdquo; box towards the end of the configuration page:\n1 remote-cert-tls server #UPDATE For enhanced security with the 4096 key, SHA256, and AES256 encryption, paste the following into the \u0026ldquo;Custom options\u0026rdquo; box instead of the text above:\n1 2 3 remote-cert-tls server; persist-key; persist-tun; #UPDATE When opting for the 4096 key, SHA256, and AES256 encryption, use Port 1197 instead of Port 1198, as shown in the image below.\n#UPDATE For the 4096 key, SHA256, and AES256 encryption configuration, select only AES-256-CBC under \u0026ldquo;Data Encryption Algorithms\u0026rdquo;.\nChoose AES-256-CBC (256 bit key, 128 bit block) for \u0026ldquo;Fallback Data Encryption Algorithm\u0026rdquo;.\nSet \u0026ldquo;Auth digest algorithm\u0026rdquo; to SHA256 (256-bit).\nEnabling \u0026ldquo;Hardware Crypto\u0026rdquo;, preferably Intel RDRAND engine - RAND, is highly recommended for enhanced encryption.\n#UPDATE When using the 4096 key, SHA256, and AES256 encryption, ensure to paste the following into the \u0026ldquo;Custom options\u0026rdquo; box:\n1 2 3 remote-cert-tls server; persist-key; persist-tun; pfSense Gateway and Interface Assignment Now comes the more complex part. This step involves creating a Gateway on pfSense for routing traffic. My setup uses a WAN (10.99.1.0/24 subnet on Unifi), LAN (10.99.2.0/24 subnet on Unifi), VLAN500 (VLAN tag on Unifi), and a PIA interface. The configuration of each will be detailed below.\nConfigure the Gateway\nIn pfSense, go to System \u0026gt; Routing \u0026gt; Gateways and click \u0026quot;+ Add\u0026quot; to create a new Gateway. Enter the Gateway IP address of your Unifi Gateway, which in my case is 10.99.1.1, corresponding to the UDMP\u0026rsquo;s address on the 10.99.1.0/24 subnet (named pfSense WAN on Unifi). Save the configuration.\nInterface Assignments\nNavigate to Interfaces \u0026gt; Assignments. If a WAN interface is already assigned, proceed to edit it. Ensure the settings resemble the image below, replacing 10.99.1.15 with your preferred static WAN IP address (any address within the 10.99.1.0/24 subnet except the Gateway address, 10.99.1.1). Verify that the IPv4 Upstream gateway is set to the newly created Gateway and that the \u0026ldquo;Block bogon networks\u0026rdquo; option is checked at the bottom.\nSimilarly, check your existing LAN interface or create a new one with the settings shown below. This LAN interface corresponds to the 10.99.2.0/24 subnet, which is where devices tagged with VLAN 500 on Unifi will obtain their DHCP leases. Ensure the IPv4 Upstream gateway is set to \u0026ldquo;None\u0026rdquo; and DO NOT enable \u0026ldquo;Block bogon networks\u0026rdquo; for the LAN interface.\nAdd a PIA Interface\nFrom the \u0026ldquo;Available Network Ports\u0026rdquo; dropdown, select your PIA VPN and click \u0026ldquo;Add.\u0026rdquo;\nOnce added, click on the newly added interface (e.g., OPT2, OPT3) and enable it. You can rename it for clarity (e.g., PIA_Netherlands) and ensure that the reserved network checkboxes are unchecked.\nNext, go to Interfaces \u0026gt; VLANs and click \u0026ldquo;Add.\u0026rdquo; Choose the previously configured LAN interface as the parent interface and select a VLAN tag between 1 and 4094. This tag (500 in my case) will be used in the Unifi configuration.\nFinally, connect the physical interfaces for your WAN and LAN to the pfSense box. Identifying the correct ports is crucial, especially if you have more than two NICs, as you will need to tag the LAN interface with VLAN 500 in Unifi (this can be done now or later in the Unifi setup).\nOutbound NAT Rules Go to Firewall \u0026gt; NAT and select the \u0026ldquo;Outbound\u0026rdquo; tab.\nChange the outbound NAT mode to Hybrid and click Save.\nCreate rules to direct traffic to the VPN, which will be the subnet created in Unifi (e.g., 10.99.2.0/24 in my case). Create the following rules (refer to the image below): Localhost to PIA rule, ISAKMP Localhost to PIA rule, LAN (Subnet) to PIA rule, ISAKMP LAN (Subnet) to PIA rule.\nOutbound NAT Rules Configuration\nThe settings for each rule are detailed below. Replace the placeholder values with your specific configuration.\nYour \u0026ldquo;localhost\u0026rdquo; and \u0026ldquo;127.0.0.0\u0026rdquo; information will likely remain the same. Replace \u0026ldquo;PIA_CHICAGO\u0026rdquo; with your configured PIA Interface and adjust the subnet (e.g., 10.99.2.0/24) accordingly.\nISAKMP - localhost to WAN\nlocalhost to WAN\nISAKMP - LAN (Subnet) to WAN\nLAN to WAN\nThis completes the pfSense setup. Let\u0026rsquo;s move on to the Unifi configuration.\nUnifi Setup and Configuration In the Unifi Settings page, create two Corporate LAN networks: one for the pfSense WAN and the other for the pfSense LAN. The subnets should match those configured in pfSense (as shown in the image below).\npfSense WAN on Unifi\nCreate a new Corporate LAN for your pfSense WAN in Unifi, matching the options as shown in the image below. Ensure the subnet corresponds to the WAN interface subnet configured in pfSense.\npfSense LAN on Unifi\nCreate another Corporate LAN in Unifi for the pfSense LAN, using the settings shown in the image below as a guide. Configure the DHCP Gateway IP address to match the static IP address assigned to the pfSense LAN Interface (e.g., 10.99.2.15). Importantly, set the VLAN ID to the same value used in pfSense (e.g., VLAN 500).\nConnect the pfSense physical port corresponding to the LAN interface to the Unifi network and manually configure the port to use the configured VLAN.\nTagging Ports on Unifi Switches\nIn the Unifi clients page, select the client you wish to add to pfSense and go to its overview. Choose the Port where the client is connected (e.g., Family Room - UniFi Switch 8 POE-60W #7, as shown below).\nThis will open the port configuration page, allowing you to override the settings. Select the new pfSense (500) Port Profile. If the Port Profile is not listed, refer to the troubleshooting section at the end of this guide. Once applied, the device will obtain a new DHCP lease from Unifi in the pfSense subnet, routing its traffic through PIA.\nAdding Wireless Clients to pfSense on Unifi APs\nGo to Settings \u0026gt; Wireless Networks and click \u0026ldquo;CREATE NEW WIRELESS NETWORK.\u0026rdquo; Configure the new wireless network according to the image below, ensuring that you select pfSense as the Network for the WLAN. Devices connecting to this wireless network will have their traffic routed through PIA. You can verify this by visiting \u0026ldquo;whatismyip.com\u0026rdquo; from a device connected to this WLAN.\nCongratulations on successfully configuring your network!\nTROUBLESHOOTING Adding Missing Port Profiles\nIf the Port Profile is not automatically created, navigate to Settings \u0026gt; Profiles in the Unifi dashboard and switch from RADIUS to SWITCH PORTS. Click \u0026ldquo;ADD NEW PORT PROFILE,\u0026rdquo; name it, and then select \u0026ldquo;Native Network.\u0026rdquo; Assign the pfSense network to this profile, as shown below, and save the configuration. You can now return to the port assignment section and assign ports to the pfSense VPN using this new profile.\n","date":"2023-06-14T13:18:05Z","image":"https://raw.githubusercontent.com/Nexus-Security/image-1/main/how-to-use-pfsense-and-unifi-to-anonymize-and-encrypt-vlan-tagged-traffic.png","permalink":"https://Nexus-Security.github.io/using-pfsense-and-unifi-to-anonymize-and-encrypt-vlan-tagged-traffic-a-step-by-step-guide/","title":"Using pfSense and Unifi to anonymize and encrypt VLAN tagged traffic: A step-by-step guide"},{"content":"W3RDW Blog - First Post This is the first post from my HAM radio hobby site, https://w3rdw.radio. From now on, I will be posting HAM radio content only on the W3RDW site. You can find a link on my WhiteMatterTech site.\nCreating an ADS-B Flight Tracker Using Affordable Equipment and Docker on Ubuntu This guide will demonstrate how to set up an ADS-B flight tracker using an inexpensive antenna, Docker, and Ubuntu. ADS-B, which stands for Automatic Dependent Surveillance – Broadcast, is a technology that enables aircraft to determine their location using satellite navigation and transmit this information periodically. By establishing your own ADS-B receiver, you can monitor flights in your vicinity and contribute to the global flight tracking network.\nMy setup consists of an ESXi host with an Ubuntu 21.04 VM. The recommended USB dongle is passed through to this VM. While a deep understanding of Docker isn\u0026rsquo;t mandatory, it can be helpful for addressing potential USB issues and making custom configurations. If you need help installing Docker, refer to this Docker installation tutorial and this Docker Compose installation tutorial.\nPrerequisites To get started, you\u0026rsquo;ll need an RTL SDR USB dongle, an ADS-B antenna, and an Ubuntu machine (either physical or virtual). The SMA extension cable is optional but can be useful for extending the antenna\u0026rsquo;s reach. Here are some recommended components:\nPlease note that I may earn a commission for purchases made through the affiliate links below, which helps support this blog. The price remains the same for you, regardless of whether you use my links or not. Thank you for your support!\nRTL SDR (Kit) RTL SDR USB Dongle (Standalone) Optional SMA Extension Cable Recommended ADS-B Antenna Step 1: Hardware Setup Setting up the hardware is simple.\nConnect the ADS-B antenna to the RTL SDR USB dongle, using an extension cable if needed. Connect the RTL SDR USB dongle to your Ubuntu machine. Verify that your system recognizes the USB dongle. Step 2: Configuring Environment Variables Create a .env file in your project directory and configure the following environment variables with your specific information:\n1 touch .env 1 2 3 4 5 6 7 8 9 FEEDER_TZ=America/New_York ADSB_SDR_SERIAL=0 ADSB_SDR_PPM=001 FEEDER_LAT=your_latitude FEEDER_LONG=your_longitude FEEDER_ALT_M=altitude_in_meters FEEDER_ALT_FT=altitude_in_feet FEEDER_NAME=whatever_you_want FR24_SHARING_KEY=sharingkey_from_flightradar24.com Step 3: Defining the Docker Compose Configuration We\u0026rsquo;ll create a docker-compose.yml file to define the services for our ADS-B flight tracker. This file will include readsb for data collection, fr24 for Flightradar24 integration, tar1090 for web-based visualization, and watchtower for automated updates.\nreadsb: Collects data from the ADS-B receiver and makes it available for processing, visualization, and integration. fr24: Integrates your ADS-B data with Flightradar24, a widely used flight tracking service. Sharing your data with Flightradar24 allows you to contribute to their network and access additional features. tar1090: A web-based tool for visualizing ADS-B data, providing a user-friendly interface to track aircraft, view flight paths, and explore flight information. watchtower: An automated update tool for Docker containers that ensures your ADS-B flight tracker components are always current by automatically downloading the latest Docker images and restarting the services. Create the docker-compose.yml file in your project directory:\n1 touch docker-compose.yml Paste the following configuration into the docker-compose.yml file:\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 version: \u0026#39;3.8\u0026#39; volumes: readsbpb_rrd: readsbpb_autogain: services: readsb: image: ghcr.io/sdr-enthusiasts/docker-readsb-protobuf:latest tty: true container_name: readsb hostname: readsb restart: always devices: - /dev/bus/usb:/dev/bus/usb ports: - 8080:8080 environment: - TZ=${FEEDER_TZ} - READSB_DEVICE_TYPE=rtlsdr - READSB_RTLSDR_DEVICE=0 - READSB_GAIN=autogain - READSB_LAT=${FEEDER_LAT} - READSB_LON=${FEEDER_LONG} - READSB_RX_LOCATION_ACCURACY=2 - READSB_STATS_RANGE=true - READSB_NET_ENABLE=true volumes: - readsbpb_rrd:/run/collectd - readsbpb_autogain:/run/autogain tmpfs: - /run/readsb - /var/log fr24: image: ghcr.io/sdr-enthusiasts/docker-flightradar24:latest tty: true container_name: fr24 restart: always depends_on: - readsb ports: - 8754:8754 environment: - BEASTHOST=readsb - TZ=${FEEDER_TZ} - FR24KEY=${FR24_SHARING_KEY} tmpfs: - /var/log tar1090: image: mikenye/tar1090:latest tty: true container_name: tar1090 restart: always depends_on: - readsb environment: - TZ=${FEEDER_TZ} - BEASTHOST=readsb - MLATHOST=mlathub - LAT=${FEEDER_LAT} - LONG=${FEEDER_LONG} - GRAPHS1090_DARKMODE=true tmpfs: - /run:exec,size=64M - /var/log:size=32M ports: - 8078:80 watchtower: image: containrrr/watchtower:latest tty: true container_name: watchtower restart: always environment: - TZ=${FEEDER_TZ} - WATCHTOWER_CLEANUP=true - WATCHTOWER_POLL_INTERVAL=86400 - WATCHTOWER_ROLLING_RESTART=true volumes: - /var/run/docker.sock:/var/run/docker.sock You should now have a .env file and a docker-compose.yml file in your directory, ready to launch the services.\nStep 4: Launching the ADS-B Flight Tracker Open your terminal, navigate to the directory containing the docker-compose.yml file, and run the following command to start the ADS-B flight tracker:\n1 docker-compose up -d This command downloads the required Docker images and starts the services defined in your docker-compose.yml file. Your ADS-B flight tracker will then begin receiving and processing data from aircraft in your area.\nStep 5: Accessing the Web Interface Once the services are operational, you can access the web interfaces:\nADSB Data: http://localhost:8080 (using readsb) Flightradar24 Integration: http://localhost:8754 These interfaces display real-time data about nearby aircraft, including flight paths and other pertinent information.\nTo make these resources accessible on the web, consider setting up a CloudFlare tunnel for secure tunneling. You can find a guide on how to do this here: https://nexus-security.github.io/using-cloudflared-for-tunneling-to-internal-resources/.These tutorial can help you securely access your ADS-B data both internally and externally, just like I do: https://adsb.w3rdw.radio.\nConclusion You\u0026rsquo;ve successfully set up an ADS-B flight tracker on Ubuntu using affordable equipment and Docker. This setup allows you to track flights and contribute to the global ADS-B network. The web interfaces provide real-time flight data, paths, and other valuable information about air traffic in your region. Feel free to explore additional features and customize your setup to enhance your ADS-B flight tracking experience. Happy tracking!\n","date":"2023-04-27T17:59:43Z","image":"https://raw.githubusercontent.com/Nexus-Security/image-1/main/adsb.png","permalink":"https://Nexus-Security.github.io/automatic-dependent-surveillance-broadcast-adsb/","title":"Automatic Dependent Surveillance-Broadcast (ADSB)"},{"content":"This guide explains how to set up a Tails OS virtual machine (VM) within Unraid, enabling the persistent storage feature.\nImportant Considerations Before Starting While running Tails in a VM isn\u0026rsquo;t generally advised due to potential security compromises, certain situations might justify it. However, it\u0026rsquo;s crucial to understand the security implications involved. Virtualization necessitates trusting the hypervisor, which possesses elevated privileges that could impact the security and privacy of your VM.\nFor a comprehensive understanding of virtualization-related security concerns, refer to Tails\u0026rsquo; official documentation: https://tails.boum.org/doc/advanced_topics/virtualization/\nSituations where this setup might be advantageous include tasks requiring heightened anonymity. Examples include managing cryptocurrency wallets or minimizing your digital footprint across the internet.\nCreating a Bootable Tails USB Drive The first step involves creating a bootable Tails USB drive. This drive needs to be connected to your Unraid server whenever you intend to use the Tails VM. If persistence isn\u0026rsquo;t a requirement, you can bypass this step.\nDetailed instructions on creating a bootable Tails USB drive can be found on the official Tails website: https://tails.boum.org/install/index.en.html\nThis guide utilizes a Sony USB drive, specifically chosen from the list of recommended drives by Unraid developers (https://wiki.unraid.net/USB_Flash_Drive_Preparation). Additionally, this guide uses Tails version 4.26.\nConnecting the USB Drive to Unraid After connecting the USB drive to your Unraid server, access your Unraid server through SSH or the Unraid GUI\u0026rsquo;s terminal. Execute the command ls -lha /dev/disk/by-id/ to list connected drives.\nLocate your USB drive\u0026rsquo;s full path from the output, ensuring you select the entry without \u0026ldquo;-PART\u0026rdquo; or similar suffixes. It should resemble: /dev/disk/by-id/usb-_USB_DISK_3.0_xxxxxxxxxxxxxxxxxx-0:0.\nIn cases where multiple USB drives are connected (including the one Unraid boots from), use contextual clues like brand names or unplug the Tails USB temporarily to pinpoint the correct drive.\nCreating the Tails VM Navigate to Unraid\u0026rsquo;s VM manager and select \u0026ldquo;ADD VM.\u0026rdquo; Choose \u0026ldquo;Linux\u0026rdquo; as the operating system. Configure the core distribution and RAM allocation according to your needs. Set the \u0026ldquo;Machine\u0026rdquo; to \u0026ldquo;i440fx-6.1\u0026rdquo;, \u0026ldquo;Bios\u0026rdquo; to \u0026ldquo;SeaBIOS\u0026rdquo;, and \u0026ldquo;USB Controller\u0026rdquo; to \u0026ldquo;2.0 (EHCI).\u0026rdquo;\nFor the \u0026ldquo;Primary vDisk Location,\u0026rdquo; choose \u0026ldquo;Manual\u0026rdquo; and input the USB path identified earlier (e.g., /dev/disk/by-id/usb-_USB_DISK_3.0_xxxxxxxxxxxxxxxxxx-0:0). The \u0026ldquo;2nd vDisk Location\u0026rdquo; represents your persistent storage disk; either let Unraid manage it (\u0026ldquo;Auto\u0026rdquo;) or define it yourself. Set the \u0026ldquo;2nd vDisk BUS\u0026rdquo; to \u0026ldquo;VirtIO.\u0026rdquo; For networking, select a suitable Network Bridge. Routing this bridge through a VPN for enhanced anonymity is recommended. Save the VM configuration, but do not start it yet.\nLaunching the VM From Unraid\u0026rsquo;s VM Manager, start your newly created VM. As soon as it becomes available, swiftly select \u0026ldquo;VNC Remote.\u0026rdquo; Once the VM starts loading, press the \u0026ldquo;Tab\u0026rdquo; key to access boot options.\nIn the boot options menu, delete the \u0026ldquo;live-media=removable\u0026rdquo; and \u0026ldquo;nopersistence\u0026rdquo; options using the arrow keys, Backspace, or Delete. This step needs to be repeated every time the VM restarts. While permanent solutions likely exist, they are beyond this guide\u0026rsquo;s scope.\nAfter removing the boot options, press \u0026ldquo;Enter\u0026rdquo; twice to boot into Tails.\nSetting Up Persistence On the Tails welcome screen, choose \u0026ldquo;Start Tails,\u0026rdquo; followed by \u0026ldquo;Applications\u0026rdquo; \u0026gt; \u0026ldquo;Configure persistent volume.\u0026rdquo; The 2nd vDisk you defined earlier should be automatically selected. Proceed with the on-screen instructions to format the volume, create a strong encryption password, and save your settings.\nRestart Tails, ensuring you repeat the steps to remove the specific boot options as outlined earlier. Upon successful boot, the welcome screen will now feature an \u0026ldquo;Encrypted Persistent Storage\u0026rdquo; section. Enter your encryption password and click \u0026ldquo;Unlock\u0026rdquo; to access your persistent Tails environment within Unraid.\nConclusion With these steps, you\u0026rsquo;ve successfully configured a Tails VM on your Unraid server with encrypted persistent storage. Remember that while this setup offers enhanced privacy and anonymity, it\u0026rsquo;s crucial to be aware of the security considerations inherent in running Tails within a virtualized environment.\n","date":"2023-04-10T00:30:31Z","image":"https://raw.githubusercontent.com/Nexus-Security/image-1/main/tailsunraid.png","permalink":"https://Nexus-Security.github.io/running-tails-os-with-encrypted-persistence-on-unraid-as-a-virtual-machine/","title":"Running Tails OS with Encrypted Persistence on Unraid as a virtual machine"},{"content":"Leveraging NTLM Hash Lookup for Enhanced Security In the ever-evolving cybersecurity landscape, staying ahead of threats is paramount. NTLM hash lookup services provide a powerful mechanism to achieve this. This post will explore how to leverage a bulk check API for efficient analysis of multiple NTLM hashes, bolstering security posture and penetration testing endeavors.\nUnderstanding NTLM Hashes NTLM, short for NT LAN Manager, is a suite of Microsoft security protocols ensuring authentication and data protection. An NTLM hash represents a transformed version of a user\u0026rsquo;s password. Security professionals analyze these hashes to uncover potential weaknesses and compromised credentials.\nObtaining NTLM Hashes While outside the scope of this post, pentesters employ various methods to retrieve NTLM hashes. These include techniques like local access and memory dumps, pass-the-hash attacks, phishing campaigns, credential harvesting, man-in-the-middle attacks, Active Directory enumeration, and exploiting system vulnerabilities.\nIntroducing the NTLM Hash Lookup Bulk Check API This post focuses on a novel NTLM hash lookup bulk check API, a service capable of analyzing numerous NTLM hashes concurrently. This particular service was highlighted in a recent post on X, linked here. This API streamlines the conversion of hashes to plaintext, enhancing security and pentesting effectiveness.\nNotable Features: Bulk Processing: Analyze up to 100 hashes per request. Efficiency: Swiftly identify uncached or compromised hashes. Integration: Seamlessly integrate with diverse programming languages and tools. Getting Started with the API 1. Preparing Your Hash List Start by creating a text file (your-text-file-with-hashes.txt) containing the list of NTLM hashes, each on a separate line.\n2. Using PowerShell for API Interaction Execute the following PowerShell command, replacing [your-text-file-with-hashes.txt] with your file\u0026rsquo;s path, to send your hash list for analysis:\n1 Invoke-WebRequest -Method Post -Infile [your-text-file-with-hashes.txt] https://ntlm.pw/api/bulklookup | Select-Object -Expand Content 3. Utilizing Curl for API Interaction Alternatively, use the following curl command:\n1 curl -X POST -H \u0026#34;Content-Type: text/plain\u0026#34; --data-binary \u0026#34;@[your-text-file-with-hashes.txt]\u0026#34; https://ntlm.pw/api/bulklookup Interpreting API Responses The API will process submitted hashes and return a corresponding response. Analyze the HTTP status codes and content to understand the outcome.\nSuccessful Lookup: The response will include a list of identified hashes with their respective details. Insufficient Points: A 429 error suggests a lack of points to process the request, assuming all hashes were uncached. Waiting for approximately 15 minutes should replenish the points for subsequent requests. Conclusion Integrating an NTLM hash lookup bulk check API into your cybersecurity arsenal can significantly bolster threat detection and penetration testing efforts. By converting multiple hashes to plaintext efficiently, security professionals can proactively uncover vulnerabilities, prevent unauthorized access, and protect critical assets.\n","date":"2023-03-14T22:29:06Z","image":"https://raw.githubusercontent.com/Nexus-Security/image-1/main/cli.png","permalink":"https://Nexus-Security.github.io/introducing-a-new-api-for-bulk-checking-ntlm-hash-lookups/","title":"Introducing a new API for bulk checking NTLM hash lookups"},{"content":"A Little Background and Some Thoughts Modern technology exposes us to a constant stream of High Energy Visible (HEV) blue light from our devices, coupled with irregular lighting patterns in our homes and workplaces. Our internal clocks, known as circadian rhythms, are essential to human health and well-being. For a deeper understanding of circadian rhythms and their significance, the NIH provides a comprehensive fact sheet. This post aims to demonstrate how we can leverage home automation and open-source tools to align our living spaces with these natural rhythms.\nBy programming our homes and lighting to mimic natural light patterns, we can utilize technology to create a healthier environment. This is particularly crucial given the disruptive effects of HEV blue light and inconsistent lighting. Implementing this type of home automation can significantly benefit both mental and physical well-being, particularly for families.\nExpectant mothers, for instance, can experience negative consequences for their offspring due to circadian rhythm disruptions. Studies indicate that simple lifestyle changes, such as adjusting sleep schedules, can positively influence both mother and child during the postpartum period. Extensive research highlights the positive link between aligning with natural inclinations and overall health. Given our evolution in a world governed by the sun\u0026rsquo;s cycle, syncing with natural rhythms often proves beneficial. Circadian lighting helps achieve this balance, even for those with unconventional schedules. Let\u0026rsquo;s explore how to set up this automation for you and your loved ones.\nGetting Started This automation relies on Home Assistant, a platform designed to manage various smart home devices from a central interface. You\u0026rsquo;ll need compatible smart lights ( Philips Hue with a Hue Bridge is recommended), a device to host Home Assistant (such as a Raspberry Pi, Docker container, NAS, or PC), and some patience for the initial setup.\nDisclosure: As an Amazon Associate, I earn from qualifying purchases. Rest assured, pricing remains consistent whether you use my links or not. Thank you for your support!\nInstalling Home Assistant\nThe web offers numerous tutorials for installing and setting up Home Assistant. The official Home Assistant Documentation is a great place to start.\nConfiguring Home Assistant and Installing Circadian Lighting\nBegin by downloading the Circadian Lighting plugin. Choose Option 1 for Philips Hue with a Hue bridge or Option 2 for other smart lights. Click the green \u0026ldquo;Code\u0026rdquo; button on the chosen link and select \u0026ldquo;Download Zip.\u0026rdquo; Extract the Zip file\u0026rsquo;s contents using your preferred method.\nWithin your Home Assistant configuration root folder, create a folder named \u0026ldquo;custom_components\u0026rdquo; (without quotes). This should be in the same directory as the configuration.yaml file. Inside custom_components, create a folder called \u0026ldquo;circadian_lighting\u0026rdquo; (without quotes). Move the extracted Zip file contents (___init___.py, manifest.json, sensor.py, services.yaml, and switch.py) into this folder. The final folder structure should resemble: config/custom_components/circadian_lighting/.\nFolder Structure Setup\nAdd the following lines to your configuration.yaml file:\n1 2 # Example configuration.yaml entry circadian_lighting: Create a switch within Home Assistant to control Circadian Lighting and allow for manual adjustments. For example:\n1 2 3 4 5 6 # Example configuration.yaml entry switch: - platform: circadian_lighting lights_ct: - light.desk - light.lamp You can create multiple switches to control Circadian Lighting in different rooms independently. Add lights to each switch using their entity IDs from Home Assistant. A personal example showcasing multiple switches is available on this Home Assistant GitHub repo.\nHue Specific Setup (Option 1)\nOption 1 enables you to configure room-specific scenes for your Hue switches, allowing them to activate lights with appropriate Circadian Lighting settings. For each room where you\u0026rsquo;d like to use a Hue switch/dimmer to activate a Circadian Light scene, create a scene named \u0026ldquo;Circadian\u0026rdquo; (with a capital \u0026ldquo;C\u0026rdquo;) within that room. The iConnectHue app on iPhone is a reliable option for creating these scenes. Other apps may not function correctly.\nWithin the chosen room, like the kitchen, add a scene including the bulbs you want to control with your switch. Color and brightness values don\u0026rsquo;t matter as the plugin will overwrite them. After creating the Circadian scene, assign it to your switch (refer to the screenshots below for a visual guide using iConnectHue).\nKitchen Lights Example\nCreated Scene Example\nOnce the scene is set up (only for Option 1), your setup is complete! Option 2 users will also be finished at this point. Restart your Home Assistant instance. Your Circadian Lights will now adjust according to natural light patterns. The \u0026ldquo;Issues\u0026rdquo; section on GitHub provides advanced setup ideas.\nThank you for reading!\nNote: When launching the plugin for the first time, you might need to manually turn on the Switch(es) through your Home Assistant Dashboard. Add entities to your home page, as shown in the image below, and toggle the switch to activate Circadian Lighting for those lights.\nCL Switches on Home Assistant Dashboard\n","date":"2023-03-02T08:08:04Z","image":"https://raw.githubusercontent.com/Nexus-Security/image-1/main/live-better-with-circadian-lighting.jpg","permalink":"https://Nexus-Security.github.io/improve-your-life-with-circadian-lighting/","title":"Improve Your Life with Circadian Lighting"},{"content":"Overview This post aims to provide tips for addressing network security concerns when hosting a publicly accessible web server from your home network, specifically using Unifi networking equipment. While screenshots will feature the Unifi Dream Machine Pro (UDMP), the concepts explained can be adapted to other networking gear.\nDisclosure: As an Amazon Associate, I earn from qualifying purchases. This does not affect pricing for you.\nStrong Passwords/Encryption Keys Using strong passwords and encryption keys for all services, operating systems, logins, databases, etc., is crucial. A password manager (like 1password.com) can simplify generating and securely storing these complex credentials. Strong passwords are essential as they can mitigate security risks even if a hacker breaches your network. For example, if your WordPress database has a strong encryption key, it will take an impractical amount of time for a hacker to brute-force it, rendering access to the network practically useless. This principle applies to operating system root user passwords, application login credentials, database encryption keys, and any resource secured with authentication. The goal is to prevent unauthorized network access, but if that happens, having strong, unique passwords and encryption keys acts as a significant line of defense.\nCreating an Isolated \u0026ldquo;DMZ\u0026rdquo;-like Network DMZ, which stands for \u0026ldquo;demilitarized zone,\u0026rdquo; is an older networking concept where less secure servers and resources are grouped, hoping to prevent hackers from pivoting to the internal network. Although not as common today, the term is still relevant, particularly in certifications like CompTIA Security+. This post utilizes a similar concept by creating isolated subnets with firewall rules on the router to prevent any traffic from the \u0026ldquo;DMZ\u0026rdquo; network to internal networks.\nNote: The screenshots are from the older Unifi settings page, accessible by disabling the new interface in Settings \u0026gt; System Settings.\nStart by creating a new Corporate LAN in Unifi (Settings \u0026gt; Networks) and name it (e.g., \u0026ldquo;DMZ\u0026rdquo;). Assign a VLAN to this subnet (e.g., VLAN 777). This process might differ slightly for other networking gear, but the objective is to establish an isolated subnet.\nNext, assign devices to this new network. In Unifi, this is done by configuring the port profile for the specific port connected to your web host. Other networking devices should have similar settings.\nConfiguring Firewall Rules\nTo set firewall rules, navigate to Settings \u0026gt; Routing \u0026amp; Firewall \u0026gt; Firewall and select LAN IN. Create a new rule with a descriptive name (e.g., Block DMZ -\u0026gt; IoT) and ensure it is Enabled and applied \u0026ldquo;Before predefined rules\u0026rdquo;. Set the Action to \u0026ldquo;Drop\u0026rdquo; and choose \u0026ldquo;All\u0026rdquo; for the IPv4 Protocol. Under Advanced, select all states (New, Established, Invalid, Related) and leave the IPsec option unchanged. For the Source, choose \u0026ldquo;Network\u0026rdquo; as the \u0026ldquo;Source Type\u0026rdquo; and select your DMZ network. Similarly, for Destination, choose \u0026ldquo;Network\u0026rdquo; and select your internal network. This will block all IPv4 traffic from your DMZ network to your internal network, regardless of its state. Keep in mind this also blocks access to your WebHost from your internal networks, including SSH and RDP. If you need to allow specific protocols, see the next section.\nRepeat this rule creation for each internal network defined as a Corporate LAN in Unifi if you have multiple internal networks.\nAllowing Specific Traffic From DMZ to Internal Network Proceed with caution as any adjustments here can introduce security risks. Explore all other options before making these changes to prioritize the protection of your internal resources.\nAdding Firewall Rules\nIf specific protocols need to be open between your WebHost and internal network, modify the firewall rules. The changes include: Action = Accept, Invalid = Unchecked, Source = Address/Port Group.\nFor the Source, it\u0026rsquo;s recommended to specify the WebHost requiring access. Create an IPv4 Address Group with a descriptive name (e.g., WebHost) and add the static IP of the WebHost under \u0026ldquo;Address\u0026rdquo;. Your WebHost should have a static IP; configure it before proceeding if not already done.\nNext, create a Port Group under Source and add the ports needing access from your internal network (e.g., SSH port 22 and RDP port 3389). Under Destination, you can either specify the networks/devices that require access or allow access from any internal network by leaving both boxes set to \u0026ldquo;Any\u0026rdquo;.\nEnable IPS/IDS The Unifi Dream Machine Pro offers excellent security features, including IPS/IDS found under Settings \u0026gt; Threat Management (old GUI) or Settings \u0026gt; Security \u0026gt; Internet Threat Management (new GUI). IPS (Intrusion Prevention System) actively monitors traffic and blocks potentially malicious activity based on defined rules, while IDS (Intrusion Detection System) passively monitors traffic and logs/alerts suspicious activity.\nConsider IPS as an essential security measure for protecting your home network while web hosting. While other hardware or software options might provide this service, Unifi\u0026rsquo;s solution is explored here.\nWithin the IPS/IDS settings, choosing IPS for your Protection Mode is recommended. Be aware that enabling IPS might impact your overall download speed if you\u0026rsquo;re not using a Unifi Dream Machine Pro. The device will indicate the maximum throughput with IPS enabled. The Unifi Dream Machine Pro supports up to 3.5Gbps throughput with IPS enabled.\nWrapping Up Network security, especially when hosting a web server on your home network, is paramount. Implementing the outlined security measures, including setting up an isolated network and enabling IPS on your Unifi Dream Machine Pro (or similar Unifi Gateway device), is highly recommended.\n","date":"2023-02-16T23:53:21Z","image":"https://raw.githubusercontent.com/Nexus-Security/image-1/main/network-hardening-webhosting.jpg","permalink":"https://Nexus-Security.github.io/tips-for-strengthening-your-network-security-for-hosting-websites-at-home/","title":"Tips for Strengthening Your Network Security for Hosting Websites at Home"},{"content":"Introduction I recently worked with a university research lab to create vapor sensors that could provide approximate measurements of ethanol vapor levels in an operant chamber. This project was quite engaging.\nMy lack of experience with Arduino and the limited circuit documentation online led to some challenges. Despite this project being outside my usual area of expertise, I persisted, and the end product turned out well.\nThis project\u0026rsquo;s open-source nature stems from the work of Dr. Brian McCool. He developed the Arduino code (available on GitHub), the general project design and suggested parts list, and the concept of using this project in alcohol research.\nThe Lab This project was developed for Dr. Anna Radke\u0026rsquo;s Reward and Addictive Disorders (RAD) Lab at Miami University.\nThese sensors will be used in an ongoing experiment involving mouse operant chambers. They will provide a visual representation of ethanol vapor pressure within these chambers. This experiment, utilizing these modules, is overseen by Elizabeth Sneddon, who was a PhD candidate at the time and a recipient of the prestigious DSPAN F99/K00 grant from NINDS.\nTo learn more about Dr. Radke, her team, and their projects, I encourage you to visit the RAD Lab\u0026rsquo;s website.\nSide note: I\u0026rsquo;d also like to highlight a research paper from the RAD Lab that I co-authored, which can be found on PubMed.\nFirst Successful Unit\nThe Parts Per Module Quick List: Arduino Uno R3 Arduino Proto Shield for Arduino Kit (Stackable) Clear Enclosure for Arduino (adafruit.com) Standard LCD 16x2 screen (options available on adafruit.com and Amazon) i2c/SPI character LCD backpack MQ-3 alcohol gas sensor (sparkfun.com) Power supply (see notes below) Standard USB printer cable 500Ohm Trim Potentiometer (rat operant boxes); 1k-2kOhm Trim Potentiometer (mouse operant boxes) - a kit containing both is recommended Wiring: a kit with various wire types is suggested Optional: Breadboard (helpful for testing but not mandatory) Optional: Soldering iron kit (required for soldering) and solder sucker kit (recommended for fixing mistakes) Power supply notes: While a 5V 2A power supply might be considered, I encountered issues with voltage drops. Using a standard USB printer cable with a USB power block (minimum 5V and 2A) is highly recommended. A USB printer cable is also necessary for program transfer to the Arduino.\nThis list is adapted from Dr. Brian McCool\u0026rsquo;s recommendations.\nDisclosure: As an Amazon Associate, I earn from qualifying purchases. Your support through these links helps maintain this blog, and the pricing remains the same for you. Thank you!\nThe Circuit The circuit diagram below, drawn by my father (a radio and broadcast engineer), illustrates the project\u0026rsquo;s wiring. It can be a valuable resource for anyone building this project. Drawn by RW 09/04/2021\nArduino with MQ-3B Sensor Module Wiring Diagram\nAccess the diagram here: https://drive.google.com/file/d/1gVgyCg9lm8LaurHVQtRFqXGifgDlCiEb/view?usp=sharing\nAssembling the Arduino The following steps outline the assembly process, though the order can be adjusted as needed.\nThis was my first Arduino project, so I am by no means an expert. While I found the process enjoyable overall, there were moments of frustration. Unfortunately, I didn\u0026rsquo;t document the process as thoroughly as I should have. I\u0026rsquo;ll aim to provide more comprehensive visuals if I have the opportunity to build more units in the future.\nPreparing the Protoshield and Connecting to the Mainboard Assembling the Protoshield requires soldering. The components included with the Protoshield should be arranged as shown in the image below. Pay close attention to the placement of capacitors, resistors, switches/buttons, and LEDs. Soldering should be done from the board\u0026rsquo;s underside.\nAlign and solder the black pinouts as illustrated in the image above, also from the bottom (see image below for clarity). These will be used to connect the Protoshield to the Mainboard by inserting the pins into the corresponding pinouts.\nYour assembled Protoshield and Mainboard should resemble the first image.\nConnect the LCD Backpack to the LCD Screen The backpack connects to the screen using the 16-pin connector. Solder all 16 pins to the front of the screen using the short ends of the connectors (see image below).\nTip: Attach the 5-screw pins to the backpack before soldering it to the screen. Adding them afterward is considerably difficult.\nFlip the assembly over and solder all 16 pins of the backpack as depicted in the image below.\nThe GND, 5V, CLK, and DAT ports are labeled as the READ-OUT BOARD on the diagram.\nWire the Protoshield While I don\u0026rsquo;t have many detailed photos of the wired Protoshield, the image below demonstrates how I connected the 5V and Ground from the Mainboard to their central counterparts on the Protoshield. This allows for easy access when soldering the remaining required 5V and Ground wires.\nUsing a breadboard during the wiring process is highly recommended. As shown in the image below, a breadboard allows you to test your connections before making them permanent. While some components will require soldering (e.g., screen, backpack), it\u0026rsquo;s beneficial to test your wiring on the breadboard first.\nSuccessful Test Readout with Breadboard\nCleaned Up Wiring\nProgramming Download the code provided by Dr. McCool on GitHub. For instructions on uploading the code to your Arduino, refer to this helpful post. Once the connections are made as described and the program is uploaded, your screen should display the voltage reading.\nFine-Tuning the Read-Out The backpack includes a small, circular \u0026ldquo;Contrast\u0026rdquo; potentiometer near the top-right corner. If your screen illuminates but doesn\u0026rsquo;t show a reading, adjust the contrast using this potentiometer. I found it necessary to fine-tune the contrast for each module I built.\nCaution: The potentiometer is fragile and easily damaged, as illustrated in the image below. Exercise caution when adjusting it to avoid breakage. My broken potentiometer required rewiring the module with a new external potentiometer for contrast control. While I don\u0026rsquo;t currently have a wiring diagram for this situation, I recommend replacing the backpack if you encounter this issue.\nBroken Backpack Potentiometer\nWrapping Up Congratulations! I hope this tutorial was beneficial in guiding you through the process of building your own sensor module. I trust that this information will be valuable for both your research and personal endeavors.\nFirst Successful Unit\nFirst Completed Unit\nUndershot of a Successful Build\n","date":"2023-01-17T12:45:04Z","image":"https://raw.githubusercontent.com/Nexus-Security/image-1/main/IMG_0825-min-768x1024.jpeg","permalink":"https://Nexus-Security.github.io/investigating-ethanol-levels-with-the-arduino-mq-3b-sensor-in-behavioral-neuroscience-studies/","title":"Investigating Ethanol Levels with the Arduino MQ-3B Sensor in Behavioral Neuroscience Studies"},{"content":" Introduction My previous post detailed how to connect to a Unifi router using HTTPS, eliminating the \u0026ldquo;Your connection is not private\u0026rdquo; warning. However, that method relies on an external DNS server like CloudFlare to resolve your Unifi router\u0026rsquo;s address. This introduces latency and requires opening external ports, increasing potential security risks.\nThis post presents a solution for maintaining secure, external accessibility while also enabling local HTTPS access without opening external ports. This approach is particularly well-suited for a robust homelab environment.\nThe Solution: \u0026ldquo;Split-Brain DNS\u0026rdquo; Split-Brain DNS utilizes two sets of DNS servers to accurately resolve traffic both internally and externally. This allows for the correct resolution of internal resources from both inside and outside the network.\nIn my setup, PiHole acts as the internal DNS server, handling internal DNS requests for internal resources. CloudFlare manages external resolution for these internal resources. For instance, if my local network is 10.10.10.0/24 with my Unifi Dream Machine Pro (UDMP) at 10.10.10.1 and my MacBook Pro (MBP) at 10.10.10.50, requests from the MBP to the UDMP are resolved directly by PiHole (see the first Terminal image). However, if my MBP is on a different network, the external DNS server would direct traffic to my network\u0026rsquo;s public IP, which would then be routed by the reverse proxy to reach the UDMP (see the second Terminal image).\nFirst Image: Executing dig unifi.white.fm on an internal machine using PiHole as the DNS server resolves unifi.white.fm directly to the reverse proxy within my network.\nSecond Image: Executing dig unifi.white.fm from an external machine resolves unifi.white.fm to my dynamic DNS host (DuckDNS), which points to my network\u0026rsquo;s public IP. Port forwarding rules would then direct the traffic to the reverse proxy and subsequently to the UDMP, if enabled.\nIn both scenarios, my browser establishes a secure HTTPS connection to my Unifi Machine using Let\u0026rsquo;s Encrypt certificates. Before delving into the specific setup, let\u0026rsquo;s review the tools I employ to achieve this.\nTools PiHole PiHole serves as the internal DNS server in this setup. While there are various ways to implement PiHole within a network, my configuration involves multiple instances across different subnets and VLANs for granular control over resolution.\nIt\u0026rsquo;s important to note that PiHole isn\u0026rsquo;t the only DNS server capable of achieving this. Its appeal lies in its straightforward yet powerful DNS blocking features. If network-wide tracking and ad-blocking aren\u0026rsquo;t requirements, alternative tools might be more suitable. This tutorial will utilize PiHole for demonstration purposes.\nUnnecessary Note: Each PiHole instance in my network is configured for anonymous resolution using Oblivious DNS Over HTTPS (ODOH). For containerized instances, I leverage the mschirrmeister/odoh-resolver container (for more details, see the GitHub repository).\nNginx Proxy Manager PiHole is configured to resolve internal resource requests to Nginx Proxy Manager, which handles traffic routing to the desired resource and provides HTTPS protection through Let\u0026rsquo;s Encrypt certificates. My previous post provides a guide on setting up Nginx Proxy Manager with Docker.\nSimilar to PiHole, alternative reverse proxies can achieve this outcome. I opt for Nginx Proxy Manager due to its user-friendly nature.\nInternal Routing With at least one DNS server and a reverse proxy operational, we can establish the internal component of our split-brain DNS. In this tutorial, my network is 10.99.100.0/24, with an Nginx Proxy Manager instance at 10.99.100.10 and a CyberChef Docker container at 10.99.100.82 with a GUI accessible on port 3000 (i.e., http://10.99.100.82:3000).\nNginx Proxy Manager receives client communication on ports 80 and 443, forwarding traffic to a specified host and port. For CyberChef, this would be 10.99.100.82 on port 3000. The first step is configuring Nginx Proxy Manager to route traffic for the host cyberchef.white.fm, which we will define in our DNS server. In Nginx Proxy Manager, navigate to Hosts and click Add Proxy Host. Input your chosen domain (e.g., cyberchef.white.fm). In most cases, you\u0026rsquo;ll want to retain the default Scheme setting of http, as the initial traffic isn\u0026rsquo;t encrypted. Add the host\u0026rsquo;s IP address (e.g., 10.99.100.82) and the communication port (e.g., port 3000). While enabling Block Common Exploits and Websockets Support is recommended, these options should be explored and tested based on your specific host and requirements. The image below illustrates this page configured as described.\nOn the same page, navigate to SSL and select Request a new SSL Certificate from the dropdown menu. This automatically requests a certificate from Let\u0026rsquo;s Encrypt based on the hostname (e.g., cyberchef.white.fm) provided earlier. If you possess an existing certificate or prefer a wildcard certificate for all internal resources, those options are also available. Enable the options based on your host\u0026rsquo;s requirements. The image below provides an example configuration for this screen.\nAfter the certificate request is finalized, click Save. A new Proxy Host will appear in your Nginx Proxy Manager.\nIn PiHole, select Local DNS from the left panel, then choose DNS Records. Under Domain, enter the intended domain target from Nginx Proxy Manager (e.g., cyberchef.white.fm) and the IP address of your Nginx Proxy Manager host (e.g., 10.99.100.10), which differs from the target host. Click Add. The image below depicts this configuration.\nWith these configurations saved, ensure your client utilizes PiHole (or your chosen DNS server) as its DNS server.\nIf you\u0026rsquo;re using Unifi with the latest User Interface, this setting is found under Settings \u0026gt; Networks \u0026gt; Name_of_Your_Network \u0026gt; DHCP \u0026gt; DHCP Service Management: Show options \u0026gt; DHCP DNS Server: Enable.\nEnable this setting and enter your PiHole IP(s). After saving, renew your client\u0026rsquo;s DHCP lease (or reconnect if static) to utilize PiHole as its DNS server. You can also manually assign the DNS server on your client for a more localized change. The Unifi settings are shown in the image below.\nWith the new DNS server(s) added, attempt to connect to your host via HTTPS (e.g., https://cyberchef.white.fm). A successful configuration results in the secure HTTPS connection indicator in your browser (see the image below).\nA dig command on cyberchef.white.fm will resolve to the IP address of our Nginx Proxy Manager (IP 10.99.100.10). See the image below.\nAttempting to capture data transmitted over HTTPS reveals encryption. The screenshot below shows a WireShark capture of data sent from my PC to an InfluxDB instance on my network using this method. All communication with InfluxDB is now encrypted. Various data sources, including Unifi Poller, Varken, and Home Assistant, now transmit data to InfluxDB over HTTPS as well. See the image below.\nWhile this method of implementing HTTPS support is not a comprehensive security solution, it enhances privacy and reduces the likelihood of malicious actors on your network accessing sensitive information.\nExcellent work! You\u0026rsquo;ve successfully configured the internal half of your \u0026ldquo;split-brain DNS.\u0026rdquo; Repeat these steps to add additional hosts to both Nginx Proxy Manager and PiHole. This setup extends to the Nginx Proxy Manager web page, Unifi router, PiHole web interface, ESXi servers, Proxmox servers, or any Docker containers running within your environment. The possibilities are vast.\nExternal Routing Prepare Your Domain Note: This section is an excerpt from my post: Hosting Your Own Site with Traefik and Wordpress. This outlines the external half of the \u0026ldquo;split-brain DNS\u0026rdquo; and is optional if you don\u0026rsquo;t need external access to your services and Nginx Proxy Manager.\nAdd DNS records for your domain (e.g., whitematter.tech) that point to your server\u0026rsquo;s public IP address. A Dynamic DNS service like duckdns.org is recommended for automatically updating your changing public IP address if you lack a static IP. The image below demonstrates CloudFlare pointing my domain to my DuckDNS address (replace \u0026ldquo;Content\u0026rdquo; with your public IP if not using DuckDNS).\nAdd a CNAME record for each host you want to access externally. For example, a CNAME record for cyberchef would point cyberchef.white.fm to your dynamic DNS host or public IP.\nNext, forward ports 80 and 443 to your Nginx Proxy Manager host. Consult your networking equipment\u0026rsquo;s documentation for instructions.\nOnce ports 80 and 443 are forwarded, any host added to CloudFlare (or your DNS service) will be externally accessible.\n","date":"2022-08-05T11:39:46Z","image":"https://raw.githubusercontent.com/Nexus-Security/image-1/main/https.webp","permalink":"https://Nexus-Security.github.io/internal-https-with-lets-encrypt-using-split-brain-dns/","title":"Internal HTTPS with Let's Encrypt using 'Split-Brain DNS'"},{"content":"Introduction This guide explains how to route specific network traffic through a VPN client on pfSense using Unifi\u0026rsquo;s policy-based routing. This setup allows you to manage your devices and subnets with Unifi\u0026rsquo;s Network app while benefiting from pfSense\u0026rsquo;s VPN capabilities. The result is a secure and private network experience without sacrificing speed.\nThis guide focuses solely on setting up policy-based routing and assumes you have already configured a VPN client on pfSense and segmented your Unifi network with subnets and VLANs.\nRequirements A Unifi Gateway (at least two WAN ports) An active VPN Client A pfSense router (with an unused port connected to the Unifi Gateway) Overview The final setup involves a few key components working together.\nOn pfSense, you\u0026rsquo;ll create a dedicated subnet (e.g., 10.99.3.0/24) and an interface (e.g., 10.99.3.15) to act as the gateway for Unifi\u0026rsquo;s WAN2. This pfSense subnet\u0026rsquo;s traffic will be directed through your VPN client, encrypting it before sending it out via pfSense\u0026rsquo;s WAN connection (e.g., 10.99.1.0/24 subnet on Unifi) and eventually exiting through Unifi\u0026rsquo;s WAN1 (your ISP connection).\nHere\u0026rsquo;s a simplified representation of the traffic flow:\nClient device \u0026ndash;policy-based routing\u0026ndash;\u0026gt; Unifi WAN2 (10.99.3.10) \u0026ndash;\u0026gt; pfSense subnet (10.99.3.0/24) (via pfSense Interface [10.99.3.15]) \u0026ndash;\u0026gt; VPN Gateway (traffic encrypted) \u0026ndash;\u0026gt; pfSense WAN (10.99.1.15) \u0026ndash;\u0026gt; Unifi \u0026ldquo;pfSense WAN\u0026rdquo; subnet (10.99.1.0/24) \u0026ndash;\u0026gt; Unifi WAN1 (ISP)\nPreparing pfSense This guide assumes you\u0026rsquo;ve already set up a VPN client on pfSense and configured the necessary NAT rules.\nSetting the Interface Create a new interface and subnet for Unifi\u0026rsquo;s WAN2 connection: Interfaces \u0026gt; Assignments \u0026gt; select an available Network port \u0026gt; Add. Configure the new interface: Enable the interface and assign a static IP address within a subnet that won\u0026rsquo;t clash with existing Unifi/pfSense subnets (e.g., 10.99.3.15/24). Ensure the CIDR allows communication between Unifi WAN2 and the pfSense address. Outbound NAT Rules Switch the outbound NAT mode to Hybrid: Firewall \u0026gt; NAT \u0026gt; Outbound \u0026gt; select Hybrid \u0026gt; Save. Create rules to direct the Unifi subnet traffic to the VPN. You\u0026rsquo;ll need rules for both regular traffic and ISAKMP traffic: Localhost to VPN rule, ISAKMP Localhost to VPN rule, LAN (Subnet) to VPN rule, ISAKMP LAN (Subnet) to VPN rule. Adjust the rules with your specific VPN interface, subnets, and gateway information. Firewall in pfSense Create an allow rule for the new interface: Firewall \u0026gt; Rules \u0026gt; select your new interface \u0026gt; Add \u0026gt; allow traffic from any source/port to any destination/port \u0026gt; under Advanced, set the gateway to your VPN gateway \u0026gt; Save. Create a block rule for the new subnet to prevent unencrypted traffic leaks: Firewall \u0026gt; Rules \u0026gt; select your new interface \u0026gt; Add \u0026gt; under Source, input your subnet (e.g., 10.99.3.0/24) \u0026gt; Save. Preparing Unifi Assign WAN2 Configure Unifi\u0026rsquo;s WAN2: Settings \u0026gt; Internet \u0026gt; select the second WAN option. Connect the cable from your pfSense interface to the WAN2 port. Set a static IP address within the designated subnet (e.g., 10.99.3.10) and use the pfSense static address as the gateway (e.g., 10.99.3.15) \u0026gt; Save. Policy-Based Routing Set up the routing rule: Settings \u0026gt; Traffic Management \u0026gt; Routes \u0026gt; Create New Route. Define the traffic to be routed. For instance, select your IoT subnet as the source and WAN2 as the destination. All traffic routed through WAN2 will now be encrypted through your VPN.\n","date":"2022-08-01T06:02:23Z","image":"https://raw.githubusercontent.com/Nexus-Security/image-1/main/policy-route-unifi-pfsense-pia.png","permalink":"https://Nexus-Security.github.io/utilizing-policy-based-routing-with-unifi-pia-and-pfsense-directing-my-iot-external-traffic-through-pia-vpn/","title":"Utilizing Policy Based Routing with Unifi, PIA, and pfSense: Directing My IoT External Traffic through PIA VPN"},{"content":"Introduction This tutorial explains how to run a Dockerized MacOS on Unraid using the sickcodes/Docker-OSX project. This project allows for MacOS security research within containers on Linux and Windows.\nThis tutorial focuses on utilizing Unraid\u0026rsquo;s Docker-Compose functionality for container management and building a special VNC-compatible image necessary for Unraid. If you\u0026rsquo;re not using Unraid, the default image might be a better fit.\nThe standard sickcodes/Docker-OSX image on Docker Hub relies on KVM for screen rendering. While this tutorial doesn\u0026rsquo;t offer native NoVNC support like other Unraid containers (yet), it allows connection to the container via a VNC client. A solution for native NoVNC support is currently in progress.\nThis guide offers a concise solution for getting started with a functional and maintained container.\nBuilding the VNC-compatible Image on Unraid Connect to your Unraid server through SSH or the web portal\u0026rsquo;s Terminal. Execute the following commands to download the required Dockerfile and build the image:\n1 2 3 4 5 6 mkdir /var/tmp/macos cd /var/tmp/macos wget https://raw.githubusercontent.com/sickcodes/Docker-OSX/master/vnc-version/Dockerfile docker build -t docker-osx-vnc . Initial image building attempts may encounter invalid key errors for numerous packages and prerequisites. If encountered, add the following lines to the Dockerfile after the \u0026ldquo;ARG MIRROR_COUNT=10\u0026rdquo; line:\n1 2 3 4 5 # Disable signature checks for invalid key errors RUN sudo sed -i \u0026#39;s/SigLevel = Required DatabaseOptional/SigLevel = Never/g\u0026#39; /etc/pacman.conf RUN if [[ \u0026#34;${LINUX}\u0026#34; == true ]]; then \\ sudo pacman -Syu linux libguestfs --noconfirm \\ ; fi Open the Dockerfile:\n1 nano /var/tmp/macos/Dockerfile Paste the lines, save the file (CTRL + X), then rebuild the image:\n1 docker build -t docker-osx-vnc . The image should now build without errors.\nCreating a Docker-Compose stack on Unraid Docker-Compose simplifies container management and provides granular control. This guide leverages this tool for its ease of configuration transfer and sharing capabilities.\nEnsure you have the \u0026ldquo;Docker Compose Manager\u0026rdquo; plugin (by dcflachs) installed from Unraid\u0026rsquo;s Community Applications. At the bottom of the Docker tab, select ADD NEW STACK and name it (e.g., \u0026ldquo;MacOS\u0026rdquo;). Click the gear icon next to the stack name, choose EDIT STACK, then select COMPOSE FILE. Paste the following configuration into the provided text box:\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 version: \u0026#39;3\u0026#39; services: macos: container_name: \u0026#39;macos\u0026#39; image: \u0026#39;docker-osx-vnc:monterey\u0026#39; privileged: true devices: - /dev/kvm - /dev/snd - /dev/null ports: - \u0026#39;8888:5999\u0026#39; - \u0026#39;50922:10022\u0026#39; environment: - \u0026#39;USERNAME=user\u0026#39; - \u0026#39;PASSWORD=pass\u0026#39; - \u0026#39;DISPLAY=${DISPLAY:-:0.0}\u0026#39; Save the changes and click COMPOSE UP to start the MacOS container. The new container will appear alongside your other containers in the Unraid GUI, providing access to logs and console.\nRunning the container without Docker-Compose To run the container without Docker-Compose, execute the following command via SSH or the GUI Terminal:\n1 docker run --device /dev/kvm --device /dev/snd -p 8888:5999 -p 50922:10022 -d --privileged docker-osx-vnc:latest Connecting to the MacOS Container with VNC Protocol Before connecting, obtain the generated VNC password. On Unraid, locate the macos container, click it, and select \u0026quot;Console\u0026quot;.\nIn the new Terminal window, type:\n1 cat vncpasswd_file The output is your container\u0026rsquo;s VNC password.\nWindows\nDownload and install a VNC client that supports TigerVNC (e.g., TightVNC Client). Launch the client and enter your container\u0026rsquo;s IP address followed by \u0026quot;::8888\u0026quot;. Provide the VNC password when prompted.\nMacOS\nUtilize the built-in VNC client by pressing CMD+K and entering vnc://IP_OF_MACOS_DOCKER:8888. Enter the VNC password when prompted.\n","date":"2022-07-04T20:41:21Z","image":"https://raw.githubusercontent.com/Nexus-Security/image-1/main/dockermac.jpg","permalink":"https://Nexus-Security.github.io/running-dockerized-macos-on-unraid-a-step-by-step-guide/","title":"Running Dockerized MacOS on Unraid: A Step-by-Step Guide"}]