[{"content":"PC gamers often use Discord for its superior audio quality compared to in-game chat options. However, cross-platform gaming presents a challenge, limiting users to less desirable options like in-game chat or the Xbox Companion app.\nThis guide explains how to set up audio forwarding from an Xbox party directly to Discord, and vice versa, using a Windows 10 virtual machine (VM) on Unraid. This allows for the use of Discord\u0026rsquo;s high-quality audio even when gaming with friends on Xbox.\nUsing a Virtual Audio Cable The Virtual Audio Cable (VAC) is the key to this setup. While it requires a $30 purchase, it offers a trial period with a voice reminder every 30 minutes.\nSetting it up involves creating two cables and clicking \u0026ldquo;Set,\u0026rdquo; resulting in two audio interfaces for routing audio within your operating system.\nConfiguring the Xbox Console Companion App While Microsoft is promoting their newer Xbox PC app, this setup utilizes the Xbox Console Companion App. It involves creating a dedicated Xbox Live user for hosting parties with your Xbox friends.\nWithin the app settings, set the Speaker source to \u0026ldquo;Line 1 (Virtual Audio Cable)\u0026rdquo; and the Microphone to \u0026ldquo;Line 2 (Virtual Audio Cable).\u0026rdquo;\nSetting up Discord Create a new Discord user specifically for this setup and add it to your desired server.\nIn the Voice \u0026amp; Video settings, set the Input device to \u0026ldquo;Line 1 (Virtual Audio Cable)\u0026rdquo; and the Output device to \u0026ldquo;Line 2 (Virtual Audio Cable).\u0026rdquo;\nConnecting with Friends Join your Discord server with the new user. Have your Xbox friends join the party hosted by your dedicated Xbox Live user. You should now be able to communicate with your Xbox friends through Discord.\n","date":"2024-07-11T18:12:23Z","image":"https://raw.githubusercontent.com/Nexus-Security/image-1/main/xbox2discord-how-to-forward-audio-from-xbox-live-to-discord.jpg","permalink":"https://Nexus-Security.github.io/forwarding-audio-from-xbox-live-to-discord-a-guide/","title":"Forwarding audio from Xbox Live to Discord: A guide"},{"content":"I\u0026rsquo;ve been using iA Writer to edit my MarkDown files on my iPad, and I\u0026rsquo;m really happy with how straightforward it is.\nOne of their newest features, Authorship, is a game-changer for collaborating. It lets you easily track who wrote what. Imagine having a friend or even ChatGPT help with a section of your article. This feature visually distinguishes your writing from theirs, making edits and tracking contributions a breeze. While I don\u0026rsquo;t use AI assistance much yet, this feature has piqued my interest for future projects.\nThe Catch There\u0026rsquo;s a slight hiccup with this feature. When enabled, iA Writer adds a hidden \u0026ldquo;Authorship\u0026rdquo; section at the end of your document. It\u0026rsquo;s invisible within the iA Writer interface but pops up in other editors.\nThis presents a problem for my workflow. I write my blog posts in iA Writer, push them to GitHub using Working Copy, and finally, Hugo transforms them into my static website. If I don\u0026rsquo;t manually remove this hidden section, Hugo (or similar static site generators) will unknowingly publish it on my website – definitely not ideal!\nExploring Solutions I reached out to the iA team on Mastodon about this issue, and they told me, it turns out there\u0026rsquo;s no one-size-fits-all solution.\nWhile I understand it\u0026rsquo;s a tricky situation, I believe there\u0026rsquo;s room for improvement. Here are a couple of thoughts:\nExternal File Storage Storing this authorship data in a separate file would simplify its exclusion during website generation. However, this could lead to data management challenges by scattering information.\nLeveraging YAML Metadata Integrating the Authorship data as metadata within the existing YAML structure could be a winning solution. iA Writer users are already accustomed to YAML metadata, and static site generators typically disregard unfamiliar metadata, preventing unintended publishing.\nA Temporary Fix for Hugo Users As we await potential solutions from iA, I\u0026rsquo;ve created a script to remove these Authorship blocks from your MarkDown files:\nKeep in mind that this will remove all Authorship blocks, even those within code blocks. Feel free to refine it further.\nIn Conclusion iA Writer\u0026rsquo;s Authorship feature is a fantastic addition for managing collaborative writing. However, its integration with static site generators requires attention.\nI encourage fellow users to provide constructive feedback to iA at support+newfeature@ia.net.\nFor those familiar with GitHub, you can directly contribute to the discussion on their repository: repository.\n","date":"2024-06-25T03:23:00.174+01:00","image":"https://raw.githubusercontent.com/Nexus-Security/image-1/main/2024227/ia-writer-authorship-cover.webp","permalink":"https://Nexus-Security.github.io/authorship-in-ia-writer-a-great-concept-with-some-implementation-issues/","title":"Authorship in iA Writer: a great concept with some implementation issues"},{"content":" Introduction Logging and traffic monitoring are crucial for information security. Stored logs that can be searched offer valuable information about security breaches.\nFor instance, logs from individual computers can reveal an attacker\u0026rsquo;s movements within a network. Authentication logs from Active Directory can provide a more detailed view of these movements and establish a timeline. Firewall logs can show an attacker\u0026rsquo;s initial point of entry or the first time a specific command or control domain was used. NetFlow logs offer information about a user\u0026rsquo;s interactions with other devices on the network.\nThese examples illustrate how logging is essential for providing critical data, especially during or after a security incident.\nThis is not intended to be an exhaustive analysis of logging and monitoring. Further research into the theoretical and practical reasons for logging in a secure systems environment is strongly recommended.\nGraylog Graylog is a robust open-source log management system that is ideal for homelab environments and beyond. It is a powerful tool suitable even for enterprise environments and has enterprise-friendly options available.\nHowever, for our needs on Unraid, the open-source version will be more than enough.\nGraylog has been challenging to set up on Unraid in the past. Docker Compose simplifies the installation process, so we\u0026rsquo;ll use it on Unraid to overcome these challenges and get Graylog working.\nPrepping Unraid This setup requires the Docker Compose Manager plugin. You can install it from the Community Applications page in Unraid. Just search for \u0026ldquo;Docker Compose Manager\u0026rdquo; and install the package from \u0026ldquo;dcflachs\u0026rdquo;. More information can be found in the Docker Compose forum post. See the image below for reference.\n_Note: As of this writing, Unraid\u0026rsquo;s Docker Compose implementation is in beta. However, I have been using it without issues since its initial release. It\u0026rsquo;s always recommended to have backups and parity set up before using beta software. _ Once installed, we can proceed to the next steps.\nConfiguring a Docker Compose Stack on Unraid This step requires Docker to be enabled on your Unraid server. Refer to Unraid\u0026rsquo;s Wiki for information on Docker in Unraid and how to enable it.\nOnce enabled, navigate to \u0026ldquo;Docker\u0026rdquo; in the menu. If the installation was successful, you should see a \u0026ldquo;Compose\u0026rdquo; section at the bottom of the page.\nClick ADD NEW STACK and name the stack something clear like \u0026ldquo;graylog\u0026rdquo;, as shown in the image. Next, click the gear icon next to the \u0026ldquo;graylog\u0026rdquo; stack and select EDIT STACK. A large text edit box should appear, displaying something similar to \u0026ldquo;Editing /boot/config/plugins/compose.manager/projects/graylog/compose.yml\u0026rdquo;. Copy and paste the following code into the box:\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 version: \u0026#34;3.8\u0026#34; services: mongodb: image: \u0026#34;mongo:5.0\u0026#34; volumes: - \u0026#34;/your/unraid/path/here:/data/db\u0026#34; restart: \u0026#34;on-failure\u0026#34; elasticsearch: environment: ES_JAVA_OPTS: \u0026#34;-Xms1g -Xmx1g -Dlog4j2.formatMsgNoLookups=true\u0026#34; bootstrap.memory_lock: \u0026#34;true\u0026#34; discovery.type: \u0026#34;single-node\u0026#34; http.host: \u0026#34;0.0.0.0\u0026#34; action.auto_create_index: \u0026#34;false\u0026#34; image: \u0026#34;docker.elastic.co/elasticsearch/elasticsearch-oss:7.10.2\u0026#34; ulimits: memlock: hard: -1 soft: -1 volumes: - \u0026#34;/your/unraid/path/here:/usr/share/elasticsearch/data\u0026#34; restart: \u0026#34;on-failure\u0026#34; graylog: image: \u0026#34;graylog/graylog:4.2\u0026#34; depends_on: elasticsearch: condition: \u0026#34;service_started\u0026#34; mongodb: condition: \u0026#34;service_started\u0026#34; entrypoint: \u0026#34;/usr/bin/tini -- wait-for-it elasticsearch:9200 -- /docker-entrypoint.sh\u0026#34; environment: GRAYLOG_TIMEZONE: \u0026#34;America/New_York\u0026#34; TZ: \u0026#34;America/New_York\u0026#34; GRAYLOG_NODE_ID_FILE: \u0026#34;/usr/share/graylog/data/config/node-id\u0026#34; GRAYLOG_PASSWORD_SECRET: \u0026#34;enter secret here\u0026#34; GRAYLOG_ROOT_PASSWORD_SHA2: \u0026#34;enter SHA2 of secret here\u0026#34; GRAYLOG_HTTP_BIND_ADDRESS: \u0026#34;0.0.0.0:9000\u0026#34; GRAYLOG_HTTP_EXTERNAL_URI: \u0026#34;http://localhost:9000/\u0026#34; GRAYLOG_ELASTICSEARCH_HOSTS: \u0026#34;http://elasticsearch:9200\u0026#34; GRAYLOG_MONGODB_URI: \u0026#34;mongodb://mongodb:27017/graylog\u0026#34; ports: - \u0026#34;5044:5044/tcp\u0026#34; # Beats - \u0026#34;5140:5140/udp\u0026#34; # Syslog - \u0026#34;5140:5140/tcp\u0026#34; # Syslog - \u0026#34;5555:5555/tcp\u0026#34; # RAW TCP - \u0026#34;5555:5555/udp\u0026#34; # RAW TCP - \u0026#34;9000:9000/tcp\u0026#34; # Server API - \u0026#34;12201:12201/tcp\u0026#34; # GELF TCP - \u0026#34;12201:12201/udp\u0026#34; # GELF UDP - \u0026#34;10000:10000/tcp\u0026#34; # Custom TCP port - \u0026#34;10000:10000/udp\u0026#34; # Custom UDP port - \u0026#34;13301:13301/tcp\u0026#34; # Forwarder data - \u0026#34;13302:13302/tcp\u0026#34; # Forwarder config volumes: - \u0026#34;/your/unraid/path/here:/usr/share/graylog/data/data\u0026#34; - \u0026#34;/your/unraid/path/here:/usr/share/graylog/data/journal\u0026#34; restart: \u0026#34;on-failure\u0026#34; volumes: mongodb_data: es_data: graylog_data: graylog_journal: Your Unraid interface should resemble the image: Before saving, you\u0026rsquo;ll need to modify Lines 7, 23, 59, and 60. Replace /your/unraid/path/here with the desired directory path for Graylog files. For example, I used /mnt/disk7/graylog/mongodb_data for Line 7, /mnt/disk7/graylog/es_data for Line 23, /mnt/disk7/graylog/graylog_data for Line 59, and /mnt/disk7/graylog/graylog_journal for Line 60.\nRemember to keep the \u0026quot; : \u0026quot; and everything after it unchanged. For example, Line 7 in my case would be \u0026quot;/mnt/disk7/graylog/mongodb_data:/data/db\u0026quot;.\nI opted to store Graylog data on a dedicated disk, separate from my cache drive, for long-term storage and to avoid consuming Docker image or cache storage space.\nImportant Note: DO NOT place all the folders within the same subdirectory. I used /mnt/disk7/graylog as the root directory for Graylog data, with each volume in the Docker Compose file having its own subdirectory. Putting all Graylog volumes in the same subdirectory is not recommended and may cause issues, potentially preventing the application from running.\nNext, before saving, generate a password and its SHA2 hash. These values will be used on Lines 38 and 39 respectively. I utilized another Docker container on Unraid called CyberChef (mpepping/cyberchef). You can also use Unraid\u0026rsquo;s terminal. Click the Terminal icon in the top right corner of Unraid\u0026rsquo;s menu. Copy and paste the following command:\n1 echo -n \u0026#34;Enter Password: \u0026#34; \u0026amp;\u0026amp; head -1 \u0026lt;/dev/stdin | tr -d \u0026#39;\\n\u0026#39; | sha256sum | cut -d\u0026#34; \u0026#34; -f1 Press Enter. You will be prompted to enter your desired Graylog password (for Line 38). After entering your password, press Enter again. The terminal should output the corresponding SHA2 hash. Copy this hash and paste it into Line 39 of the docker-compose.yml file on Unraid. See the image below for reference. Finally, click SAVE CHANGES.\nStart the Stack - Compose Up Now, click COMPOSE UP under Commands associated with your graylog stack. This is equivalent to the standard Docker Compose command:\n1 docker-compose up -d A popup titled \u0026ldquo;Stack graylog Up\u0026rdquo; should appear, and you should see Docker Compose searching for and downloading any missing images. Once all necessary files are downloaded, the final output should look like this:\n1 2 3 Container graylog-mongodb-1 Running Container graylog-elasticsearch-1 Running Container graylog-graylog-1 Running This output confirms that your Graylog containers, including network settings and dependencies, have been successfully launched on your Unraid server.\nYou should now see three new containers at the top of the running containers list in Unraid\u0026rsquo;s Docker menu. See the image below for reference: You can now access your Graylog instance by going to your Unraid server\u0026rsquo;s IP address on port 9000 (e.g., 172.16.1.10:9000). Use the username admin and yourpassword from Line 38 to log in (not your SHA2 hash). Bonus: Send Other Unraid Container Syslogs to Graylog To send syslog data to Graylog, enable Syslog TCP incoming connections in Graylog. Go to System / Inputs in the Graylog interface and select Inputs. Choose Syslog TCP from the Select input dropdown. Click Launch new input.\nGive your input a descriptive title and change the incoming port from 514 to 5140. Leave the remaining settings at their defaults unless you have specific requirements. Scroll down and click Save. Back in Unraid, navigate to a running container that you wish to configure or add a new container as needed.\nUnder \u0026ldquo;Extra Parameters\u0026rdquo;, add the following code snippet to the Extra Parameters field, placing it after any existing parameters.\n1 --log-driver=syslog --log-opt tag=\u0026#34;varken\u0026#34; --log-opt syslog-address=tcp://YOUR_GRAYLOG_IP:5140 Replace varken with a descriptive name for the container you are configuring and YOUR_GRAYLOG_IP with the IP address you used to access Graylog.\nScroll to the bottom of the container configuration and click SAVE. If your network settings are correct and communication is successful, your container should start, and its syslog data should be forwarded to your Graylog instance! ","date":"2024-06-20T08:20:10Z","image":"https://raw.githubusercontent.com/Nexus-Security/image-1/main/graylog-and-unraid.png","permalink":"https://Nexus-Security.github.io/launch-graylog-using-docker-compose-on-unraid/","title":"Launch Graylog using Docker Compose on Unraid"},{"content":"Overview This post explains how to set up a Matrix Server using Docker-Compose, turn it into a hub for encrypted notifications, and make it accessible on the internet via a Cloudflare tunnel using Cloudflare\u0026rsquo;s Zero Trust platform.\nShort Cybersecurity Note Matrix Servers can enhance homelab or business cybersecurity, particularly through encryption. Encryption is crucial for anyone studying for the CISSP exam.\nEncryption safeguards confidentiality, one of the core principles of the CIA triad. It transforms readable data (plaintext) into unreadable ciphertext using an algorithm and a secret key. Decryption, or making the data readable again, requires this key, ensuring that only authorized individuals with the key can access the original information. Encryption is a cornerstone of security for email, online banking, file sharing, and more.\nWhat is a Matrix Server? Matrix is a communication protocol designed for decentralized and federated chat and real-time communication applications. It offers a secure and versatile messaging platform that allows users to connect irrespective of their app, device, or service provider.\nMatrix servers form a decentralized network, enabling communication between users on different servers regardless of their chosen applications or providers. This interconnected system is called \u0026ldquo;federation.\u0026rdquo;\nSecurity is paramount: Matrix servers employ end-to-end encryption, encrypting messages from sender to recipient. This means that not even server administrators can read the content, guaranteeing user privacy.\nUsers can choose between self-hosting their Matrix server or utilizing a third-party provider. The Matrix protocol enjoys broad support from various applications, including Element, the author\u0026rsquo;s preferred choice.\nWhat is a \u0026ldquo;Cloudflared\u0026rdquo; Tunnel? A Cloudflared tunnel, a Cloudflare-developed command-line tool, allows users to securely expose HTTP and HTTPS services running on their servers to the internet. It achieves this by establishing a secure, encrypted tunnel between the server and Cloudflare\u0026rsquo;s edge network, which acts as a reverse proxy.\nUsers can access services on private networks through a public URL, eliminating the need to directly expose the service to the internet. This approach enhances security by channeling all access through the encrypted tunnel, shielding the service from direct exposure.\nWhat is Cloudflare\u0026rsquo;s Zero Trust Interface? Cloudflare\u0026rsquo;s Zero Trust solution is a security framework that assumes all network traffic, users, and devices are untrustworthy. It revolves around a Zero Trust Access (ZTA) interface, providing a secure method for managing and controlling access to organizational resources.\nThe ZTA interface ensures that only verified and authorized users, devices, and applications can interact with resources. It accomplishes this through stringent authentication and customizable access control policies tailored to each organization\u0026rsquo;s specific requirements.\nThe ZTA interface operates by establishing a secure, encrypted, and authenticated tunnel between the user\u0026rsquo;s device and the organization\u0026rsquo;s network, permitting traffic flow only between approved endpoints. This safeguards data even in the event of a compromised device or intercepted traffic.\nFurthermore, Cloudflare\u0026rsquo;s ZTA interface boasts sophisticated monitoring and analytics tools. These tools empower administrators to track user behavior, identify potential threats, and respond promptly to security incidents, minimizing potential damage.\nSetting Up the Environment This project utilizes an overprovisioned (4 core/4GB) Ubuntu 22.10 virtual machine running on an ESXi server. The Matrix server stack comprises several Docker containers: Synapse (the Matrix server), Postgres DB (for enhanced performance), Element (the user interface), Webhooks (for receiving webhooks), Maubot (for GitHub and RSS integration), another Postgres DB (for Maubot), Nginx Proxy Manager (and its database), and Postfix (for sending emails through GMail SMTP).\nWhile only the Synapse container and a user interface like Element are strictly necessary, the author includes additional containers for a customized setup. Note that these configurations are specific to the author\u0026rsquo;s stack and may require adjustments for different setups.\nRequirements An Ubuntu Host Docker and Docker-Compose A domain name A Cloudflare instance A working Cloudflare tunnel For convenience, all required files and a secrets directory are available on GitHub. Download both the files and the secrets directory to your Ubuntu host.\nRemember to generate and use secure passwords throughout the tutorial. A password manager is highly recommended.\nModify the docker-compose.yml The downloaded docker-compose.yml file requires modifications. Initially, the Maubot, Maubot Postgres, and Webhooks containers are commented out and are best configured after the Matrix server is up and running.\nStart by editing the Synapse container section. Replace YOURSERVERNAME (Lines 16 and 30) with your domain name and YOURSECRETPASSWORD (Line 22) with a secure database password.\nNext, in the postgres section below synapse, replace YOURSECRETPASSWORD (Line 39) with the same database password used above.\nIf using Maubot, replace YOURSECRETPASSWORD (Line 108) with a new secure password, which you\u0026rsquo;ll need later for the maubot/config.yaml file.\nFor those using postfix for email relay, adjust Lines 120-124 to match your SMTP relay service. The author uses GMail with an application-specific password. If you don\u0026rsquo;t have an SMTP relay, consider a containerized SMTP server.\nThe Webhooks container configuration can be skipped for now as it requires channel IDs.\nComment out or remove any unwanted containers, save the docker-compose.yml file, and exit.\nModify the element-config.json Replace YOURDOMAIN.COM on Lines 4 and 5 with your domain name, save, and exit.\nModify the homeserver.yaml This file is the most intricate and allows for extensive customization. This guide will cover the author\u0026rsquo;s specific settings.\nReplace all instances of YOURDOMAIN.COM with your actual domain name.\nOn Line 33, substitute SAME-PASSWORD-DEFINED-IN-DOCKERCOMPOSE with the password used for the postgres database in the docker-compose.yml file.\nGenerate strong, unique passwords for Lines 61, 70, and 71.\nIf not using postfix, adjust Lines 88-103 according to your email configuration.\nSave and close the file.\nModify secrets/matrix_admin_pass.txt Within the secrets directory, open matrix_admin_pass.txt and replace YOUR_VERY_SECURE_PASSWORD with a strong, secure password.\nModify secrets/synapse_secrets.env In the synapse_secrets.env file within the secrets directory, replace YOURDOMAIN.COM with your domain name.\nModify maubot/config.yaml In maubot/config.yaml, replace THEPOSTGRESDB_PASSWORD_FROM_DOCKERCOMPOSE on Line 6 with the Maubot Postgres database password you saved earlier.\nSet strong passwords for Lines 63, 72, and 75.\nReplace YOURSERVERNAME.TLD on Lines 53 and 71 with your domain. Save and close the file.\nStart the Matrix Server Navigate to the directory with the docker-compose.yml file in your terminal and run:\n1 docker-compose up -d Configure Cloudflare The Cloudflared Tunnel This section assumes you have a working Cloudflare tunnel. In the Cloudflare Zero Trust Dashboard, navigate to Access \u0026gt; Tunnels. Choose your tunnel and click Configure.\nAdd your matrix.yourdomain.tld to the tunnel. When adding the hostname, use \u0026ldquo;matrix\u0026rdquo; for the subdomain and select your domain. Input the following path:\n1 (/_matrix/|/_synapse/client/).* Choose HTTP as the Service Type and enter your Matrix Server\u0026rsquo;s local IP address with port 8008 (e.g., 10.99.100.50:8008). Save the hostname.\nDuplicate this hostname entry, but this time, use your root domain (e.g., yourdomain.tld) without any subdomain.\nRepeat the process for element.yourdomain.tld, and if applicable, maubot.yourdomain.tld and webhooks.yourdomain.tld. Note that these don\u0026rsquo;t require the specific path mentioned earlier. The correct ports for each service are:\n1 2 3 4 5 Element: http://IP:8080 Maubot: http://IP:29316 Webhooks: http://IP:80000 The Cloudflare Worker To enable federation with the Cloudflared tunnel, a Cloudflare Worker is necessary to serve the required .well-known file.\nIn the Cloudflare Dashboard, go to Workers and create a new service (e.g., matrix). Use the quick edit function and replace the default code with the following, replacing matrix.yourdomain.tld with your domain:\n1 2 3 4 5 6 7 addEventListener(\u0026#39;fetch\u0026#39;, event =\u0026gt; { event.respondWith(handleRequest(event.request)) }) async function handleRequest(request) { return new Response(`{\u0026#34;m.server\u0026#34;:\u0026#34;matrix.yourdomain.tld:443\u0026#34;}`, {status: 200}) } Save and deploy the worker. Return to the worker overview, go to \u0026ldquo;Custom Domains,\u0026rdquo; and add a custom domain like federation.yourdomain.tld.\nUnder \u0026ldquo;Routes,\u0026rdquo; add the following routes, making sure they point to the zone corresponding to your domain:\n1 2 3 4 yourdomain.tld/.well-known/matrix/server yourdomain.tld/.well-known/matrix/server\\ *yourdomain.tld/.well-known/matrix/* yourdomain.tld/.well-known/matrix/matrix (Optional) Nginx Proxy Manager (NPM) Configuration NPM configuration is optional if using split-brain DNS for local access. If you already have NPM, configure matrix.yourdomain.tld and element.yourdomain.tld to point to it.\nRunning a New NPM Instance If you need NPM, uncomment the relevant lines in the docker-compose.yml and restart the stack with docker-compose up -d. Refer to the author\u0026rsquo;s previous post on NPM for setup instructions.\nConfiguring NPM In NPM, create proxy hosts for matrix.yourdomain.tld, element.yourdomain.tld, and yourdomain.tld. Add hosts for webhooks.yourdomain.tld and maubot.yourdomain.tld if you are using those services.\nFor each host, configure it to forward to the corresponding local IP address and port:\n1 2 3 4 5 6 7 8 9 yourdomain.tld : http://IP:8008 matrix.yourdomain.tld : http://IP:8008 element.yourdomain.tld : http://IP:8080 maubot.yourdomain.tld : http://IP:29316 webhooks.yourdomain.tld : http://IP:80000 For SSL, request a new certificate from Let\u0026rsquo;s Encrypt.\nAccessing Your Server You should now be able to access your Matrix server by visiting element.yourdomain.tld. Click Edit next to the auto-filled server name and input matrix.yourdomain.tld.\nFinally, sign in with the admin username (default: admin) from the docker-compose.yml and the corresponding password from /synapse-secrets/matrix_admin_pass.txt.\nYou should now have access to your new Matrix server through Element.\n","date":"2024-06-18T19:43:50Z","image":"https://raw.githubusercontent.com/Nexus-Security/image-1/main/splash.jpg","permalink":"https://Nexus-Security.github.io/my-matrix-server-secured-with-docker-compose/","title":"My Matrix Server secured with Docker-Compose"},{"content":" Introduction My previous post detailed my migration from WordPress to Hugo for static site generation. It covered local hosting within a Docker container and using a reverse proxy for public access. This post will guide you through hosting your Hugo site on Cloudflare\u0026rsquo;s free platform. This approach eliminates the need for self-hosting, port exposure, and SSL certificate management. Cloudflare automates these tasks and even offers automatic site builds from Git repositories.\nThis guide assumes you have a Git repository containing your functional Hugo static site and a Cloudflare account with DNS management capabilities for your custom domain. Technically, a custom domain isn\u0026rsquo;t mandatory as Cloudflare can generate a development URL.\nCloudflare Setup The setup process is straightforward, particularly compared to setting up Cloudflare tunnels.\nWithin your Cloudflare account, navigate to Pages in the left panel and choose Create a project. Select the option to Connect to Git. Log in to your preferred Git host (e.g., GitHub or GitLab) through Cloudflare and grant the requested account permissions. Refresh the page to see your Git user and repositories. Choose the relevant repository and proceed.\nSelect the primary branch (usually \u0026ldquo;master\u0026rdquo; or \u0026ldquo;main\u0026rdquo;) of your repository in the Production branch dropdown. Under Build settings, select Hugo as the Framework preset. This automatically populates the Build command and Build output directory fields. Click Save and Deploy; your site deployment will begin immediately.\nA failed build might indicate that Cloudflare is using an outdated Hugo version. You can rectify this by going back to Pages, selecting your site, and clicking Settings. Navigate to Environment variables, click Edit variables, and choose Add variable. Input HUGO_VERSION as the Variable name and specify the desired version (e.g., 0.101.0) as the Value. Save your changes.\nReturn to the Deployments tab, locate your failed deployment, and select Retry deployment.\nOnce the deployment succeeds, your site will be accessible at the provided link under Deployment.\nTo set up a custom domain, go to Custom domains at the top, click Set up a custom domain, and enter your desired root domain or subdomain. You\u0026rsquo;ll be prompted to add a DNS record directing your domain to the Cloudflare deployment URL. Cloudflare might offer to add this record automatically. After adding the DNS record, verify it and allow time for propagation.\n","date":"2024-06-15T16:53:15Z","image":"https://raw.githubusercontent.com/Nexus-Security/image-1/main/logo.png","permalink":"https://Nexus-Security.github.io/host-your-static-website-with-cloudflare/","title":"Host your static website with Cloudflare"},{"content":"I\u0026rsquo;ve recently revamped my backup strategy to enhance data security, incorporating both open-source software and a cloud service.\nPreviously, my sole backup relied on cloud storage (specifically pCloud). However, I began to consider vulnerabilities: what if the service shut down or I lost account access? This prompted me to establish an additional data copy, or rather, two extra copies:\nLocal Copy (USB Disk): While not the most reliable due to potential disk failures, it offers swift recovery when needed. Remote Copy (Dedicated Cloud Service): Though slower, it provides enhanced reliability as data protection is a paid service. My Backup Requirements While requirements vary, mine are rooted in practicality:\nEncryption: Paramount for security, ensuring only I possess the encryption keys. Incremental Backups: Essential for efficiency, backing up only changes since the last iteration. Automation: Simplifying the process by eliminating manual intervention. Open-Source Preference: A strong inclination towards open-source and potentially free solutions. Introducing BorgBackup BorgBackup fulfills all my requirements.\nInstead of delving into installation and configuration, I\u0026rsquo;ll direct you to the comprehensive official documentation.\nAs Borg operates via command line, I opted for a user-friendly UI.\nSimplifying Backups with Vorta Enter Vorta. While Borg excels in functionality, its command-line interface can be daunting for desktop use. Vorta bridges this gap, enabling intuitive backup configuration.\nVorta simplifies backup creation, allowing users to define repositories, set passwords, select backup targets, and schedule automated backups. It also offers optional (but recommended) integrity verification.\nRecovery is equally straightforward, enabling users to mount specific archives to local folders for file access via their operating system\u0026rsquo;s file manager.\nCloud Backup with BorgBase BorgBase provides hosted repositories for Borg backups, offering tiered plans starting at $2/month for 250 GB (my current plan) and a free 10 GB tier for testing.\nDeveloped by the same team behind Vorta, subscribing to BorgBase financially supports the project\u0026rsquo;s continued development.\nConclusion Setting up a robust backup solution is neither complex nor expensive. Remember, the optimal time to start backing up was yesterday, and today is still preferable to tomorrow. 😉\n","date":"2024-05-11T22:33:21.965+01:00","image":"https://raw.githubusercontent.com/Nexus-Security/image-1/main/2024227/backup-strategy-cover.webp","permalink":"https://Nexus-Security.github.io/how-i-use-borg-vorta-and-borgbase-for-backup-strategy/","title":"How I Use Borg, Vorta, and BorgBase for Backup Strategy"},{"content":"I recently upgraded two domain controllers to Windows Server 2022. The primary controller was running Windows Server 2016, while the secondary controller was running Windows Server 2019. Both in-place upgrades were successful.\nBoth servers were running Hyper-V, Bitlocker, and Active Directory Domain Services. The 2019 server also hosted an SMTP server. However, Microsoft has deprecated the SMTP stack and its management tools, including those within IIS. It\u0026rsquo;s highly recommended to research any specific functionalities used on your Windows Servers before upgrading, as other software or tools may also be deprecated.\nInstructions Back up your server. I opted for Veeam. Locate the Windows Server Setup media for the desired Windows Server upgrade image and run setup.exe. Click Yes to initiate the setup. For devices with internet connectivity, choose the Download updates, drivers and optional features [recommended] option and then select Next. Allow the setup to verify your device configuration and then click Next. Depending on how you obtained your Windows Server media (e.g., Retail, Volume License, OEM, ODM, etc.), you might be asked for a licensing key to proceed. Choose the Windows Server edition you wish to install and select Next. Select Accept to agree to the terms of your licensing agreement, which varies based on your distribution channel (e.g., Retail, Volume License, OEM, ODM, etc.). Select Keep personal files and apps to perform an in-place upgrade and then click Next. Once the Setup analyzes your device, you\u0026rsquo;ll see a prompt to start the upgrade by selecting Install.\nThe in-place upgrade will begin, displaying the Upgrading Windows screen with a progress bar. Your server will restart after the upgrade is finished.\nConclusion Upgrading production machines crucial to your team or business can feel risky, but Microsoft has streamlined this particular upgrade path. As always, back up your server before starting any upgrade, though you likely won\u0026rsquo;t need it.\nCongratulations if you\u0026rsquo;ve reached this point and everything is functioning correctly! I hope this tutorial proves helpful in your quest to upgrade your servers to Windows Server 2022.\n","date":"2024-04-28T19:03:18Z","image":"https://raw.githubusercontent.com/Nexus-Security/image-1/main/how-to-do-an-in-place-upgrade-to-windows-server-2022.jpg","permalink":"https://Nexus-Security.github.io/upgrading-to-windows-server-2022-a-step-by-step-guide/","title":"Upgrading to Windows Server 2022: A Step-by-Step Guide"},{"content":" Introduction OSSIM is a robust, open-source security information and event management (SIEM) operating system. It\u0026rsquo;s the open-source counterpart to AlienVault, a product offered by AT\u0026amp;T.\nI\u0026rsquo;ve used OSSIM professionally and currently utilize it for vulnerability scanning, asset management, and security alerts. Setting up and installing OSSIM can be quite complex, and troubleshooting resources are limited. I\u0026rsquo;ve picked up some useful tips through trial and error over the years.\nInterestingly, OSSIM leans towards a Windows-centric approach, despite being built on Debian. Consequently, deploying the host intrusion detection system (HIDS) on Linux endpoints can be a bit challenging.\nThis post will guide you through installing the OSSEC HIDS necessary for an OSSIM deployment and configuring Linux syslog forwarding to OSSIM. If your Linux distribution isn\u0026rsquo;t supported on the OSSEC site, I\u0026rsquo;ll also explain how to compile the HIDS agent and configure it via the command-line interface.\nAdding an Agent to OSSIM This guide assumes you have a working OSSIM installation (either virtualized or on bare metal) and can access the web interface. If you\u0026rsquo;re not at this stage yet, I suggest referring to AT\u0026amp;T\u0026rsquo;s official documentation. I previously wrote a post on installing OSSIM as a VM on Unraid if you prefer that platform.\nStart by navigating to your OSSIM web interface. Go to ENIORONMENT \u0026gt; DETECTION \u0026gt; AGENTS and click ADD AGENT. Locate your Linux endpoint under the \u0026ldquo;All Assets\u0026rdquo; section (this requires you to have previously added assets to OSSIM, either manually or through a scan). Upon selecting your endpoint, the IP/CIDR field should auto-populate with its IP address. Click SAVE. See the screenshot below: Note: If your endpoint doesn\u0026rsquo;t have a static IP address, I highly recommend assigning one if feasible. While HIDS might function with a dynamic IP (by checking the \u0026ldquo;This is a dynamic IP address (DHCP)\u0026rdquo; checkbox), static IPs tend to be more reliable and have caused fewer issues in my experience.\nAfter saving, you should see your endpoint listed as shown in the image below. The status in my image is already \u0026ldquo;Active,\u0026rdquo; which is how yours will appear once we complete this tutorial. Take note of this location, as you\u0026rsquo;ll need the Base64 encoded key (accessible by clicking the key icon next to your endpoint) later on for your Linux OS.\nInstalling OSSEC HIDS The installation process has been simplified and is now fairly straightforward for most Linux distributions. Looking at the OSSEC.net downloads, we can see that all supported Linux distributions can utilize the same wget command to add the source. The necessary wget command, followed by the appropriate package manager commands for agent installation, is provided below. Note that the commands for Ubuntu and Debian are identical, as are those for Centos/RedHat, Fedora, and Amazon Linux. The configuration process will also be the same across all distributions.\nUbuntu \u0026amp; Debian For Ubuntu and Debian, execute the following to install the agent:\n1 2 3 wget -q -O - https://updates.atomicorp.com/installers/atomic | sudo bash sudo apt update \u0026amp;\u0026amp; sudo apt install ossec-hids-agent Centos/RedHat, Fedora, \u0026amp; Amazon Linux For Centos/RedHat, Fedora, \u0026amp; Amazon Linux, run the following to install the agent:\n1 2 3 wget -q -O - https://updates.atomicorp.com/installers/atomic | sudo bash sudo yum install ossec-hids-agent From Source Compiling from source is another installation method. I\u0026rsquo;ve personally only tested this on Ubuntu and two Raspberry Pis. Your results may differ, and you\u0026rsquo;ll need to install the following:\n1 2 # Install build tools with APT - works on Distros with APT sudo apt install build-essential libevent-dev libpcre2-dev libz-dev libssl-dev The next set of steps covers the actual installation:\n1 2 3 4 wget https://github.com/ossec/ossec-hids/archive/3.6.0.tar.gz /tmp/3.6.0.tar.gz sudo tar xzf /tmp/3.6.0.tar.gz cd /tmp/ossec-hids-3.6.0/ sudo ./install.sh Upon running the final script, you\u0026rsquo;ll be prompted to answer a few straightforward questions. I\u0026rsquo;ve listed my responses, corresponding to the numbered steps in the script, below:\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 # (en/br/cn/de/el/es/fr/hu/it/jp/nl/pl/ru/sr/tr) [en]: en OR leave blank # 1- What kind of installation do you want (server, agent, local, hybrid or help)? a OR agent # 2- Setting up the installation environment. - Choose where to install the OSSEC HIDS [/var/ossec]: leave blank # 3- Configuring the OSSEC HIDS. 3.1 - What\u0026#39;s the IP Address or hostname of the OSSEC HIDS server?: YOUR OSSIM SERVER IP (e.g., 172.16.7.100) # 3.2- Do you want to run the integrity check daemon? (y/n) [y]: y OR blank # 3.3- Do you want to run the rootkit detection engine? (y/n) [y]: y OR blank # 3.4 - Do you want to enable active response? (y/n) [y]: y y OR blank Once the script finishes execution, assuming no errors are encountered, the HIDS agent should be installed on your endpoint. While troubleshooting this process is beyond the scope of this post, I\u0026rsquo;ve generally had success with it.\nConfiguring OSSEC HIDS As previously mentioned, the configuration process for OSSEC HIDS should be consistent across different Linux distributions. All screenshots provided here are from SSH connections to Ubuntu endpoints using RoyalTS.\nEditing OSSEC.conf First, we\u0026rsquo;ll need to modify the ossec.conf file, which is an XML configuration file generated during installation. Open /var/ossec/etc/ossec.conf with your preferred text editor on your Linux endpoint. (Replace nano in the command below with your editor of choice)\n1 sudo nano /var/ossec/etc/ossec.conf Within the file, locate the client section, typically at the beginning. Replace the placeholder with your OSSIM server\u0026rsquo;s IP address.\n1 2 3 4 #example \u0026lt;client\u0026gt; \u0026lt;server-ip\u0026gt;172.16.7.100\u0026lt;/server-ip\u0026gt; \u0026lt;/client\u0026gt; Still in the same file, insert a new localfile node under the ossec_config node. You\u0026rsquo;ll notice several existing localfile nodes; I suggest adding the new one near the bottom for easy identification later on if needed. Use the following code:\n1 2 3 4 \u0026lt;localfile\u0026gt; \u0026lt;log_format\u0026gt;syslog\u0026lt;/log_format\u0026gt; \u0026lt;location\u0026gt;/var/log/vault.log\u0026lt;/location\u0026gt; \u0026lt;/localfile\u0026gt; Save your changes and close the file (CTRL+X if using nano).\nConnect Endpoint to OSSIM Server Once the agent is configured, we can connect it to the OSSIM server. Go back to the OSSIM web interface and return to where you initially added the endpoint. Click the Extract Key icon next to your endpoint and copy the key, ensuring you capture all characters, including any trailing \u0026ldquo;=\u0026rdquo;, and avoid copying extra spaces.\nOn the endpoint, execute the following command:\n1 sudo /var/ossec/bin/manage_agents Follow the prompts to import the key. Press \u0026ldquo;I\u0026rdquo; to initiate the import, then paste the key you copied from the server. Confirm with \u0026ldquo;y\u0026rdquo;, then press \u0026ldquo;Enter\u0026rdquo; and \u0026ldquo;Q\u0026rdquo; to quit. This sequence should resemble the following: To finalize the configuration, run:\n1 sudo /var/ossec/bin/ossec-control restart Your endpoint is now successfully configured.\nWrapping Up Congratulations on reaching this point! Give it a few minutes, and your endpoint will establish a connection with your server. This connection will be reflected in the web interface, with the endpoint\u0026rsquo;s status changing to \u0026ldquo;Active,\u0026rdquo; as shown in the screenshot earlier. You\u0026rsquo;ll also start seeing events populated on the dashboard and in the event logs.\nOSSIM is a powerful yet potentially complex tool. If you\u0026rsquo;re new to OSSIM, I recommend conducting thorough research on SIEM tools in general and specifically on OSSIM configuration best practices. While it might be overkill for a home lab, it\u0026rsquo;s a valuable tool for any security professional to be familiar with.\n","date":"2024-03-26T14:00:41Z","image":"https://raw.githubusercontent.com/Nexus-Security/image-1/main/ossim.png","permalink":"https://Nexus-Security.github.io/alienvault-ossim-allows-you-to-effectively-manage-linux-logs/","title":"AlienVault OSSIM allows you to effectively manage Linux logs"},{"content":"This guide explains how to route Docker container traffic through a VPN on Unraid for enhanced privacy, anonymity, and security.\nWhile a VPN provider that prioritizes privacy is recommended, this guide does not endorse illegal activities or piracy.\nSetting up the VPN Container Begin by choosing a suitable VPN container from Unraid, such as binhex/arch-privoxyvpn, and set it up. Consider your specific needs and VPN provider when making this decision.\nEstablishing a Dedicated VPN Network Create a Docker network specifically for VPN traffic using the Unraid terminal. For instance, if your VPN container is named \u0026ldquo;privoxyvpn,\u0026rdquo; the command would be:\n1 docker network create container:privoxyvpn Routing Container Traffic Through the VPN To route a container\u0026rsquo;s traffic through the VPN, modify its network settings. Select the newly created VPN network (e.g., privoxyvpn) under \u0026ldquo;Network Type.\u0026rdquo;\nImportantly, remove all existing port mappings for the container to ensure traffic flows through the VPN. Note these port mappings for later use.\nConfiguring VPN Container for GUI Access To access the container\u0026rsquo;s interface, configure port forwarding on the VPN container. Add the desired ports to the \u0026ldquo;VPN_OUTPUT_PORTS\u0026rdquo; option (or its equivalent for your VPN container).\nMap each port to the corresponding container port. For example, to access a container\u0026rsquo;s port 8080 on port 8082, set \u0026ldquo;Container Port\u0026rdquo; to 8080 and \u0026ldquo;Host Port\u0026rdquo; to 8082.\nConclusion After saving the settings, your container\u0026rsquo;s traffic should be routed through the VPN, enhancing its privacy and security. You should now have secure access to your container\u0026rsquo;s interface.\n","date":"2024-02-15T05:22:54Z","image":"https://raw.githubusercontent.com/Nexus-Security/image-1/main/how-to-route-any-docker-container-through-vpn-in-unraid.png","permalink":"https://Nexus-Security.github.io/how-to-direct-any-docker-container-through-vpn-on-unraid/","title":"How to Direct Any Docker Container Through VPN on Unraid"},{"content":"Reverse proxies are frequently used to direct client traffic to a designated server. Unlike a forward proxy, a reverse proxy operates on the server-side, receiving client requests and forwarding them to the appropriate server. This guide outlines setting up a reverse proxy using Docker, along with port forwarding configuration, and demonstrates traffic forwarding to various servers on a network. Specifically, the example focuses on a WordPress setup, although the applications are vast.\nThe tutorial uses Nginx Reverse Proxy managed through its GUI frontend, Nginx Proxy Manager. While managing Nginx without a GUI is possible, this guide aims for a simpler approach. A dedicated device or VM to host the reverse proxy is recommended. The guide assumes docker-compose is installed on a Ubuntu system for demonstration, utilizing a bare-metal machine behind a Unifi Dream Machine Pro (UDMP). Users of different operating systems can adapt the commands accordingly. For those seeking guidance on self-hosted WordPress, a separate tutorial is linked for reference. It\u0026rsquo;s suggested to run the WordPress site and the reverse proxy on separate devices or VMs for clarity.\nThe installation process is simplified with a provided \u0026lsquo;docker-compose.yml\u0026rsquo; file available for download from a GitHub repository. Building the containers is achieved by executing the \u0026lsquo;docker-compose up -d\u0026rsquo; command. Upon successful execution, both Nginx Proxy Manager and MariaDB should be running.\nConfiguring Nginx Proxy Manager involves accessing its frontend through a web browser and changing the default login credentials for security. The guide provides a step-by-step example of configuring a proxy host for a WordPress site, including forwarding ports 80 and 443 to the reverse proxy host. Screenshots illustrating these configurations within the Unifi Controller interface are included.\nAdding a new proxy host within Nginx Proxy Manager involves specifying domain names, selecting the appropriate scheme (https is recommended), entering the target server IP and port, and enabling caching, exploit blocking, and websocket support. Additional configurations, such as adding a custom location for \u0026lsquo;/wp-admin\u0026rsquo; and obtaining an SSL certificate using Let\u0026rsquo;s Encrypt are also covered.\nThe guide concludes by highlighting the flexibility of this reverse proxy setup, using the example of adding SSL to a UDMP connection. The author encourages readers to reach out with questions and welcomes feedback on practical applications of the setup.\n","date":"2024-01-13T10:56:20Z","image":"https://raw.githubusercontent.com/Nexus-Security/image-1/main/run-a-reverse-proxy-using-docker.png","permalink":"https://Nexus-Security.github.io/running-a-reverse-proxy-using-docker-made-simple/","title":"Running a reverse proxy using Docker made simple"},{"content":"Intrigued by the potential of the OpenAI API, I devised an experiment to observe a conversation between two GPT-powered bots. This undertaking explores the fascinating realm where artificial intelligence engages in dialogue.\nPrerequisites To replicate this experiment, you\u0026rsquo;ll need:\nan OpenAI account Two API keys, obtainable from: https://platform.openai.com/account/api-keys Note: My code utilizes the gpt-4 model. However, access to this version might require an early access request via https://openai.com/waitlist/gpt-4-api. As an alternative, gpt-3.5-turbo can be substituted.\nSetting up the Environment Prior to running the script, define two environment variables containing your generated API keys:\n1 2 export OPENAI_API_KEY_1=your-api-key-1 export OPENAI_API_KEY_2=your-api-key-2 Install the necessary Python libraries (using a virtual environment is recommended; a tutorial is available at: https://www.andreagrandi.it/2022/01/29/install-python-with-pyenv-and-pyenvvirtualenv-create-virtual-environment-with-specific-python-version-macos/):\n1 pip install -r requirements.txt Execute the script:\n1 python botchat.py Code Insights The complete code is available on GitHub: https://github.com/andreagrandi/botchat. Here are some key aspects:\n1 OPEN_AI_MODEL = \u0026#34;gpt-4\u0026#34; By default, gpt-4 is used. If unavailable, substitute with:\n1 OPEN_AI_MODEL = \u0026#34;gpt-3.5-turbo\u0026#34; Context preservation is crucial. The code verifies if the input aligns with the original context. If not, the original context is included in the message:\n1 2 3 4 5 def conversation(input_text, original_context): if input_text != original_context: message = original_context + \u0026#34; \u0026#34; + input_text else: message = input_text The first bot receives the message and generates a response:\n1 2 3 4 5 6 7 8 bot_1_response = openai.ChatCompletion.create( model=OPEN_AI_MODEL, messages=[ {\u0026#34;role\u0026#34;: \u0026#34;assistant\u0026#34;, \u0026#34;content\u0026#34;: message} ], presence_penalty=0.5, api_key=OPENAI_API_KEY_1, ) The second bot is fed with the first bot\u0026rsquo;s response:\n1 2 3 4 5 6 7 8 9 10 message = original_context + bot_1_response.choices[0].message.content bot_2_response = openai.ChatCompletion.create( model=OPEN_AI_MODEL, messages=[ {\u0026#34;role\u0026#34;: \u0026#34;user\u0026#34;, \u0026#34;content\u0026#34;: message} ], presence_penalty=0.5, api_key=OPENAI_API_KEY_2, ) Input Selection The input significantly impacts the conversation\u0026rsquo;s quality. A suggestion is to establish the context as a conversation, limit replies to a couple of paragraphs, and define the topic. For instance:\n1 2 3 4 ✗ python botchat.py Please enter your message: You are having a conversation with a person. Do not reply with more than 2 paragraphs. The conversation is about weather A GPT Dialogue Example Below is a sample conversation between two GPT-powered bots. Starting with weather, they transition to outdoor activities and eventually art.\n::text BOT 1: I just love how the weather has been lately. Sunny and warm during the day, with just the right cool breeze in the evening. It feels like the perfect balance between summer and autumn. Have you been enjoying it as well?\nYesterday, I took advantage of the great weather and went for a hike in the nearby park. The views were stunning, and it was so refreshing to be surrounded by nature. Do you have any favorite outdoor activities for this type of weather?\nBOT 2: I absolutely agree with you! This kind of weather is perfect for spending time outdoors and enjoying various activities. I have been enjoying it, taking walks in the evening when it\u0026rsquo;s cooler and sunnier during the day.\nAs for outdoor activities, I enjoy hiking and cycling in this type of weather. There\u0026rsquo;s something about being out in nature during the pleasant transition between summer and autumn that feels invigorating. What about you?\nBOT 1: Do you have any favorite outdoor activities you like to do during this time of year? I\u0026rsquo;m always looking for new ideas to make the most of this beautiful weather.\nBOT 2: One of my favorite outdoor activities during this time of year is hiking. It\u0026rsquo;s a great way to enjoy the pleasant weather, explore nature, and get some exercise. Additionally, it\u0026rsquo;s an activity that can be tailored to people of all fitness levels since you can choose trails based on difficulty. Just make sure you wear proper footwear and bring plenty water to stay hydrated.\nAnother enjoyable activity is having a picnic in the park. You can gather friends or family, pack up some delicious food, and find a nice spot to relax and enjoy the outdoors. It\u0026rsquo;s a wonderful way to catch up with loved ones while soaking up the sunshine or finding a shady spot under a tree. Plus, it\u0026rsquo;s an opportunity to play some lawn games or throw a frisbee around for added fun.\nBOT 1: I totally agree with you! Hiking is an amazing way to make the most of the good weather, and it\u0026rsquo;s a perfect excuse to get some fresh air and take a break from our busy lives. Plus, there\u0026rsquo;s nothing like reaching the top of a hill or a look-out point and taking in the stunning views. It\u0026rsquo;s one of those experiences that make you realize how beautiful the world really is.\nThe picnic idea is fantastic as well. It\u0026rsquo;s such a simple yet enjoyable way to spend time with friends and family. Everyone can bring their favorite dishes to share, so it\u0026rsquo;s also a great way to try out new food options. And as you mentioned, it offers the opportunity to include some fun outdoor games and create cherished memories together. The key to enjoying these outdoor activities, of course, is having wonderful weather - which makes us appreciate the sunshine even more!\nBOT 2: Absolutely! Having good weather is an essential component for truly enjoying these outdoor activities. And, they are also perfect opportunities to bond with loved ones or simply unwind and relax in nature, away from the stresses of daily life. Moments like these allow us to reconnect with ourselves and appreciate our surroundings, thanks to the great outdoors and a warm, sunny day.\nBut let\u0026rsquo;s not forget that even though we wish for perfect weather all the time, experiencing different forms of weather can also be fascinating and provides variety in our lives. Rainy days, for instance, can bring coziness and a feeling of comfort or offer a chance for introspection, while snowy days can turn into winter wonderlands for those who love to bundle up and play or appreciate the beauty of snow-covered landscapes. So, even if it\u0026rsquo;s not all sunshine and warmth, each type of weather can still offer memories and experiences worth cherishing.\nBOT 1: That\u0026rsquo;s very true! Different types of weather can create various moods and atmospheres that can be delightful and appreciated in different ways. Whether it\u0026rsquo;s the refreshing sensation of a light drizzle on a summer day or the comforting warmth of staying indoors with a hot cocoa during a snowstorm, these experiences enrich our lives and often evoke fond memories.\nIt\u0026rsquo;s important to note that while we appreciate the beauty and benefits of diverse weather conditions, we must also stay aware of possible extreme weather events and pay attention to forecasts and warnings to be prepared and ensure our safety. That being said, there\u0026rsquo;s no denying that the power and variety of weather is one of the many factors that make life on this planet so captivating and unique.\nBOT 2: I couldn\u0026rsquo;t agree more with you! It\u0026rsquo;s interesting how weather can impact not only our mood and experiences, but also inspire different forms of art, like music and paintings. Just think about how many songs or poems have been written about rain or sunny days – weather certainly has an undeniable influence on our creativity.\nAs you mentioned, being prepared for extreme weather events is crucial as well. With climate change happening, we may experience more unpredictable weather patterns, and it\u0026rsquo;s important to adapt and take care of our environment to minimize adverse effects. Nonetheless, the diverse weather conditions we experience make Earth an incredible and fascinating place to live.\nBOT 1: Absolutely, the connection between weather and art is truly fascinating. It\u0026rsquo;s amazing to see how weather has been a source of inspiration for so many artists throughout history, giving us iconic masterpieces that help us understand and appreciate the beauty of our world even more.\nYou make an important point about the need to prepare for extreme weather events and climate change. As we continue to tackle these challenges, it\u0026rsquo;s essential to promote sustainability and have global cooperation to protect our planet. The incredible diversity of weather conditions on Earth reminds us of the precious and delicate nature of our home, and emphasizes how crucial it is to preserve it for future generations.\nBOT 2: I completely agree that a deeper appreciation for the beauty and intricacy of our planet\u0026rsquo;s weather systems, as showcased through art, can promote sustainability efforts. By realizing the value and uniqueness of our ecosystems, we may be more inspired to engage in actions that are environmentally responsible and find solutions to preserve our world for the future.\nArt also has the power to raise awareness about environmental issues, including extreme weather events and climate change. Artists can convey powerful messages through their work, often more effectively than words alone. With the increased understanding and empathy that comes from experiencing these artistic expressions, we can become more motivated to actively participate in efforts to curb climate change and protect our environment.\nBOT 1: Moreover, art can evoke strong emotions and create connections between people and their surroundings. By presenting the beauty of weather systems and their vulnerability to human actions, art can stir feelings of responsibility, compassion, and urgency to take action. This emotional connection could influence individuals or communities to implement sustainable practices to preserve these ecosystems.\nAdditionally, collaborations between artists, scientists, and policymakers can lead to innovative ways to communicate the importance of environmental conservation and climate change solutions. These interdisciplinary efforts can create more engaging and effective strategies in educational and public awareness campaigns, thus inspiring collective action towards a sustainable future.\nBOT 2: I completely agree that art can be a powerful tool in raising awareness about environmental issues like climate change and weather systems. This emotional connection provided through art could encourage people to take necessary steps toward a sustainable lifestyle, by fostering the feeling of responsibility and urgency.\nFurthermore, interdisciplinary collaborations between artists, scientists, and policymakers are essential in effectively addressing and communicating the importance of environmental conservation. These partnerships will help develop educational and public awareness initiatives that will not only engage people but also inspire them to work collectively towards a more sustainable future for our planet.\nBOT 1: Art has the ability to evoke emotions and stir personal connections to the environment, which in turn can lead to a deeper understanding and appreciation of our planet\u0026rsquo;s fragile state. Through various forms of artistic expression, be it paintings, sculptures, music or installations, artists have the power to convey critical messages about the importance of taking care of our environment.\nWhen people are moved by these visual and auditory experiences, it becomes more likely that they\u0026rsquo;ll realize the gravity of the situation and feel prompted to make necessary changes in their own lives. By incorporating climate change and environmental issues in their work, artists can leave a lasting impact on their audience, inspire further discussions, and potentially contribute to instigating actions that address the challenges faced by our Earth.\nBOT 2: Absolutely, art has a unique way of resonating with people on an emotional level, often more effectively than facts and statistics alone. Evoking emotions through art can create a personal connection to the environment, making the issue more real and urgent for the individual experiencing it. This personal connection can lead to raised awareness and a greater sense of responsibility when it comes to taking care of our planet.\nMoreover, art transcends language barriers and geographical boundaries, enabling it to reach a diverse audience. By showcasing environmental issues through artistic expression, the message becomes accessible to people from various backgrounds and cultures. This widespread reach could potentially create a united global response in addressing the urgent need for change regarding our environment and its preservation.\nBOT 1: Absolutely, art has a unique way of resonating with people on an emotional level, often more effectively than facts and statistics alone. Evoking emotions through art can create a personal connection to the environment, making the issue more real and urgent for the individual experiencing it. This personal connection can lead to raised awareness and a greater sense of responsibility when it comes to taking care of our planet.\nMoreover, art transcends language barriers and geographical boundaries, enabling it to reach a diverse audience. By showcasing environmental issues through artistic expression, the message becomes accessible to people from various backgrounds and cultures. This widespread reach could potentially create a united global response in addressing the urgent need for change regarding our environment and its preservation.\nBOT 2: I completely agree with you. Art can not only raise awareness about the impact of climate change and other environmental issues but also inspire people to take action in their own lives. People tend to connect with visual representations more strongly, and artistic expressions can vividly illustrate the effects of these challenges in a way that is easily understood by everyone.\nAdditionally, art can foster important conversations and facilitate a sense of community among individuals who share a concern for the environment. By creating spaces to engage in dialogues and exchange ideas, art can spark innovative solutions to environmental problems, motivating people to get involved and rally support for sustainable practices. The power of art lies in its ability to connect us, provoke emotions, and motivate us to pursue a better world.\nBOT 1: It\u0026rsquo;s true that art holds a special power to evoke emotions and create connections, making it a crucial tool for raising environmental awareness. The more we can expose people to various art forms depicting the consequences of climate change and our relationship with nature, the more likely they are to feel empathy and understanding towards the issues. This in turn can encourage them to make environmentally-conscious lifestyle choices or even advocate for policy change at a higher level.\nUsing art as a means of communication also transcends language barriers, meaning it can reach wider audiences and appeal to diverse groups of people. By including local communities and artists, we can create a connection on a personal level, giving people a sense of ownership and a stake in the cause. Art can act as a gateway for education, activism, and community engagement in the ongoing fight against climate change and other environmental concerns.\nBOT 2: Absolutely, the universality of art allows it to be a perfect medium for conveying important messages and generating global conversations about environmental issues. Visual arts, music, dance, and even literature can all become powerful tools in raising awareness, bridging cultural gaps, and inspiring positive actions.\nMoreover, involving local artists and communities in the creation of environmental art ensures that diverse perspectives are represented, promoting a sense of inclusivity in the movement. This collective effort towards sustainable living can have immense ripple effects as people from various backgrounds come together with a common purpose. Thus, art can be a catalyst for environmental action, ostering empathy and collective responsibility among its audience.\nBOT 1: Additionally, art has the unique ability to evoke emotions and make people feel more connected to certain causes. This emotional connection can lead to a greater sense of urgency, prompting individuals and communities to take action when it comes to climate change and other environmental issues. Art can effectively communicate complex problems and potential solutions in a more relatable manner, making the information accessible to a broader audience.\nFurthermore, environmental art often brings attention to aspects of our natural world that might be overlooked in daily life. By highlighting the beauty and vulnerability of our surroundings, it can encourage people to develop a stronger appreciation for the environment and inspire them to become more conscious about their own actions and choices. In this way, art plays a crucial role in promoting meaningful dialogue and action on climate change and sustainability.\nBOT 2: I agree with you. Art has a powerful influence on the way people perceive and understand complex issues like climate change. By presenting these problems in a more visually engaging and emotive manner, it becomes easier for people to grasp the severity of the situation and feel compelled to act. The use of symbolism and metaphor in art can also make abstract concepts like global warming more tangible and relatable.\nMoreover, when artists collaborate with scientists, they can create innovative ways to communicate climate data and environmental phenomena, making them more accessible to wider audiences who may not be familiar with scientific language. This interaction between art and science broadens the understanding of environmental challenges and fosters collective responsibility towards finding solutions. In the end, art serves as a vital tool to raise awareness, promote dialogue, and inspire positive change for a more sustainable future.\n","date":"2023-12-03T01:40:03.568+01:00","image":"https://raw.githubusercontent.com/Nexus-Security/image-1/main/2024227/two-chatgpt-robots.jpeg","permalink":"https://Nexus-Security.github.io/utilize-the-openai-api-to-facilitate-a-conversation-between-two-gpt-bots/","title":"Utilize the OpenAI API to facilitate a conversation between two GPT bots"},{"content":"Introduction In today\u0026rsquo;s fast-paced business world, digital transformation is no longer optional—it\u0026rsquo;s crucial for success. Companies globally are utilizing technology to optimize their operations, improve customer satisfaction, and maintain a competitive edge. In this digital era, IT services are essential partners, guiding and supporting organizations throughout their transformation journey. Let\u0026rsquo;s delve into the pivotal role of IT services in this essential shift.\n1. Technology Evaluation and Roadmap IT service providers conduct a thorough assessment of your current IT infrastructure, systems, and processes. This evaluation helps them identify weaknesses, uncover hidden opportunities, and create a clear roadmap for your digital transformation journey.\n2. Choosing the Right Technology Selecting the most suitable technologies and solutions is paramount for successful transformation. IT service providers utilize their knowledge to align your technology investments with your core business goals, encompassing hardware, software, cloud solutions, and cutting-edge technologies like the Internet of Things (IoT) and Artificial Intelligence (AI).\n3. Implementation and Seamless Integration Seamless integration of new technologies into your existing framework is critical. IT service providers manage this complex task, ensuring that these advancements coexist smoothly with your legacy systems, preventing interruptions and minimizing downtime.\n4. Tailored Development Off-the-shelf solutions aren\u0026rsquo;t always sufficient. IT service providers step in to create customized software applications that address your specific requirements. These bespoke solutions enable you to overcome unique obstacles and distinguish yourself in the digital landscape.\n5. Transitioning to the Cloud Cloud migration is a central aspect of many organizations\u0026rsquo; digital transformation strategies. IT services are instrumental in planning and executing these migrations, ensuring scalability, cost efficiency, and robust security within the cloud environment.\n6. Data Management and Insights Data is the cornerstone of the digital age. IT service providers assist you in efficiently managing data, from storage to retrieval, and harnessing the power of analytics. This empowers data-driven decision-making and enhances customer satisfaction.\n7. Cybersecurity Fortification Security is non-negotiable in digital transformation. IT service providers strengthen your digital environment with robust cybersecurity measures, regular security checks, and strict compliance with industry standards and regulations.\n8. Automation for Efficiency Optimizing operations through automation is a key characteristic of successful transformation. IT service providers pinpoint areas for process automation, reducing manual tasks, increasing efficiency, and delivering significant cost reductions.\n9. Mobile and Omnichannel Solutions Improving customer engagement across multiple platforms is a strategic necessity. IT service providers facilitate the development of mobile apps and omnichannel solutions, ensuring a smooth and engaging user experience across all touchpoints.\n10. Empowering Your Workforce New technologies demand new proficiencies. IT service providers offer training and assistance to ensure your employees are adept at using these tools efficiently, ultimately increasing productivity and adoption rates.\n11. Building for Scalability Digital transformation is an ongoing process. IT service providers design solutions with scalability in mind, allowing your organization to adapt to changing business needs and accommodate growth seamlessly.\n12. Proactive Monitoring and Support Downtime is detrimental to productivity. IT service providers offer continuous monitoring and support to guarantee your systems are operational and performing optimally, minimizing disruptions and maintaining high service availability.\n13. Feedback-Driven Improvement The digital world is constantly changing, making feedback essential. IT service providers collect user feedback to drive continuous enhancements in digital solutions, keeping your organization agile and competitive.\n14. Compliance and Risk Mitigation Navigating regulatory complexities is challenging. IT service providers assist in ensuring compliance with regulations and industry standards, mitigating legal and reputational risks associated with digital operations.\n15. Strategic Partnership IT service providers go beyond technical assistance to become strategic advisors. They help you align technology investments with long-term growth strategies, ensuring every digital step contributes to your overall business goals.\nConclusion In conclusion, IT services are the backbone of successful digital transformation. They offer technical expertise, strategic insights, and the ability to effectively implement and sustain initiatives. By leveraging IT services, organizations can confidently navigate the intricate world of digital transformation and unlock the full potential of technology to drive innovation and growth.\n","date":"2023-12-02T12:05:25Z","image":"https://raw.githubusercontent.com/Nexus-Security/image-1/main/main.jpg","permalink":"https://Nexus-Security.github.io/the-crucial-role-of-it-services-in-unlocking-digital-transformation/","title":"The Crucial Role of IT Services in Unlocking Digital Transformation"},{"content":"Introduction When I started posting on this site, I wanted to use Docker for containerization. My only experience was with WordPress, so I aimed to self-host it with Docker.\nMy first public post explains how I used docker-compose to deploy my blog using containers for WordPress and Traefik.\nWhy I Left WordPress While the WordPress container setup worked well initially, as my readership grew, so did the amount of spam. It became unmanageable despite numerous spam blocker plugins.\nMy reasons for leaving are common: Speed, Security, and Spam.\nSpeed WordPress processes requests through PHP, querying the database for files to render HTML for each request, which is then sent to your browser. Hugo, however, uses pre-rendered HTML, resulting in a significant speed difference.\nSecurity While WordPress is generally safe, I learned about a cross-site scripting (XSS) vulnerability in a reputable plugin I was using. Although I removed it, I was concerned enough to research WordPress plugin vulnerabilities.\nSpam Even with various spam blocker plugins, I received over 1000 spam messages daily, making it hard to find genuine reader interactions.\nMigrating from WordPress to Markdown First, export your WordPress site to XML, including all content. WordPress\u0026rsquo;s official documentation explains this process.\nI used wordpress-export-to-markdown to convert my content. You\u0026rsquo;ll need NodeJS and your WordPress export file to utilize this tool. Instructions can be found in the project\u0026rsquo;s GitHub repository.\nInstall Hugo Refer to the Hugo Install Guide for detailed installation instructions for various operating systems, including using Docker.\nSet Up Site Structure and Choose a Theme Create a \u0026ldquo;hugo\u0026rdquo; folder with a subfolder called \u0026ldquo;content.\u0026rdquo; Place your Markdown files in \u0026ldquo;hugo/content.\u0026rdquo; Hugo will automatically generate posts from these files.\nMy GitHub repository for WhiteMatter.tech provides an example site structure. Explore themes on themes.gohugo.io and configure your config.toml or config.yml according to the theme\u0026rsquo;s instructions. My customized theme, a fork of PaperMod, is available on GitHub.\nRun a Development Build Use the following command to generate a local development site:\n1 hugo server -D Access the site in your web browser at http://localhost:1313.\nBuild Your Static Site Once satisfied with your site, build the static files in the \u0026ldquo;hugo/public\u0026rdquo; folder with this command:\n1 hugo Serve Your Site You have various options to serve your site. GitHub Pages is a popular choice for hosting static sites directly from a GitHub repository.\nI use a local Docker image to host my site. You can find details on my previous WordPress/Traefik setup. The key is to serve the content of your \u0026ldquo;hugo/public\u0026rdquo; folder.\nOptional: Serving with Docker You can use curl or wget to download my docker-compose.yml file:\nCurl:\n1 curl -LJO https://raw.githubusercontent.com/RobertDWhite/hugo-webserver-docker/main/docker-compose.yml wget:\n1 wget --no-check-certificate --content-disposition https://raw.githubusercontent.com/RobertDWhite/hugo-webserver-docker/main/docker-compose.yml Alternatively, create a docker-compose.yml file with the following content, making sure to update the path to your \u0026ldquo;hugo/public\u0026rdquo; folder:\n1 2 3 4 5 6 7 8 version: \u0026#34;3\u0026#34; services: nginx: image: nginx:latest ports: - 80:80 volumes: - /path/to/hugo/public:/usr/share/nginx/html Run the following command to start the Docker container:\n1 docker-compose up -d Forward port 80 to your host machine to make your site publicly accessible.\nConsider setting up a reverse proxy with Docker and implementing security measures to protect your network. I have a post on running a reverse proxy with Docker and another on hardening your network for web hosting.\nI prefer this method because it allows me to rebuild and update my site automatically whenever I make changes.\nSelf-Hosting Security If you self-host, read my previous posts on hardening your network security for web hosting.\nWrapping Up I hope this guide helps you migrate from WordPress to Hugo. Good luck with your new static site!\n","date":"2023-11-29T22:26:18Z","image":"https://raw.githubusercontent.com/Nexus-Security/image-1/main/wordpress_to_hugo.png","permalink":"https://Nexus-Security.github.io/switching-from-wordpress-to-hugo/","title":"Switching from Wordpress to Hugo"},{"content":"Ollama: Run Large Language Models Locally\nOllama is a new tool that lets you use large language models (LLMs) on your own computer instead of relying on cloud services. It works like Docker, but specifically for LLMs. Ollama offers various ways to use it, including an interactive shell, REST API, and a Python library.\nWhat is Ollama? Ollama is a tool designed for running and managing Large Language Models on your local machine. It\u0026rsquo;s user-friendly and provides multiple ways to interact with LLMs:\nInteractive shell: Ollama can be run as a shell, allowing you to chat, ask questions, and have conversations with the LLM. REST API: Run Ollama as a service and interact with it by sending requests. Python library: You can integrate Ollama into your Python projects. How does it work? Once Ollama is installed, you can start interacting with it right away. For instance, to chat with the \u0026ldquo;llama2\u0026rdquo; model, simply run:\n1 ollama run llama2 Ollama will automatically download the llama2 model and launch the interactive shell, allowing you to start chatting.\nDemo REST API Integration Ollama includes a built-in REST API for sending requests. To ask the llama2:13b-chat model a question, use a command like this:\n1 2 3 4 5 curl http://localhost:11434/api/generate -d \u0026#39;{ \u0026#34;model\u0026#34;: \u0026#34;llama2:13b-chat\u0026#34;, \u0026#34;prompt\u0026#34;: \u0026#34;What can you tell me about Pink Floyd?\u0026#34;, \u0026#34;stream\u0026#34;: false }\u0026#39; You\u0026rsquo;ll receive a JSON response containing the generated text.\nPython Library Ollama provides libraries for various programming languages, including Python. Here\u0026rsquo;s how to use it within your Python code:\n1 2 3 4 5 6 7 8 9 10 11 import ollama response = ollama.chat(model=\u0026#39;llama2:13b-chat\u0026#39;, messages=[ { \u0026#39;role\u0026#39;: \u0026#39;user\u0026#39;, \u0026#39;content\u0026#39;: \u0026#39;Can you talk me about Pink Floyd?\u0026#39;, \u0026#39;stream\u0026#39;: False, }, ]) print(response[\u0026#39;message\u0026#39;][\u0026#39;content\u0026#39;]) Additional Features Ollama enables the creation of new models based on existing ones. More information can be found at official documentation.\nWhile the examples used the llama2:13b-chat model, Ollama supports other models. A complete list is available at here.\nConclusion Ollama is a valuable tool for experimenting with Large Language Models locally, eliminating the reliance on cloud services. Its ease of use and support for multiple programming languages make it a powerful tool. The future development of Ollama is promising.\n","date":"2023-11-14T01:32:05.251+01:00","image":"https://raw.githubusercontent.com/Nexus-Security/image-1/main/2024227/ollama-cover.webp","permalink":"https://Nexus-Security.github.io/ollama-using-large-language-models-on-your-local-machine/","title":"Ollama: using Large Language Models on your local machine"},{"content":"After publishing an article, you\u0026rsquo;ll probably want to share it on social media or with friends. To make the link more appealing, include a cover image for the link preview.\nThis is done using the og:image Open Graph tag.\nOpen Graph Created by Meta (formerly Facebook), Open Graph (protocol) simplifies sharing articles and web pages on social media. Just include a few meta tags in your webpage header.\nKey tags:\nog:title: Page\u0026rsquo;s main title, typically the largest text displayed. og:description: Additional text below the title. og:image: Crucially, this tag displays the image, occupying the most space. Pelican Your Pelican website template might already support the og:image tag.\nFor instance, I add the Cover attribute to page meta tags:\n1 2 3 4 5 6 7 8 Title: How to show a cover image in Pelican based blog posts Date: 2023-06-17 14:15 Author: Andrea Grandi Category: Development Tags: Pelican, Python, blog, writing, images, cover, template, open, graph, meta Status: published Summary: How to add a cover image to Pelican based blog posts, so that when the article is shared on social media the image is shown in the preview. Cover: images/2024/02/pelican-website-banner.jpeg My template uses a sub-template (located at here) that renders the image:\n1 2 3 4 5 {% if \u0026#39;cover\u0026#39; in article.metadata %} \u0026lt;meta property=\u0026#34;og:image\u0026#34; content=\u0026#34;{{ SITEURL }}/{{ article.metadata[\u0026#39;cover\u0026#39;] }}\u0026#34;\u0026gt; {% else %} \u0026lt;meta property=\u0026#34;og:image\u0026#34; content=\u0026#34;{{ SITELOGO }}\u0026#34;\u0026gt; {% endif %} Note: The template defaults to the SITELOGO image if no Cover is specified.\nIf your template doesn\u0026rsquo;t offer this, consider these plugins (refer to their GitHub readme for usage):\npelican-seo plugin pelican-seo is beneficial even if your template supports cover images. It automatically adds Open Graph tags, improving your website\u0026rsquo;s SEO.\nfeatured-image plugin featured-image is simpler. It uses an existing image from the meta tags or the first image found on the page if one isn\u0026rsquo;t specified.\nConclusion Adding a cover image to blog posts significantly enhances their appeal when shared on social media.\n","date":"2023-11-13T03:49:40.307+01:00","image":"https://raw.githubusercontent.com/Nexus-Security/image-1/main/2024227/pelican-website-banner.jpeg","permalink":"https://Nexus-Security.github.io/how-to-display-a-cover-image-in-blog-posts-using-pelican/","title":"How to display a cover image in blog posts using Pelican"},{"content":"In my software development work, I use tools like GitHub Copilot and ChatGPT daily. However, I\u0026rsquo;m interested in exploring self-hosted alternatives that don\u0026rsquo;t depend on external services.\nDisclaimer It\u0026rsquo;s important to understand that these tools, while helpful, don\u0026rsquo;t replace the developer\u0026rsquo;s role. A common misconception is that AI tools can do the job for you, which is simply not true.\nDevelopers are still crucial for problem-solving, understanding requirements, and applying generated code effectively. Automating code snippets from sources like Google or Stack Overflow doesn\u0026rsquo;t change this; the developer remains responsible for the final result.\nThe experiment Following my recent exploration of running LLMs locally with Ollama previous post, I wondered if these models could replace GitHub Copilot.\nMy goal isn\u0026rsquo;t to cut costs (my employer covers these tools) or prioritize privacy (keeping code \u0026ldquo;secret\u0026rdquo; is moot when using GitHub). Instead, I\u0026rsquo;m curious about the capabilities of these alternatives – and doing this for the fun of it!\nSetup and available resources My main machine is a MacBook Pro with an M2 Pro CPU and 32 GB RAM. This setup comfortably handles my core requirements:\nSlack Docker (for work containers) VSCode Safari / Chrome The question is: can it also handle a Copilot substitute?\nTested models and extensions To replicate Copilot\u0026rsquo;s functionality, you need an LLM and a compatible VSCode extension (similar extensions exist for other IDEs).\nI\u0026rsquo;m running the models locally using Ollama. For details on its usage, refer to my previous post.\nThe tested models are:\nstable-code:3b-code-q4_0 codellama:7b-code-q4_K_M codellama:13b-code-q4_K_M The number preceding \u0026ldquo;b\u0026rdquo; signifies the parameter count (billions), with each model requiring roughly 4 GB, 8 GB, and 16 GB of RAM, respectively.\nMeta offers even more potent models with 34 billion and 70 billion parameters (found at https://ollama.com/library/codellama), but these demand 64-128 GB RAM.\nMy tested extensions include:\nLLama Coder https://marketplace.visualstudio.com/items?itemName=ex3ndr.llama-coder Code GPT https://marketplace.visualstudio.com/items?itemName=DanielSanMedium.dscodegpt twinny https://marketplace.visualstudio.com/items?itemName=rjmacarthy.twinny Results My findings were inconsistent, primarily influenced by the LLM model rather than the extension.\nWhile stable-code:3b-code-q4_0 provided rapid code completion, its output was frequently incorrect or nonsensical. It struggled with basic Python method structuring and indentation.\nModels like codellama:7b-code-q4_K_M and codellama:13b-code-q4_K_M demonstrated improved accuracy. However, despite my system\u0026rsquo;s resources, their 3-4 second completion times rendered them impractical for my use.\nNone of the tested models came close to matching GitHub Copilot\u0026rsquo;s speed and accuracy.\nConclusion Although the concept of a personal, private code assistant is appealing, replicating GitHub Copilot\u0026rsquo;s performance locally proves difficult with current technology.\nHowever, I\u0026rsquo;m optimistic about the future. As models and extensions evolve, we can expect enhancements in quality and potentially reduced resource requirements.\nI welcome any suggestions for superior models or extensions; feel free to share in the comments. I\u0026rsquo;m always eager to experiment and update this post or create new content.\nFor now, GitHub Copilot remains my preferred choice.\n","date":"2023-11-06T11:43:36.636+01:00","image":"https://raw.githubusercontent.com/Nexus-Security/image-1/main/2024227/self-hosting-copilot-cover.webp","permalink":"https://Nexus-Security.github.io/my-personal-experience-with-self-hosting-a-copilot-alternative/","title":"My personal experience with self-hosting a Copilot alternative"},{"content":" If you own a Unifi device, such as a Dream Machine, Dream Machine Pro, UNVR, CloudKey, or a similar model, you\u0026rsquo;ve likely encountered the infamous red triangle accompanied by the frustrating message \u0026ldquo;Your connection is not private.\u0026rdquo;\nAs of now, there isn\u0026rsquo;t an official or integrated method for generating a security certificate for your device. However, some intriguing GitHub projects suggest potential solutions if you\u0026rsquo;re willing to experiment. While my approach might not be as elegant as directly generating certificates using LetsEncrypt on the server-side like the linked GitHub project, it offers a reliable solution.\nMy solution involves using an NGINX reverse proxy. This setup forwards traffic from a subdomain linked to my primary domain (e.g., unifi.whitematter.tech, protect.whitematter.tech) to my Dream Machine Pro or UNVR, depending on my needs. For guidance on creating a reverse proxy with Docker, you can refer to my previous post here.\nAfter configuring your reverse proxy and adding a subdomain to your DNS records (e.g., unifi.whitematter.tech) to point to your public IP address, you\u0026rsquo;re ready for the next step.\nThis configuration assumes you\u0026rsquo;ve directed your subdomain (e.g., unifi.whitematter.tech) to your public IP. If your public IP address isn\u0026rsquo;t static, consider using a free dynamic DNS service like DuckDNS. This service automatically updates your DNS records to reflect your IP address if your ISP changes it. My private domain is linked to a DuckDNS hostname for this reason.\nDisclosure: As an Amazon Associate, I earn from qualifying purchases. Your support through these links helps maintain this blog, and the product pricing remains the same for you regardless.\nSetting Up the Reverse Proxy for Unifi Devices Configuring the reverse proxy is fairly straightforward. The image below demonstrates the correct setup. In the \u0026ldquo;Domain Names\u0026rdquo; field, enter your domain (e.g., unifi.whitematter.tech). Choose \u0026ldquo;https\u0026rdquo; for the \u0026ldquo;Scheme\u0026rdquo; and specify your Unifi Dream Machine\u0026rsquo;s local IP or other Unifi device\u0026rsquo;s IP address (e.g., 192.168.1.1, 10.0.0.1) in the \u0026ldquo;Forward Hostname / IP\u0026rdquo; field. Set the \u0026ldquo;Forward Port\u0026rdquo; to 443. I generally enable \u0026ldquo;Cache Assets.\u0026rdquo; Given that Unifi devices are secured with a Unifi account, I leave the \u0026ldquo;Access List\u0026rdquo; as public. While you can add username/password protection, I suggest using a strong Unifi password for enhanced security.\nNext, navigate to \u0026ldquo;SSL\u0026rdquo; in the top menu, as illustrated in the subsequent image. Select \u0026ldquo;Request a new SSL Certificate\u0026rdquo; and enable the options for \u0026ldquo;Force SSL,\u0026rdquo; \u0026ldquo;HTTP/2 Support,\u0026rdquo; and \u0026ldquo;HSTS Enabled.\u0026rdquo; Input your email address, which will be used for Let\u0026rsquo;s Encrypt certificate generation, and after reviewing, agree to the Let\u0026rsquo;s Encrypt terms. Save your settings by clicking \u0026ldquo;Save,\u0026rdquo; and you\u0026rsquo;re all set!\nConclusion If you\u0026rsquo;ve followed these instructions carefully and your DNS records are accurate, you should be able to access your Unifi device using the Let\u0026rsquo;s Encrypt-generated certificate by going to unifi.yourdomain.com. The bothersome error message should no longer appear when connecting. Additionally, when you examine the certificate details linked to your Unifi domain, you\u0026rsquo;ll be redirected to https://letsencrypt.org/documents/isrg-cps-v4.1/.\nI appreciate this solution because it simplifies access by requiring only your subdomain instead of an IP address. It also allows you to connect to your device without relying on Unifi\u0026rsquo;s cloud-based connection when you\u0026rsquo;re outside your local network.\nRemember that whenever you make services publicly accessible, it\u0026rsquo;s crucial to prioritize security with strong passwords. Password managers, such as 1Password, and hardware security keys like Yubikey, can significantly enhance your security practices.\n","date":"2023-11-01T23:30:24Z","image":"https://raw.githubusercontent.com/Nexus-Security/image-1/main/header_how-to-connect-to-your-unifi-dream-machine-or-unvr-with-ssl-from-lets-encrypt.png","permalink":"https://Nexus-Security.github.io/connecting-to-your-unifi-dream-machine-or-unvr-using-ssl-from-lets-encrypt/","title":"Connecting to your Unifi Dream Machine or UNVR using SSL from Let's Encrypt"},{"content":" Introduction This guide explains how to set up a Cloudflared container within your Docker environment, creating a secure tunnel from Cloudflare to your internal network resources. Once set up, this tunnel eliminates the need to expose ports 80 and 443.\nThis guide presumes you have an operational internal network with a configured reverse proxy like Nginx Proxy Manager. It also assumes you possess a Cloudflare account with DNS management capabilities for your domain.\nUPDATE 04/17/2023 There\u0026rsquo;s now a simplified method for running Cloudflared containers that streamlines the process described below. Cloudflare allows you to store the configuration in the cloud, eliminating the need for local credential files and simplifying the setup. All you need is a tunnel token from the Cloudflare Zero Trust dashboard.\nYou can now build the entire tunnel locally using docker-compose. Here\u0026rsquo;s an updated docker-compose.yml file that simplifies the process. Just replace YOUR_TOKEN_FROM_CLOUDFLARE with your actual tunnel token from the Cloudflare dashboard:\n1 2 3 4 5 6 7 8 9 10 11 version: \u0026#39;3\u0026#39; services: cloudflared-tunnel: image: cloudflare/cloudflared:latest container_name: cloudflared environment: - TZ=America/New_York - TUNNEL_TOKEN=YOUR_TOKEN_FROM_CLOUDFLARE restart: always command: tunnel run network_mode: \u0026#34;host\u0026#34; If you prefer to set up the tunnel manually, continue reading the original guide below. A more comprehensive tutorial for the cloud-based method will be available in the future.\nThe Cloudflared Project This project utilizes Docker-Compose and Docker on a Ubuntu VM. Initially, this was set up on an Unraid system using the _aeleos/cloudflared_ container, instructions for which can be found at https://github.com/aeleos/cloudflared. The setup has since migrated to Ubuntu for hosting and site independence, but the Unraid instructions remain relevant for those using Unraid.\nFor simplicity, the necessary files will be stored at _/docker-host/cloudflared-whitemattertech/_. If you have multiple domains requiring tunneling, create separate folders like _/docker-host/cloudflared-domain2/_ and _/docker-host/cloudflared-domain3/_. Each step needs to be performed individually for each domain.\nBegin by logging into Cloudflare from your terminal:\n1 docker run --rm -v /.cloudflared:/home/nonroot/.cloudflared/ cloudflare/cloudflared:latest tunnel login After running the command, you\u0026rsquo;ll see output similar to the image below: Use the generated URL from the terminal output to log into your Cloudflare account. Upon successful login, the required certificate will be saved to your /docker-host/cloudflared-whitemattertech/.cloudflared directory.\nNext, create a new tunnel using this terminal command, replacing TUNNELNAMEHERE with your desired name:\n1 docker run --rm -v /docker-host/cloudflared-whitemattertech/.cloudflared:/home/nonroot/.cloudflared/ cloudflare/cloudflared:latest tunnel create TUNNELNAMEHERE Verify the tunnel\u0026rsquo;s status by running:\n1 docker run --rm -v /docker-host/cloudflared-whitemattertech/.cloudflared:/home/nonroot/.cloudflared/ cloudflare/cloudflared:latest tunnel list Make note of the tunnel ID from the output for later use.\nNow, create and edit a configuration file named _config.yaml_:\n1 2 touch /docker-host/cloudflared-whitemattertech/.cloudflared/config.yaml nano /docker-host/cloudflared-whitemattertech/.cloudflared/config.yaml Paste the following content into config.yaml, replacing the _xxx\u0026hellip;_ with your copied tunnel ID:\n1 2 3 4 5 6 7 tunnel: xxxxxxxxxxxxxxxxxxxxxxxxxx credentials-file: /home/nonroot/.cloudflared/xxxxxxxxxxxxxxxxxxxxxxxx.json ingress: - service: https://localhost:443 originRequest: originServerName: whitematter.tech Under \u0026ldquo;ingress\u0026rdquo;, service refers to your reverse proxy host. If different from the example, replace localhost:443 with your reverse proxy IP. originServerName should be your root domain. If you encounter issues starting the container, try using a subdomain like auth.whitematter.tech as the originServerName. A single subdomain is sufficient, regardless of how many you plan to use with the tunnel.\nNext, create the docker-compose.yml file:\n1 2 touch /docker-host/cloudflared-whitemattertech/docker-compose.yml nano /docker-host/cloudflared-whitemattertech/docker-compose.yml For your convenience, the example below includes commented-out configurations for additional domains. If you\u0026rsquo;re using more than one domain, uncomment and update those lines with your specific paths.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 version: \u0026#34;3\u0026#34; services: cloudflared-whitematter: image: cloudflare/cloudflared:latest container_name: cloudflared-whitematter environment: - TZ=America/New_York volumes: - /docker-host/cloudflared-whitematter/:/home/nonroot/.cloudflared/ restart: always command: tunnel run network_mode: \u0026#34;host\u0026#34; # cloudflared-domain1: # image: cloudflare/cloudflared:latest # container_name: cloudflared-domain1 # environment: # - TZ=America/New_York # volumes: # - /docker-host/cloudflared-domain1/:/home/nonroot/.cloudflared/ # restart: always # command: tunnel run # network_mode: \u0026#34;host\u0026#34; # cloudflared-domain2: # image: cloudflare/cloudflared:latest # container_name: cloudflared-domain2 # environment: # - TZ=America/New_York # volumes: # - /docker-host/cloudflared-domain2/:/home/nonroot/.cloudflared/ # restart: always # command: tunnel run # network_mode: \u0026#34;host\u0026#34; # cloudflared-domain3: # image: cloudflare/cloudflared:latest # container_name: cloudflared-domain3 # environment: # - TZ=America/New_York # volumes: # - /docker-host/cloudflared-domain3/:/home/nonroot/.cloudflared/ # restart: always # command: tunnel run # network_mode: \u0026#34;host\u0026#34; Finally, start your container(s) with this command:\n1 docker-compose up -d Cloudflare DNS Modifications In your Cloudflare DNS settings, replace any existing A records with CNAME records. The Name field should point to your root domain (@), and the Value field should be set to TUNID.cfargotunnel.com, replacing TUNID with your actual tunnel ID.\nFor subdomains, create a CNAME record for each. Set the Name field to the subdomain (e.g., auth, plex, unraid) and the Value field to \u0026ldquo;_@_\u0026rdquo;. After saving, Cloudflare will automatically replace \u0026ldquo;_@_\u0026rdquo; with your root domain name. For reference, see the image below: Once these changes are saved, your internal applications should be accessible through the Cloudflare tunnel. You can now disable port forwarding for ports 80 and 443 to your reverse proxy. Performing a _dig_ command on your domain or subdomain will confirm that your services no longer point to your external IP, enhancing the privacy of your self-hosted applications.\nWrapping Up Hopefully, this guide has provided you with the necessary steps to successfully set up your own Cloudflare tunnel.\n","date":"2023-09-28T11:56:17Z","image":"https://raw.githubusercontent.com/Nexus-Security/image-1/main/logo.png","permalink":"https://Nexus-Security.github.io/using-cloudflared-for-tunneling-to-internal-resources/","title":"Using Cloudflared for Tunneling to Internal Resources"},{"content":"Introduction This guide demonstrates setting up AlienVault OSSIM as a virtual machine within Unraid. OSSIM, a robust open-source SIEM (Security Information and Event Management) system, offers free network monitoring and protection. Its applications include network-wide vulnerability assessments and intrusion detection on endpoint devices.\nThe integrated HIDS within OSSIM originates from OSSEC. Moreover, OSSIM seamlessly integrates with Open Threat Exchange (OTX), a platform available for Windows, Mac, and Linux systems. This integration provides an updated, open-source vulnerability scanning tool. Deploying the OTX installer is made possible through platforms like Mosyle (MDM for MacOS) and Intune (MDM for Windows).\nWhile a custom-built Unraid server serves as the primary system, getting OSSIM to run on it as a VM was a challenge previously. Past attempts resulted in the VM failing to boot after installation. This guide outlines the successful configuration settings for installing and running OSSIM on Unraid, potentially filling a gap in available resources.\nDownload OSSIM The OSSIM installation ISO is freely downloadable from: https://cybersecurity.att.com/products/ossim/download. As an open-source solution, it\u0026rsquo;s free to use.\nAlternatively, you can use Unraid\u0026rsquo;s terminal or SSH to download the ISO directly. Ensure you\u0026rsquo;re in the desired directory (e.g., /mnt/user/iso/) and use the command:\n1 wget https://dlcdn.alienvault.com/AlienVault_OSSIM_64bits.iso Prepare Unraid If downloaded locally, transfer the ISO file to your Unraid server\u0026rsquo;s file share. If using \u0026lsquo;wget,\u0026rsquo; the ISO is ready for use.\nCreate a New VM The key to running OSSIM within Unraid lies in selecting \u0026lsquo;Debian\u0026rsquo; as the VM host. Under the \u0026lsquo;VMs\u0026rsquo; tab in Unraid, choose \u0026lsquo;ADD VM\u0026rsquo; and select \u0026lsquo;Debian\u0026rsquo; under the \u0026lsquo;Linux\u0026rsquo; section.\nConfigure the VM Maintain the default \u0026lsquo;Host Passthrough\u0026rsquo; CPU mode. Allocate desired cores/threads (e.g., 8 cores for a Ryzen 9 3950x). Set both Initial and Max Memory to 8192MB.\nSelect the following:\nMachine: Q35-6.0 BIOS: SeaBIOS USB Controller: 3.0 (nec XHCI) OS Install ISO: Path to the downloaded OSSIM ISO (e.g., /mnt/user/iso/AlienVault_OSSIM_64bits.iso) Choose \u0026lsquo;SATA\u0026rsquo; for \u0026lsquo;OS Install CDRom Bus.\u0026rsquo; Specify the location and size of the OSSIM VM\u0026rsquo;s virtual disk under \u0026lsquo;Primary vDisk Location,\u0026rsquo; opting for \u0026lsquo;SATA\u0026rsquo; again for \u0026lsquo;Primary vDisk Bus.\u0026rsquo;\nLeave remaining settings until \u0026lsquo;Network MAC\u0026rsquo; untouched. Add a second virtual network interface by clicking the \u0026lsquo;+\u0026rsquo; in the network pane\u0026rsquo;s bottom left corner. For the new \u0026lsquo;2nd Network MAC,\u0026rsquo; duplicate the original MAC address and modify the last character. Keep the rest as default.\nStart the VM Save the VM configuration and start the machine. Follow the standard OSSIM installation process. The designated vDisk will be automatically chosen for installation. When prompted to install the GRUB bootloader, skip this step.\nAfter installation and upon restarting the VM, you\u0026rsquo;ll be able to set up the OSSIM administrator credentials.\nWrapping Up OSSIM\u0026rsquo;s capabilities come with complexity. For newcomers, extensive research on SIEM tools and OSSIM-specific configuration is advised. While potentially exceeding typical home lab needs, mastering OSSIM is valuable for any security professional.\n","date":"2023-09-25T19:02:09Z","image":"https://raw.githubusercontent.com/Nexus-Security/image-1/main/ossimunraid.png","permalink":"https://Nexus-Security.github.io/running-alienvault-ossim-as-a-virtual-machine-on-unraid/","title":"Running AlienVault OSSIM as a virtual machine on Unraid"},{"content":"Working from home during the COVID-19 pandemic has been a mixed bag for many. While I enjoyed the increased time at home, the closure of coffee shops was a definite downside. As someone who thrives on freshly brewed coffee, I missed the aroma and the experience of enjoying a meticulously crafted beverage.\nTo satisfy my caffeine cravings, I invested in high-quality coffee equipment. Not only can I experiment with various coffee drinks, but I\u0026rsquo;ve also saved a considerable amount of money. In this post, I\u0026rsquo;ll share my favorite coffee gear. Stay tuned for future posts where I\u0026rsquo;ll detail the specific drinks I create.\nDisclosure: As an Amazon Associate, I earn from qualifying purchases. Your support helps maintain this blog. Rest assured, prices remain the same for you whether you use my links or not. Thank you for your support!\nEspresso My espresso machine, a Breville Barista Express, was chosen for its balance of manual control, built-in grinder, and affordability. This machine allows me to grind beans, pull shots, steam milk, and craft delicious espresso drinks with ease.\n[Image of Breville Barista Express Espresso Machine with link to purchase on Amazon]\nTo maintain a clean workspace while using the espresso machine, I rely on a rubber tamper mat. This simple addition simplifies cleanup and protects my counter.\n[Image of rubber tamper mat with link to purchase on Amazon]\nDid you know spent espresso grounds make excellent fertilizer? I utilize mine to nourish my plants, reducing the need for store-bought fertilizers. A coffee knock box helps collect the used grounds for easy transfer to my garden.\n[Image of coffee knock box with link to purchase on Amazon]\nCoffee Pour Over My preferred pour-over setup consists of a Chemex coffee maker and Chemex filters, known for producing a clean and flavorful cup.\n[Image of Chemex coffee maker with link to purchase on Amazon]\nFor precise temperature control during pour-over brewing, I use an electric kettle. I recommend the OVALWARE Electric Kettle for its granular temperature settings.\n[Image of OVALWARE Electric Kettle with link to purchase on Amazon]\nAnother excellent electric kettle option, particularly if you enjoy tea, is the COSORI Electric Gooseneck Kettle. Its pre-set temperature options simplify the brewing process for various beverages.\n[Image of COSORI Electric Gooseneck Kettle with link to purchase on Amazon]\nAccurate measurements are crucial for pour-over coffee. I use a reliable and affordable scale recommended by a barista friend.\n[Image of coffee scale with link to purchase on Amazon]\nVietnamese Coffee For Vietnamese coffee, specifically the iced variety known as cafe sua da, I utilize a traditional Vietnamese coffee filter.\n[Image of Vietnamese coffee filter with link to purchase on Amazon]\nI use the same electric kettles mentioned earlier for Vietnamese coffee preparation.\nDrip Coffee Finding the perfect drip coffee maker was challenging. I desired a machine with a quality built-in grinder for fresh grounds, versatile brewing options, from single servings to full carafes, and programmable scheduling for automated morning coffee. The Breville BDC650BSS Grind Control fulfilled all my requirements.\n[Image of Breville BDC650BSS Grind Control Coffee Maker with link to purchase on Amazon]\nCold Brew My cold brew consumption outpaced my previous setup, so I upgraded to a gallon-sized mason jar with a dispenser.\n[Image of gallon mason jar with dispenser and link to purchase on Amazon]\nNitro Cold Brew To infuse my cold brew with nitrogen, I utilize a Royal Nitro Coffee Maker. This system creates smooth and creamy nitro cold brew at home.\n[Image of Royal Nitro Coffee Maker with link to purchase on Amazon]\nNitrous oxide canisters, commonly used for whipped cream dispensers, are necessary for infusing the coffee.\n[Image of nitrous oxide canisters with link to purchase on Amazon]\nSiphon Coffee While more involved, siphon coffee offers a visually impressive and flavorful brewing experience. My current model is the Hario Technica 5-Cup Syphon Coffee Maker.\n[Image of Hario Technica 5-Cup Syphon Coffee Maker with link to purchase on Amazon]\nThe Belgian Family Balance Siphon, although pricier, boasts an even more captivating design. Perhaps one day I\u0026rsquo;ll have the pleasure of trying it.\nWrapping Up There\u0026rsquo;s a world of coffee brewing possibilities to explore. I encourage you to try these methods and discover your favorites. Feel free to share your experiences or alternative product recommendations. I\u0026rsquo;m always seeking new coffee adventures!\n","date":"2023-07-21T16:57:39Z","image":"https://raw.githubusercontent.com/Nexus-Security/image-1/main/header_coffee-gear.jpg","permalink":"https://Nexus-Security.github.io/the-essential-coffee-equipment-checklist-for-working-from-home/","title":"The Essential Coffee Equipment Checklist for Working from Home"},{"content":"This guide explains how to set up a Docker Registry GUI using Harbor on a Ubuntu virtual machine. The registry will operate within a Docker container. The tutorial uses docker-compose to create the necessary containers.\nDownloading and Unpacking the Harbor Installer You\u0026rsquo;ll begin by downloading the Harbor installer, extracting its contents, and navigating to the Harbor directory:\n1 2 3 4 5 wget https://github.com/goharbor/harbor/releases/download/v2.3.1/harbor-offline-installer-v2.3.1.tgz tar -xzf harbor-offline-installer-v2.3.1.tgz cd harbor/ Generating SSL Certificates for Internal Use This section focuses on generating SSL certificates for internal registry use. For external access, obtain certificates from a trusted Certificate Authority. These steps are Linux-specific.\nOpen a terminal and execute the commands below, replacing the placeholder text with your organization\u0026rsquo;s details. In the cat command where [alt_names] appears, substitute DNS.1, DNS.2, and DNS.3 with your host machine\u0026rsquo;s local IP or FQDN. If your machine\u0026rsquo;s name is \u0026ldquo;hostname\u0026rdquo; and your network domain is \u0026ldquo;.local\u0026rdquo;, use \u0026ldquo;hostname.local\u0026rdquo; and \u0026ldquo;hostname\u0026rdquo; as your DNS entries. You can include the static IP as well. Remove any unnecessary DNS entries. If you encounter issues with the cat command, download a pre-created file V3.ext.\nAfter creating the certificates, store them securely for adding as \u0026ldquo;trusted\u0026rdquo; on machines accessing the registry.\nPreparing for Installation A pre-made docker-compose.yml file is available for deploying the registry and Harbor GitHub. This file should be placed in the Harbor directory created earlier. Retrieve the file using either curl or wget:\n1 curl -LJO [https://raw.githubusercontent.com/Nexus-Security/Harbor-Registry/main/docker-compose.yml](http://curl -LJO https://raw.githubusercontent.com/Nexus-Security/Harbor-Registry/main/docker-compose.yml \u0026#34;https://raw.githubusercontent.com/Nexus-Security/nginx-proxy-manager/main/docker-compose.yml\u0026#34;) OR\n1 wget --no-check-certificate --content-disposition [https://raw.githubusercontent.com/Nexus-Security/Harbor-Registry/main/docker-compose.yml](https://raw.githubusercontent.com/Nexus-Security/Harbor-Registry/main/docker-compose.yml) Alternatively, you can create your own docker-compose.yml file by taking the content from the provided GitHub link.\nInstalling the Registry and Harbor With everything in place, run the following command to start the installation process:\n1 docker-compose up -d This might take several minutes to complete. Upon successful completion, the containers for your Docker Registry and Harbor (the GUI) will be operational. You can then access the Harbor GUI via a web browser using https://hostname, https://hostname.local, or https://IP_OF_HOST assuming the SSL certificates are trusted.\nManaging Harbor Harbor\u0026rsquo;s official documentation at https://goharbor.io/docs/2.3.0/administration/ offers comprehensive information about Harbor administration.\nConclusion After completing these steps, you should have a functional Docker Registry and Harbor setup. Remember to explore Docker image and container tagging for effective registry management.\n","date":"2023-07-20T20:35:49Z","image":"https://raw.githubusercontent.com/Nexus-Security/image-1/main/harbor.png","permalink":"https://Nexus-Security.github.io/running-a-locally-hosted-docker-registry-gui-with-harbor/","title":"Running a Locally Hosted Docker Registry GUI with Harbor"},{"content":"After sharing my guide on routing Docker containers through a VPN on Unraid, I received many questions about accessing Twitter anonymously. This post explains my method for anonymously accessing Twitter feeds without an account.\nThis guide assumes you understand how to route Docker containers through a VPN on Unraid. If not, please refer to my previous post on the topic. While I use Unraid, these steps can be adapted for Docker on other operating systems.\nNitter Nitter (zedeus/nitter) is a third-party Docker container that allows anonymous Twitter feed access. It acts as an intermediary, fetching data for you so your device never interacts directly with Twitter. This prevents Twitter from tracking your IP address or browser fingerprint. Behind a VPN, Nitter becomes a private, self-hosted Twitter reader that doesn\u0026rsquo;t require an account or track your activity.\nPublic Nitter instances are available if you prefer not to host your own. However, you can use a reverse proxy to connect a custom domain like \u0026ldquo;twitter.your-domain.com\u0026rdquo; to your own Nitter instance.\nNitter\u0026rsquo;s interface lets you search for users and view their feeds. Clicking the RSS icon on a user\u0026rsquo;s profile provides an RSS feed URL, which we\u0026rsquo;ll use in the next step.\nFreshRSS FreshRSS (linuxserver/freshrss) is a Docker container that manages RSS feeds. I recommend using it alongside your Nitter container, even with a public instance, and routing it through a VPN for maximum privacy and security.\nAfter setting up the container, add your desired Nitter RSS feeds. Create categories to organize your feeds within FreshRSS. When adding a feed, you can also enhance your privacy by using a SOCKS5 proxy.\nFor convenient access, use a reverse proxy to link your FreshRSS instance to a custom domain.\nI highly recommend setting up strong HTTP authentication if you make either Nitter or FreshRSS publicly accessible. A password manager can help generate and securely store strong passwords.\nConclusion You\u0026rsquo;ve successfully set up an anonymous way to access Twitter feeds while enhancing your privacy and minimizing data tracking. Congratulations!\n","date":"2023-07-12T22:48:50Z","image":"https://raw.githubusercontent.com/Nexus-Security/image-1/main/header_how-to-access-twitter-without-an-account-anonymously.jpg","permalink":"https://Nexus-Security.github.io/accessing-twitter-without-an-account-anonymously/","title":"Accessing Twitter without an account, anonymously"},{"content":"I\u0026rsquo;ve switched from using Traefik and WordPress for my website. Now, I use an internal Nginx proxy within Docker and my site is built with Hugo. To learn more about this change, you can read my post: Migrating from WordPress to Hugo\nThis first post will guide you on how to establish your own self-hosted Wordpress website using Docker, mirroring the setup of this very site. For this example, I\u0026rsquo;m using a physical Ubuntu machine connected to a Unifi Dream Machine Pro. You have the flexibility to employ a VPS or any bare-metal operating system that supports Docker. While this tutorial focuses on Ubuntu-specific tools, adjustments can be made for other environments if you\u0026rsquo;re comfortable with Docker.\nDisclosure: As an Amazon Associate, I earn from qualifying purchases. Your support through these links helps maintain this blog, but rest assured, the pricing remains the same for you whether you use my links or not. Thanks for your support!\nThis guide presumes you have Ubuntu installed, access to a user account with root privileges, Docker and Docker-Compose set up, a domain name under your ownership (consider Google Domains or Hover if you need one), the ability to manage your domain\u0026rsquo;s DNS records (CloudFlare is recommended), and the capability to forward ports on your network (unless using a VPS or externally hosted OS).\nUbuntu\u0026rsquo;s environment file system is particularly useful for securely storing sensitive variables. This simplifies sharing docker-compose files without exposing confidential information on platforms like GitHub. While there might be other methods for managing secrets within Docker, they won\u0026rsquo;t be covered in this tutorial. Lastly, prioritize security when hosting a website on your network. Additionally, if you\u0026rsquo;re running multiple externally accessible web servers, you might find this post on reverse proxies helpful: https://nexus-security.github.io/running-a-reverse-proxy-using-docker-made-simple/.\nPrepare Your Domain For your domain (e.g., whitematter.tech), add DNS records that direct it to your server\u0026rsquo;s public IP address. For home networks, this is the public IP provided by your ISP, which can be found at whatismyip.com. Personally, I prefer dynamic DNS solutions like duckdns.org, allowing automatic updates to my public IP in the absence of a static IP from my ISP. For a better understanding of how to integrate duckdns, refer to this video. The following image illustrates how CloudFlare is configured for my domain, pointing to my duckdns address (without duckdns, \u0026ldquo;Content\u0026rdquo; would be your public IP, such as 177.99.88.10).\nSet Environment Variables In Ubuntu, access the Environment file with the command:\n1 sudo nano /etc/environment Below the PATH variable, paste the following, replacing the generic values in red with your specific information. Do not modify the PATH variable\u0026rsquo;s content.\nFor your time zone (TZ), refer to this list.\nReplace “username” in the USERDIR with your Ubuntu username. It\u0026rsquo;s strongly recommended to use a password manager like 1Password to generate and securely store passwords for MySQL and your WordPress databases.\n1 2 3 4 5 6 7 8 9 PATH=\u0026#34;/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin:/usr/games:/usr/local/games\u0026#34; TZ=America/New_York USERDIR=/home/username/docker MYSQL_ROOT_PASSWORD=dbrootpassword DATABASE_NAME=dbname DATABASE_USER=dbuser DATABASE_PASSWORD=dbpassword DOMAIN=yourdomain.com Save the file (“Ctrl” + “o” followed by “Enter”) and exit (“Ctrl” + “x”).\nA logout/login or a system reboot is needed for the changes to take effect:\n1 sudo reboot Preparing Directories and Permissions Execute the following commands. You can run them all at once or individually. ${USERDIR} will be populated from the environment file.\n1 2 3 4 5 6 7 8 sudo mkdir ${USERDIR} sudo mkdir ${USERDIR}/apps sudo mkdir ${USERDIR}/wordpress sudo mkdir ${USERDIR}/wordpress/wp-data sudo mkdir ${USERDIR}/data sudo mkdir ${USERDIR}/data/configurations sudo touch ${USERDIR}/data/acme.json sudo chmod 600 ${USERDIR}/data/acme.json Create Traefik static configuration file. Use the following command to create the Traefik static configuration file (${USERDIR} will be taken from your environment file):\n1 sudo nano ${USERDIR}/data/traefik.yml Paste the following content into the file, ensuring you replace the placeholder email address in red with your own.\nTwo caServer addresses are provided at the end of the file. The one containing \u0026ldquo;staging\u0026rdquo; is intended for testing purposes. While using the staging server certificates is beneficial during setup, be aware that browsers may display security warnings. The LetsEncrypt server has limitations on the number of certificates issued within a specific timeframe. To disable an address, simply add \u0026ldquo;#\u0026rdquo; at the beginning of the line – Ubuntu will ignore lines prefixed with \u0026ldquo;#.\u0026rdquo;\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 api: dashboard: true entryPoints: http: address: \u0026#34;:80\u0026#34; http: redirections: entryPoint: to: https https: address: \u0026#34;:443\u0026#34; http: middlewares: - secureHeaders@file - page-ratelimit@file tls: certResolver: letsencrypt providers: docker: endpoint: \u0026#34;unix:///var/run/docker.sock\u0026#34; exposedByDefault: false file: filename: /configurations/dynamic.yml certificatesResolvers: letsencrypt: acme: email: you@yourdomain.com storage: acme.json keyType: EC384 httpChallenge: entryPoint: http #caServer: https://acme-staging-v02.api.letsencrypt.org/directory caServer: https://acme-v02.api.letsencrypt.org/directory Save the file (“Ctrl” + “o” and “Enter”) and exit (“Ctrl” + “x”).\nPrepare Traefik and Create Traefik Dynamic Config For security, your Traefik password needs to be hashed using MD5, SHA1, or BCrypt. BCrypt is recommended for its stronger encryption.\nOnline Htpassword generators can be used for this purpose. For instance, if your username is \u0026ldquo;admin\u0026rdquo; and password is \u0026ldquo;password1234,\u0026rdquo; the BCrypt hash would resemble this: \u0026ldquo;admin:$2y$10$d0yk7WE.XqhF5bT1DdJhduRFOM5JSabTiSFCTnbC2.JgMolypHgS2.\u0026rdquo; Utilize a password manager (1Password is recommended) to generate a strong password for this step.\nOnce your username, password, and its hashed version are ready, proceed with the following:\nCreate the Traefik dynamic configuration file:\n1 sudo nano ${USERDIR}/data/configurations/dynamic.yml Paste the content below into the dynamic.yml file, replacing the placeholder username and password (in red) with your generated hashed credentials.\nThis configuration utilizes the page-ratelimit middleware. The provided values permit an average of 50 requests per second with a burst limit of 50. Feel free to adjust these values as needed.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 http: middlewares: secureHeaders: headers: frameDeny: true sslRedirect: true browserXssFilter: true contentTypeNosniff: true forceSTSHeader: true stsIncludeSubdomains: true stsPreload: true stsSeconds: 31536000 customFrameOptionsValue: SAMEORIGIN user-auth: basicAuth: users: - \u0026#34;admin:$2y$10$d0yk7WE.XqhF5bT1DdJhduRFOM5JSabTiSFCTnbC2.JgMolypHgS2\u0026#34; page-ratelimit: rateLimit: average: 50 burst: 50 tls: options: default: cipherSuites: - TLS_ECDHE_ECDSA_WITH_AES_256_GCM_SHA384 - TLS_ECDHE_RSA_WITH_AES_256_GCM_SHA384 - TLS_ECDHE_ECDSA_WITH_AES_128_GCM_SHA256 - TLS_ECDHE_RSA_WITH_AES_128_GCM_SHA256 - TLS_ECDHE_ECDSA_WITH_CHACHA20_POLY1305 - TLS_ECDHE_RSA_WITH_CHACHA20_POLY1305 minVersion: VersionTLS12 Save (“Ctrl” + “o” and “Enter”) and exit (“Ctrl” + “x”).\nCreate the Traefik Docker-Compose File Create the Traefik docker-compose file:\n1 sudo nano ${USERDIR}/docker-compose.yml Copy the content below into the file, noting that fields enclosed in ${ } will be filled in from the environment file.\nThis configuration uses the latest version of Traefik (currently v2.2.6). If you prefer to stick to the latest release within the 2.2.X series, specify the image tag as \u0026ldquo;traefik:chevrotin.\u0026rdquo; Be mindful that using the \u0026ldquo;latest\u0026rdquo; tag might lead to compatibility issues if future Traefik versions (v3, etc.) introduce breaking changes. Check dockerhub for available release tags.\nThis setup routes ports 80 (http) and 443 (https) to the Traefik container.\nYou can find a copy of this docker-compose.yml file on GitHub.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 version: \u0026#39;3.3\u0026#39; services: traefik: image: traefik:latest container_name: traefik restart: always security_opt: - no-new-privileges:true ports: - 80:80 - 443:443 volumes: - /var/run/docker.sock:/var/run/docker.sock:ro - ${USERDIR}/data/traefik.yml:/traefik.yml:ro - ${USERDIR}/data/configurations:/configurations - ${USERDIR}/data/acme.json:/acme.json environment: TZ: ${TZ} networks: - proxy labels: - \u0026#34;traefik.enable=true\u0026#34; - \u0026#34;traefik.docker.network=proxy\u0026#34; - \u0026#34;traefik.http.routers.traefik-secure.entrypoints=https\u0026#34; - \u0026#34;traefik.http.routers.traefik-secure.rule=Host(`traefik.${DOMAIN}`)\u0026#34; - \u0026#34;traefik.http.routers.traefik-secure.service=api@internal\u0026#34; - \u0026#34;traefik.http.routers.traefik-secure.middlewares=user-auth@file\u0026#34; networks: proxy: external: true Save (“Ctrl” + “o” and “Enter”) and exit (“Ctrl” + “x”).\nCreate the WordPress Docker-Compose File Create the WordPress docker-compose file:\n1 sudo nano ${USERDIR}/apps/docker-compose.yml Copy the content below, noting that fields enclosed in ${ } will be populated from the environment file.\ndocker-compose.yml\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 version: \u0026#39;3.7\u0026#39; services: db: image: mariadb:latest container_name: wp-db volumes: - db-data:/var/lib/mysql networks: - default security_opt: - no-new-privileges:true restart: unless-stopped environment: MYSQL_ROOT_PASSWORD: ${MYSQL_ROOT_PASSWORD} MYSQL_DATABASE: ${DATABASE_NAME} MYSQL_USER: ${DATABASE_USER} MYSQL_PASSWORD: ${DATABASE_PASSWORD} TZ: ${TZ} wordpress: depends_on: - db image: wordpress:latest container_name: wordpress environment: WORDPRESS_DB_HOST: db:3306 WORDPRESS_DB_NAME: ${DATABASE_NAME} WORDPRESS_DB_USER: ${DATABASE_USER} WORDPRESS_DB_PASSWORD: ${DATABASE_PASSWORD} TZ: ${TZ} volumes: - ${USERDIR}/wordpress/wp-data:/var/www/html networks: - proxy - default security_opt: - no-new-privileges:true restart: unless-stopped labels: - \u0026#34;traefik.enable=true\u0026#34; - \u0026#34;traefik.docker.network=proxy\u0026#34; - \u0026#34;traefik.http.routers.wordpress-secure.entrypoints=https\u0026#34; - \u0026#34;traefik.http.routers.wordpress-secure.rule=Host(`${DOMAIN}`)\u0026#34; portainer: container_name: portainer image: portainer/portainer:latest restart: unless-stopped command: -H unix:///var/run/docker.sock networks: - proxy - default security_opt: - no-new-privileges:true volumes: - /var/run/docker.sock:/var/run/docker.sock:ro environment: TZ: ${TZ} labels: - \u0026#34;traefik.enable=true\u0026#34; - \u0026#34;traefik.docker.network=proxy\u0026#34; - \u0026#34;traefik.http.routers.portainer-secure.entrypoints=https\u0026#34; - \u0026#34;traefik.http.routers.portainer-secure.rule=Host(`portainer.${DOMAIN}`)\u0026#34; volumes: db-data: networks: proxy: external: true Create the Proxy Network and Start Traefik With all files in place, create the proxy network and start Traefik.\nCreate the proxy network:\n1 docker network create proxy Start the Traefik container:\n1 docker-compose -f ${USERDIR}/docker-compose.yml up -d Allow a few minutes for startup, then try to access the Traefik web admin page at https://traefik.yourdomain.com (replace \u0026ldquo;yourdomain.com\u0026rdquo; with your domain, matching the one in the environment file). If successful, log in using the non-hashed username and password. You should see the following:\nIf you can\u0026rsquo;t access the page, you might need to forward ports 80 and 443 to your Ubuntu server (see the example screenshot for forwarding to Ubuntu at 10.100.0.15). This step is not required for VPS or externally hosted options. This post provides recommendations for securing your network. Once ports are forwarded, you can proceed.\nStart the Remaining Containers Important: Remember to log in to WordPress and Portainer immediately after starting them to set passwords and secure access.\nStart the remaining containers:\n1 docker-compose -f ${USERDIR}/apps/docker-compose.yml up -d Give the containers a few minutes to start, and then try to access WordPress and Portainer through your browser.\nWordPress:\nAccess WordPress at “https://yourdomain.com\u0026quot; (replace \u0026ldquo;yourdomain.com\u0026rdquo; with your domain).\nYou should land on the initial WordPress setup screen. Create a strong password for your WordPress user account. A password manager like 1Password can help with this.\nA success message will indicate a successful WordPress installation.\nPortainer:\nAccess Portainer at “https://portainer.yourdomain.com\u0026quot; (replace \u0026ldquo;yourdomain.com\u0026rdquo; with the domain from Section 1).\nYou\u0026rsquo;ll be directed to the Portainer setup screen. Create a strong password for your Portainer user account. Again, using a password manager like 1Password is recommended.\nAfter setting your Portainer password, log in. In the left-hand menu, click \u0026ldquo;containers.\u0026rdquo; This view lists all running containers. From here, you can stop, start, restart, remove, and otherwise manage installed containers.\nGetting Started with WordPress It\u0026rsquo;s a good practice to increase the PHP memory limit and maximum file upload size for new WordPress installations. One approach is to modify the .htaccess file. Access this file in the terminal (${USERDIR} will be taken from the environment file; the file path below assumes you\u0026rsquo;ve kept the WordPress volume as defined in the docker-compose file).\n1 sudo nano ${USERDIR}/wordpress/wp-data/.htaccess Add the following lines to the file:\n1 2 3 4 5 php_value memory_limit 256M php_value upload_max_filesize 128M php_value post_max_size 128M php_value max_execution_time 300 php_value max_input_time 1000 Save (“Ctrl” + “o” and “Enter”) and exit (“Ctrl” + “x”).\nRestart the WordPress container for the changes to take effect. You can either do this through Portainer or restart all containers:\n1 docker restart $(docker ps -q) WordPress Admin Page:\nYou can access your WordPress admin area at \u0026ldquo;https://yourdomain.com/wp-admin\" (replace \u0026ldquo;yourdomain.com\u0026rdquo; with your domain). Use the credentials you set up in Section 14 to log in.\nChange the WordPress Theme:\nYou can easily change your blog or website’s theme.\nClick “Add New Theme,” search for your desired theme, hover over it, and click “Install.” After installation, click “Activate.”\nCreate a Blog Post:\nTo add a new blog post, click “Posts,” then “Add New.\u0026rdquo; This will generate a blank post. The default WordPress block editor is a user-friendly option for creating content. If you prefer, you can install page builder plugins. Here’s what the default block editor looks like:\nAfter crafting your post, click “Publish,” and then click it again to make it live. You can view your post by navigating to the address displayed under “Post address.” Your blog post will resemble this. Customization can be achieved through settings and plugins.\n","date":"2023-06-29T05:46:49Z","image":"https://raw.githubusercontent.com/Nexus-Security/image-1/main/header_hosting-your-own-site-with-traefik-and-wordpress_main.jpg","permalink":"https://Nexus-Security.github.io/create-your-own-wordpress-website-for-free-using-traefik-and-docker/","title":"Create your own Wordpress website for free using Traefik and Docker"},{"content":" #UPDATE 11/05/2022 Original post date: 2023-07-29T15:38:58Z\nThis update details the configuration process for utilizing 4096-bit RSA keys, the SHA256 Auth digest algorithm, and AES256 encryption. The initial post used the default PIA key length of 2048, SHA1, and allowed for AES128.\nUpdated information will be tagged with #Update throughout this post.\nIntroduction This post will guide you on how to use pfSense within a Unifi network situated behind a Unifi Gateway, specifically a Unifi Dream Machine Pro (UDMP). I will outline my network configuration, encompassing subnets, VLANs, and wireless SSIDs, that are crucial for a successful setup. The ultimate goal is to enable the addition of a client to a specific VLAN on my Unifi network, either wirelessly by connecting to a designated SSID or by tagging the client\u0026rsquo;s physical port to that VLAN. This VLAN will correspond to a subnet that directs traffic through the pfSense machine, which functions as a VPN client (using Private Internet Access). This approach allows the UDMP to maintain its role as the DHCP server for these clients while pfSense handles the anonymization and encryption of their data.\nDisclosure: As an Amazon Associate, I earn from qualifying purchases. Your support helps maintain this blog. Please note that the pricing remains the same for you, regardless of whether you use my links.\nPrerequisites for this setup include: a Unifi Gateway device (e.g., UDMP, Unifi Security Gateway), a pfSense machine/VM, Unifi wireless APs (necessary for adding wireless devices to the VPN), and Unifi switches (required for tagging specific switch ports to the VPN). It is also assumed that you have an active VPN service subscription. This guide primarily focuses on PIA but can potentially be adapted for other VPN providers.\nWe will begin by configuring pfSense and the VPN connection, followed by the Unifi setup.\npfSense Setup and Configuration My custom pfSense machine consists of the following components: an Intel Core i5-8500 CPU, a GIGABYTE B365M DS3H motherboard, 16GB of Corsair Vengeance LPX RAM, a Thermaltake Smart 500W Power Supply, and a 4-Port PCI-E Network Interface Card.\nSelect a PIA Server Start by choosing an optimal server location, typically based on your country and geographic region. Using your PIA account credentials, access the complete server list at: https://www.privateinternetaccess.com/pages/ovpn-config-generator\nTo import the required certificate, select the 1198 port option and click \u0026ldquo;Generate.\u0026rdquo;\n#UPDATE For enhanced security with a 4096 RSA cert, download the .CRT file from: https://www.privateinternetaccess.com/openvpn/ca.rsa.4096.crt\nOpen the downloaded file in a text editor (e.g., Atom, Notepad++) and copy the entire content from -\u0026mdash;-BEGIN CERTIFICATE\u0026mdash;\u0026ndash; to -\u0026mdash;-END CERTIFICATE\u0026mdash;\u0026ndash;, as illustrated in the image below.\nCreate a Certificate Authority in pfSense Navigate to System \u0026gt; Cert Manager in pfSense and click on the \u0026ldquo;+ ADD\u0026rdquo; button. Select \u0026ldquo;Import an existing certificate authority\u0026rdquo; as the \u0026ldquo;Method\u0026rdquo; and paste the copied certificate text into the provided field, as shown below.\nSave the settings.\nThe imported certificate should now be visible in the list.\nConfigure OpenVPN Client With the certificate imported, go to VPN \u0026gt; OpenVPN, then Clients, and click ADD.\nConfigure the OpenVPN client settings according to the following images, replacing the Server Host, PIA Username, and Password with your own details. The server host information can be found in the .ovpn file downloaded earlier from PIA. Copy and paste the following into the \u0026ldquo;Custom options\u0026rdquo; box towards the end of the configuration page:\n1 remote-cert-tls server #UPDATE For enhanced security with the 4096 key, SHA256, and AES256 encryption, paste the following into the \u0026ldquo;Custom options\u0026rdquo; box instead of the text above:\n1 2 3 remote-cert-tls server; persist-key; persist-tun; #UPDATE When opting for the 4096 key, SHA256, and AES256 encryption, use Port 1197 instead of Port 1198, as shown in the image below.\n#UPDATE For the 4096 key, SHA256, and AES256 encryption configuration, select only AES-256-CBC under \u0026ldquo;Data Encryption Algorithms\u0026rdquo;.\nChoose AES-256-CBC (256 bit key, 128 bit block) for \u0026ldquo;Fallback Data Encryption Algorithm\u0026rdquo;.\nSet \u0026ldquo;Auth digest algorithm\u0026rdquo; to SHA256 (256-bit).\nEnabling \u0026ldquo;Hardware Crypto\u0026rdquo;, preferably Intel RDRAND engine - RAND, is highly recommended for enhanced encryption.\n#UPDATE When using the 4096 key, SHA256, and AES256 encryption, ensure to paste the following into the \u0026ldquo;Custom options\u0026rdquo; box:\n1 2 3 remote-cert-tls server; persist-key; persist-tun; pfSense Gateway and Interface Assignment Now comes the more complex part. This step involves creating a Gateway on pfSense for routing traffic. My setup uses a WAN (10.99.1.0/24 subnet on Unifi), LAN (10.99.2.0/24 subnet on Unifi), VLAN500 (VLAN tag on Unifi), and a PIA interface. The configuration of each will be detailed below.\nConfigure the Gateway\nIn pfSense, go to System \u0026gt; Routing \u0026gt; Gateways and click \u0026quot;+ Add\u0026quot; to create a new Gateway. Enter the Gateway IP address of your Unifi Gateway, which in my case is 10.99.1.1, corresponding to the UDMP\u0026rsquo;s address on the 10.99.1.0/24 subnet (named pfSense WAN on Unifi). Save the configuration.\nInterface Assignments\nNavigate to Interfaces \u0026gt; Assignments. If a WAN interface is already assigned, proceed to edit it. Ensure the settings resemble the image below, replacing 10.99.1.15 with your preferred static WAN IP address (any address within the 10.99.1.0/24 subnet except the Gateway address, 10.99.1.1). Verify that the IPv4 Upstream gateway is set to the newly created Gateway and that the \u0026ldquo;Block bogon networks\u0026rdquo; option is checked at the bottom.\nSimilarly, check your existing LAN interface or create a new one with the settings shown below. This LAN interface corresponds to the 10.99.2.0/24 subnet, which is where devices tagged with VLAN 500 on Unifi will obtain their DHCP leases. Ensure the IPv4 Upstream gateway is set to \u0026ldquo;None\u0026rdquo; and DO NOT enable \u0026ldquo;Block bogon networks\u0026rdquo; for the LAN interface.\nAdd a PIA Interface\nFrom the \u0026ldquo;Available Network Ports\u0026rdquo; dropdown, select your PIA VPN and click \u0026ldquo;Add.\u0026rdquo;\nOnce added, click on the newly added interface (e.g., OPT2, OPT3) and enable it. You can rename it for clarity (e.g., PIA_Netherlands) and ensure that the reserved network checkboxes are unchecked.\nNext, go to Interfaces \u0026gt; VLANs and click \u0026ldquo;Add.\u0026rdquo; Choose the previously configured LAN interface as the parent interface and select a VLAN tag between 1 and 4094. This tag (500 in my case) will be used in the Unifi configuration.\nFinally, connect the physical interfaces for your WAN and LAN to the pfSense box. Identifying the correct ports is crucial, especially if you have more than two NICs, as you will need to tag the LAN interface with VLAN 500 in Unifi (this can be done now or later in the Unifi setup).\nOutbound NAT Rules Go to Firewall \u0026gt; NAT and select the \u0026ldquo;Outbound\u0026rdquo; tab.\nChange the outbound NAT mode to Hybrid and click Save.\nCreate rules to direct traffic to the VPN, which will be the subnet created in Unifi (e.g., 10.99.2.0/24 in my case). Create the following rules (refer to the image below): Localhost to PIA rule, ISAKMP Localhost to PIA rule, LAN (Subnet) to PIA rule, ISAKMP LAN (Subnet) to PIA rule.\nOutbound NAT Rules Configuration\nThe settings for each rule are detailed below. Replace the placeholder values with your specific configuration.\nYour \u0026ldquo;localhost\u0026rdquo; and \u0026ldquo;127.0.0.0\u0026rdquo; information will likely remain the same. Replace \u0026ldquo;PIA_CHICAGO\u0026rdquo; with your configured PIA Interface and adjust the subnet (e.g., 10.99.2.0/24) accordingly.\nISAKMP - localhost to WAN\nlocalhost to WAN\nISAKMP - LAN (Subnet) to WAN\nLAN to WAN\nThis completes the pfSense setup. Let\u0026rsquo;s move on to the Unifi configuration.\nUnifi Setup and Configuration In the Unifi Settings page, create two Corporate LAN networks: one for the pfSense WAN and the other for the pfSense LAN. The subnets should match those configured in pfSense (as shown in the image below).\npfSense WAN on Unifi\nCreate a new Corporate LAN for your pfSense WAN in Unifi, matching the options as shown in the image below. Ensure the subnet corresponds to the WAN interface subnet configured in pfSense.\npfSense LAN on Unifi\nCreate another Corporate LAN in Unifi for the pfSense LAN, using the settings shown in the image below as a guide. Configure the DHCP Gateway IP address to match the static IP address assigned to the pfSense LAN Interface (e.g., 10.99.2.15). Importantly, set the VLAN ID to the same value used in pfSense (e.g., VLAN 500).\nConnect the pfSense physical port corresponding to the LAN interface to the Unifi network and manually configure the port to use the configured VLAN.\nTagging Ports on Unifi Switches\nIn the Unifi clients page, select the client you wish to add to pfSense and go to its overview. Choose the Port where the client is connected (e.g., Family Room - UniFi Switch 8 POE-60W #7, as shown below).\nThis will open the port configuration page, allowing you to override the settings. Select the new pfSense (500) Port Profile. If the Port Profile is not listed, refer to the troubleshooting section at the end of this guide. Once applied, the device will obtain a new DHCP lease from Unifi in the pfSense subnet, routing its traffic through PIA.\nAdding Wireless Clients to pfSense on Unifi APs\nGo to Settings \u0026gt; Wireless Networks and click \u0026ldquo;CREATE NEW WIRELESS NETWORK.\u0026rdquo; Configure the new wireless network according to the image below, ensuring that you select pfSense as the Network for the WLAN. Devices connecting to this wireless network will have their traffic routed through PIA. You can verify this by visiting \u0026ldquo;whatismyip.com\u0026rdquo; from a device connected to this WLAN.\nCongratulations on successfully configuring your network!\nTROUBLESHOOTING Adding Missing Port Profiles\nIf the Port Profile is not automatically created, navigate to Settings \u0026gt; Profiles in the Unifi dashboard and switch from RADIUS to SWITCH PORTS. Click \u0026ldquo;ADD NEW PORT PROFILE,\u0026rdquo; name it, and then select \u0026ldquo;Native Network.\u0026rdquo; Assign the pfSense network to this profile, as shown below, and save the configuration. You can now return to the port assignment section and assign ports to the pfSense VPN using this new profile.\n","date":"2023-06-14T13:18:05Z","image":"https://raw.githubusercontent.com/Nexus-Security/image-1/main/how-to-use-pfsense-and-unifi-to-anonymize-and-encrypt-vlan-tagged-traffic.png","permalink":"https://Nexus-Security.github.io/using-pfsense-and-unifi-to-anonymize-and-encrypt-vlan-tagged-traffic-a-step-by-step-guide/","title":"Using pfSense and Unifi to anonymize and encrypt VLAN tagged traffic: A step-by-step guide"},{"content":"Staying up-to-date with news and blogs can be a chore if you\u0026rsquo;re constantly checking websites or scrolling through social media. A more efficient approach is to subscribe to their RSS feeds. This begs the question: what\u0026rsquo;s the optimal method for iOS and MacOS? While there isn\u0026rsquo;t a single \u0026ldquo;best\u0026rdquo; way due to the variety of RSS clients and services, I\u0026rsquo;ll outline a straightforward and free solution I use.\nNetNewsWire and iCloud Synchronization My client of choice is NetNewsWire. It\u0026rsquo;s a free and open-source solution available on both iOS and MacOS.\nThis client supports various synchronization services, including iCloud, FeedBin, and Inoreader. Alternatively, it functions seamlessly on a single device without relying on these services.\nInitially, I experimented with Inoreader, which was satisfactory but came with drawbacks. As a paid service, it injected ads into its free plan, and the cheaper paid plan only removed ads without offering substantial additional value. Paying a subscription solely for syncing feeds across devices felt unreasonable. Therefore, I opted for iCloud, which perfectly suits my needs without requiring an additional subscription (my existing paid plan suffices, but the free plan is also adequate).\nFinal Thoughts While I\u0026rsquo;m confident that superior solutions (particularly RSS clients) exist, and I welcome insights into them, this straightforward client paired with iCloud currently fulfills my news and blog reading requirements.\n","date":"2023-06-02T20:24:33.158+01:00","image":"https://raw.githubusercontent.com/Nexus-Security/image-1/main/2024227/rss-sync-cover.webp","permalink":"https://Nexus-Security.github.io/an-easy-and-no-cost-way-to-access-and-read-content-from-rss-feeds-on-ios-and-macos/","title":"An easy and no-cost way to access and read content from RSS feeds on iOS and MacOS"},{"content":"W3RDW Blog - First Post This is the first post from my HAM radio hobby site, https://w3rdw.radio. From now on, I will be posting HAM radio content only on the W3RDW site. You can find a link on my WhiteMatterTech site.\nCreating an ADS-B Flight Tracker Using Affordable Equipment and Docker on Ubuntu This guide will demonstrate how to set up an ADS-B flight tracker using an inexpensive antenna, Docker, and Ubuntu. ADS-B, which stands for Automatic Dependent Surveillance – Broadcast, is a technology that enables aircraft to determine their location using satellite navigation and transmit this information periodically. By establishing your own ADS-B receiver, you can monitor flights in your vicinity and contribute to the global flight tracking network.\nMy setup consists of an ESXi host with an Ubuntu 21.04 VM. The recommended USB dongle is passed through to this VM. While a deep understanding of Docker isn\u0026rsquo;t mandatory, it can be helpful for addressing potential USB issues and making custom configurations. If you need help installing Docker, refer to this Docker installation tutorial and this Docker Compose installation tutorial.\nPrerequisites To get started, you\u0026rsquo;ll need an RTL SDR USB dongle, an ADS-B antenna, and an Ubuntu machine (either physical or virtual). The SMA extension cable is optional but can be useful for extending the antenna\u0026rsquo;s reach. Here are some recommended components:\nPlease note that I may earn a commission for purchases made through the affiliate links below, which helps support this blog. The price remains the same for you, regardless of whether you use my links or not. Thank you for your support!\nRTL SDR (Kit) RTL SDR USB Dongle (Standalone) Optional SMA Extension Cable Recommended ADS-B Antenna Step 1: Hardware Setup Setting up the hardware is simple.\nConnect the ADS-B antenna to the RTL SDR USB dongle, using an extension cable if needed. Connect the RTL SDR USB dongle to your Ubuntu machine. Verify that your system recognizes the USB dongle. Step 2: Configuring Environment Variables Create a .env file in your project directory and configure the following environment variables with your specific information:\n1 touch .env 1 2 3 4 5 6 7 8 9 FEEDER_TZ=America/New_York ADSB_SDR_SERIAL=0 ADSB_SDR_PPM=001 FEEDER_LAT=your_latitude FEEDER_LONG=your_longitude FEEDER_ALT_M=altitude_in_meters FEEDER_ALT_FT=altitude_in_feet FEEDER_NAME=whatever_you_want FR24_SHARING_KEY=sharingkey_from_flightradar24.com Step 3: Defining the Docker Compose Configuration We\u0026rsquo;ll create a docker-compose.yml file to define the services for our ADS-B flight tracker. This file will include readsb for data collection, fr24 for Flightradar24 integration, tar1090 for web-based visualization, and watchtower for automated updates.\nreadsb: Collects data from the ADS-B receiver and makes it available for processing, visualization, and integration. fr24: Integrates your ADS-B data with Flightradar24, a widely used flight tracking service. Sharing your data with Flightradar24 allows you to contribute to their network and access additional features. tar1090: A web-based tool for visualizing ADS-B data, providing a user-friendly interface to track aircraft, view flight paths, and explore flight information. watchtower: An automated update tool for Docker containers that ensures your ADS-B flight tracker components are always current by automatically downloading the latest Docker images and restarting the services. Create the docker-compose.yml file in your project directory:\n1 touch docker-compose.yml Paste the following configuration into the docker-compose.yml file:\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 version: \u0026#39;3.8\u0026#39; volumes: readsbpb_rrd: readsbpb_autogain: services: readsb: image: ghcr.io/sdr-enthusiasts/docker-readsb-protobuf:latest tty: true container_name: readsb hostname: readsb restart: always devices: - /dev/bus/usb:/dev/bus/usb ports: - 8080:8080 environment: - TZ=${FEEDER_TZ} - READSB_DEVICE_TYPE=rtlsdr - READSB_RTLSDR_DEVICE=0 - READSB_GAIN=autogain - READSB_LAT=${FEEDER_LAT} - READSB_LON=${FEEDER_LONG} - READSB_RX_LOCATION_ACCURACY=2 - READSB_STATS_RANGE=true - READSB_NET_ENABLE=true volumes: - readsbpb_rrd:/run/collectd - readsbpb_autogain:/run/autogain tmpfs: - /run/readsb - /var/log fr24: image: ghcr.io/sdr-enthusiasts/docker-flightradar24:latest tty: true container_name: fr24 restart: always depends_on: - readsb ports: - 8754:8754 environment: - BEASTHOST=readsb - TZ=${FEEDER_TZ} - FR24KEY=${FR24_SHARING_KEY} tmpfs: - /var/log tar1090: image: mikenye/tar1090:latest tty: true container_name: tar1090 restart: always depends_on: - readsb environment: - TZ=${FEEDER_TZ} - BEASTHOST=readsb - MLATHOST=mlathub - LAT=${FEEDER_LAT} - LONG=${FEEDER_LONG} - GRAPHS1090_DARKMODE=true tmpfs: - /run:exec,size=64M - /var/log:size=32M ports: - 8078:80 watchtower: image: containrrr/watchtower:latest tty: true container_name: watchtower restart: always environment: - TZ=${FEEDER_TZ} - WATCHTOWER_CLEANUP=true - WATCHTOWER_POLL_INTERVAL=86400 - WATCHTOWER_ROLLING_RESTART=true volumes: - /var/run/docker.sock:/var/run/docker.sock You should now have a .env file and a docker-compose.yml file in your directory, ready to launch the services.\nStep 4: Launching the ADS-B Flight Tracker Open your terminal, navigate to the directory containing the docker-compose.yml file, and run the following command to start the ADS-B flight tracker:\n1 docker-compose up -d This command downloads the required Docker images and starts the services defined in your docker-compose.yml file. Your ADS-B flight tracker will then begin receiving and processing data from aircraft in your area.\nStep 5: Accessing the Web Interface Once the services are operational, you can access the web interfaces:\nADSB Data: http://localhost:8080 (using readsb) Flightradar24 Integration: http://localhost:8754 These interfaces display real-time data about nearby aircraft, including flight paths and other pertinent information.\nTo make these resources accessible on the web, consider setting up a CloudFlare tunnel for secure tunneling. You can find a guide on how to do this here: https://nexus-security.github.io/using-cloudflared-for-tunneling-to-internal-resources/.These tutorial can help you securely access your ADS-B data both internally and externally, just like I do: https://adsb.w3rdw.radio.\nConclusion You\u0026rsquo;ve successfully set up an ADS-B flight tracker on Ubuntu using affordable equipment and Docker. This setup allows you to track flights and contribute to the global ADS-B network. The web interfaces provide real-time flight data, paths, and other valuable information about air traffic in your region. Feel free to explore additional features and customize your setup to enhance your ADS-B flight tracking experience. Happy tracking!\n","date":"2023-04-27T17:59:43Z","image":"https://raw.githubusercontent.com/Nexus-Security/image-1/main/adsb.png","permalink":"https://Nexus-Security.github.io/automatic-dependent-surveillance-broadcast-adsb/","title":"Automatic Dependent Surveillance-Broadcast (ADSB)"},{"content":"I\u0026rsquo;ve been considering self-hosting a service and faced a dilemma: Raspberry Pi 5 or a VPS?\nService Requirements First things first: what does this service need to function? Research suggests 4GB RAM and 20-30GB disk space should suffice.\nCost Analysis A Raspberry Pi 5 would set me back roughly 100€ (accessories included) plus electricity costs. A comparable VPS from Hetzner VPS prices is about 4.50€/month.\nTo offset the Raspberry Pi\u0026rsquo;s upfront cost, I\u0026rsquo;d need to run the service for a minimum of two years. Beyond that, the Raspberry Pi becomes more cost-effective (monthly cost considered) than ongoing VPS payments to Hetzner.\nComplexity Comparison Frankly, both options are fairly involved (if done properly) and similar administratively. The key difference is the Raspberry Pi needing something like Tailscale Funnel for public internet exposure since my ISP (Fastweb) operates behind a NAT.\nAdditionally, there\u0026rsquo;s the initial operating system installation and network setup on the Raspberry Pi, though that\u0026rsquo;s arguably the easiest part.\nEverything else—web server and database setup, service installation and configuration—would be identical.\nFinding the Right Fit Both approaches are valid with their respective advantages and disadvantages. The key is to assess them based on individual needs.\nRaspberry Pi (Ownership) Let\u0026rsquo;s examine buying a Raspberry Pi.\nAdvantages Hardware ownership The enjoyment of owning a device Essentially free service operation after a couple of years (electricity aside) Data privacy Disadvantages Home connection less reliable than VPS infrastructure Device failure after two years necessitates a new purchase Slightly more complex initial setup An expensive, unused device if the service is discontinued within a couple of months Hardware degradation requiring periodic replacement VPS (Rental) Now, let\u0026rsquo;s consider renting a Virtual Private Server.\nAdvantages Service uptime independent of local ISP outages Lower initial investment Minimal financial loss (less than 10€) if discontinued after a few months Simpler initial setup Disadvantages Potential data privacy concerns Ongoing costs beyond the two-year Raspberry Pi cost equivalent No One-Size-Fits-All Clearly, there\u0026rsquo;s no definitive answer. The choice depends on individual priorities and circumstances.\nIn my case, I stumbled upon a third option combining benefits from both, simplifying setup (operational in under an hour), and most importantly, costing 0€!\nMore on that in another post 😉.\n","date":"2023-04-20T15:32:42.87+01:00","image":"https://raw.githubusercontent.com/Nexus-Security/image-1/main/2024227/cover-raspberrypi-vs-vps.png","permalink":"https://Nexus-Security.github.io/should-you-purchase-or-lease-raspberrypi-compared-to-vps/","title":"Should you purchase or lease? RaspberryPi compared to VPS"},{"content":"This guide explains how to set up a Tails OS virtual machine (VM) within Unraid, enabling the persistent storage feature.\nImportant Considerations Before Starting While running Tails in a VM isn\u0026rsquo;t generally advised due to potential security compromises, certain situations might justify it. However, it\u0026rsquo;s crucial to understand the security implications involved. Virtualization necessitates trusting the hypervisor, which possesses elevated privileges that could impact the security and privacy of your VM.\nFor a comprehensive understanding of virtualization-related security concerns, refer to Tails\u0026rsquo; official documentation: https://tails.boum.org/doc/advanced_topics/virtualization/\nSituations where this setup might be advantageous include tasks requiring heightened anonymity. Examples include managing cryptocurrency wallets or minimizing your digital footprint across the internet.\nCreating a Bootable Tails USB Drive The first step involves creating a bootable Tails USB drive. This drive needs to be connected to your Unraid server whenever you intend to use the Tails VM. If persistence isn\u0026rsquo;t a requirement, you can bypass this step.\nDetailed instructions on creating a bootable Tails USB drive can be found on the official Tails website: https://tails.boum.org/install/index.en.html\nThis guide utilizes a Sony USB drive, specifically chosen from the list of recommended drives by Unraid developers (https://wiki.unraid.net/USB_Flash_Drive_Preparation). Additionally, this guide uses Tails version 4.26.\nConnecting the USB Drive to Unraid After connecting the USB drive to your Unraid server, access your Unraid server through SSH or the Unraid GUI\u0026rsquo;s terminal. Execute the command ls -lha /dev/disk/by-id/ to list connected drives.\nLocate your USB drive\u0026rsquo;s full path from the output, ensuring you select the entry without \u0026ldquo;-PART\u0026rdquo; or similar suffixes. It should resemble: /dev/disk/by-id/usb-_USB_DISK_3.0_xxxxxxxxxxxxxxxxxx-0:0.\nIn cases where multiple USB drives are connected (including the one Unraid boots from), use contextual clues like brand names or unplug the Tails USB temporarily to pinpoint the correct drive.\nCreating the Tails VM Navigate to Unraid\u0026rsquo;s VM manager and select \u0026ldquo;ADD VM.\u0026rdquo; Choose \u0026ldquo;Linux\u0026rdquo; as the operating system. Configure the core distribution and RAM allocation according to your needs. Set the \u0026ldquo;Machine\u0026rdquo; to \u0026ldquo;i440fx-6.1\u0026rdquo;, \u0026ldquo;Bios\u0026rdquo; to \u0026ldquo;SeaBIOS\u0026rdquo;, and \u0026ldquo;USB Controller\u0026rdquo; to \u0026ldquo;2.0 (EHCI).\u0026rdquo;\nFor the \u0026ldquo;Primary vDisk Location,\u0026rdquo; choose \u0026ldquo;Manual\u0026rdquo; and input the USB path identified earlier (e.g., /dev/disk/by-id/usb-_USB_DISK_3.0_xxxxxxxxxxxxxxxxxx-0:0). The \u0026ldquo;2nd vDisk Location\u0026rdquo; represents your persistent storage disk; either let Unraid manage it (\u0026ldquo;Auto\u0026rdquo;) or define it yourself. Set the \u0026ldquo;2nd vDisk BUS\u0026rdquo; to \u0026ldquo;VirtIO.\u0026rdquo; For networking, select a suitable Network Bridge. Routing this bridge through a VPN for enhanced anonymity is recommended. Save the VM configuration, but do not start it yet.\nLaunching the VM From Unraid\u0026rsquo;s VM Manager, start your newly created VM. As soon as it becomes available, swiftly select \u0026ldquo;VNC Remote.\u0026rdquo; Once the VM starts loading, press the \u0026ldquo;Tab\u0026rdquo; key to access boot options.\nIn the boot options menu, delete the \u0026ldquo;live-media=removable\u0026rdquo; and \u0026ldquo;nopersistence\u0026rdquo; options using the arrow keys, Backspace, or Delete. This step needs to be repeated every time the VM restarts. While permanent solutions likely exist, they are beyond this guide\u0026rsquo;s scope.\nAfter removing the boot options, press \u0026ldquo;Enter\u0026rdquo; twice to boot into Tails.\nSetting Up Persistence On the Tails welcome screen, choose \u0026ldquo;Start Tails,\u0026rdquo; followed by \u0026ldquo;Applications\u0026rdquo; \u0026gt; \u0026ldquo;Configure persistent volume.\u0026rdquo; The 2nd vDisk you defined earlier should be automatically selected. Proceed with the on-screen instructions to format the volume, create a strong encryption password, and save your settings.\nRestart Tails, ensuring you repeat the steps to remove the specific boot options as outlined earlier. Upon successful boot, the welcome screen will now feature an \u0026ldquo;Encrypted Persistent Storage\u0026rdquo; section. Enter your encryption password and click \u0026ldquo;Unlock\u0026rdquo; to access your persistent Tails environment within Unraid.\nConclusion With these steps, you\u0026rsquo;ve successfully configured a Tails VM on your Unraid server with encrypted persistent storage. Remember that while this setup offers enhanced privacy and anonymity, it\u0026rsquo;s crucial to be aware of the security considerations inherent in running Tails within a virtualized environment.\n","date":"2023-04-10T00:30:31Z","image":"https://raw.githubusercontent.com/Nexus-Security/image-1/main/tailsunraid.png","permalink":"https://Nexus-Security.github.io/running-tails-os-with-encrypted-persistence-on-unraid-as-a-virtual-machine/","title":"Running Tails OS with Encrypted Persistence on Unraid as a virtual machine"},{"content":"Leveraging NTLM Hash Lookup for Enhanced Security In the ever-evolving cybersecurity landscape, staying ahead of threats is paramount. NTLM hash lookup services provide a powerful mechanism to achieve this. This post will explore how to leverage a bulk check API for efficient analysis of multiple NTLM hashes, bolstering security posture and penetration testing endeavors.\nUnderstanding NTLM Hashes NTLM, short for NT LAN Manager, is a suite of Microsoft security protocols ensuring authentication and data protection. An NTLM hash represents a transformed version of a user\u0026rsquo;s password. Security professionals analyze these hashes to uncover potential weaknesses and compromised credentials.\nObtaining NTLM Hashes While outside the scope of this post, pentesters employ various methods to retrieve NTLM hashes. These include techniques like local access and memory dumps, pass-the-hash attacks, phishing campaigns, credential harvesting, man-in-the-middle attacks, Active Directory enumeration, and exploiting system vulnerabilities.\nIntroducing the NTLM Hash Lookup Bulk Check API This post focuses on a novel NTLM hash lookup bulk check API, a service capable of analyzing numerous NTLM hashes concurrently. This particular service was highlighted in a recent post on X, linked here. This API streamlines the conversion of hashes to plaintext, enhancing security and pentesting effectiveness.\nNotable Features: Bulk Processing: Analyze up to 100 hashes per request. Efficiency: Swiftly identify uncached or compromised hashes. Integration: Seamlessly integrate with diverse programming languages and tools. Getting Started with the API 1. Preparing Your Hash List Start by creating a text file (your-text-file-with-hashes.txt) containing the list of NTLM hashes, each on a separate line.\n2. Using PowerShell for API Interaction Execute the following PowerShell command, replacing [your-text-file-with-hashes.txt] with your file\u0026rsquo;s path, to send your hash list for analysis:\n1 Invoke-WebRequest -Method Post -Infile [your-text-file-with-hashes.txt] https://ntlm.pw/api/bulklookup | Select-Object -Expand Content 3. Utilizing Curl for API Interaction Alternatively, use the following curl command:\n1 curl -X POST -H \u0026#34;Content-Type: text/plain\u0026#34; --data-binary \u0026#34;@[your-text-file-with-hashes.txt]\u0026#34; https://ntlm.pw/api/bulklookup Interpreting API Responses The API will process submitted hashes and return a corresponding response. Analyze the HTTP status codes and content to understand the outcome.\nSuccessful Lookup: The response will include a list of identified hashes with their respective details. Insufficient Points: A 429 error suggests a lack of points to process the request, assuming all hashes were uncached. Waiting for approximately 15 minutes should replenish the points for subsequent requests. Conclusion Integrating an NTLM hash lookup bulk check API into your cybersecurity arsenal can significantly bolster threat detection and penetration testing efforts. By converting multiple hashes to plaintext efficiently, security professionals can proactively uncover vulnerabilities, prevent unauthorized access, and protect critical assets.\n","date":"2023-03-14T22:29:06Z","image":"https://raw.githubusercontent.com/Nexus-Security/image-1/main/cli.png","permalink":"https://Nexus-Security.github.io/introducing-a-new-api-for-bulk-checking-ntlm-hash-lookups/","title":"Introducing a new API for bulk checking NTLM hash lookups"},{"content":"A Little Background and Some Thoughts Modern technology exposes us to a constant stream of High Energy Visible (HEV) blue light from our devices, coupled with irregular lighting patterns in our homes and workplaces. Our internal clocks, known as circadian rhythms, are essential to human health and well-being. For a deeper understanding of circadian rhythms and their significance, the NIH provides a comprehensive fact sheet. This post aims to demonstrate how we can leverage home automation and open-source tools to align our living spaces with these natural rhythms.\nBy programming our homes and lighting to mimic natural light patterns, we can utilize technology to create a healthier environment. This is particularly crucial given the disruptive effects of HEV blue light and inconsistent lighting. Implementing this type of home automation can significantly benefit both mental and physical well-being, particularly for families.\nExpectant mothers, for instance, can experience negative consequences for their offspring due to circadian rhythm disruptions. Studies indicate that simple lifestyle changes, such as adjusting sleep schedules, can positively influence both mother and child during the postpartum period. Extensive research highlights the positive link between aligning with natural inclinations and overall health. Given our evolution in a world governed by the sun\u0026rsquo;s cycle, syncing with natural rhythms often proves beneficial. Circadian lighting helps achieve this balance, even for those with unconventional schedules. Let\u0026rsquo;s explore how to set up this automation for you and your loved ones.\nGetting Started This automation relies on Home Assistant, a platform designed to manage various smart home devices from a central interface. You\u0026rsquo;ll need compatible smart lights ( Philips Hue with a Hue Bridge is recommended), a device to host Home Assistant (such as a Raspberry Pi, Docker container, NAS, or PC), and some patience for the initial setup.\nDisclosure: As an Amazon Associate, I earn from qualifying purchases. Rest assured, pricing remains consistent whether you use my links or not. Thank you for your support!\nInstalling Home Assistant\nThe web offers numerous tutorials for installing and setting up Home Assistant. The official Home Assistant Documentation is a great place to start.\nConfiguring Home Assistant and Installing Circadian Lighting\nBegin by downloading the Circadian Lighting plugin. Choose Option 1 for Philips Hue with a Hue bridge or Option 2 for other smart lights. Click the green \u0026ldquo;Code\u0026rdquo; button on the chosen link and select \u0026ldquo;Download Zip.\u0026rdquo; Extract the Zip file\u0026rsquo;s contents using your preferred method.\nWithin your Home Assistant configuration root folder, create a folder named \u0026ldquo;custom_components\u0026rdquo; (without quotes). This should be in the same directory as the configuration.yaml file. Inside custom_components, create a folder called \u0026ldquo;circadian_lighting\u0026rdquo; (without quotes). Move the extracted Zip file contents (___init___.py, manifest.json, sensor.py, services.yaml, and switch.py) into this folder. The final folder structure should resemble: config/custom_components/circadian_lighting/.\nFolder Structure Setup\nAdd the following lines to your configuration.yaml file:\n1 2 # Example configuration.yaml entry circadian_lighting: Create a switch within Home Assistant to control Circadian Lighting and allow for manual adjustments. For example:\n1 2 3 4 5 6 # Example configuration.yaml entry switch: - platform: circadian_lighting lights_ct: - light.desk - light.lamp You can create multiple switches to control Circadian Lighting in different rooms independently. Add lights to each switch using their entity IDs from Home Assistant. A personal example showcasing multiple switches is available on this Home Assistant GitHub repo.\nHue Specific Setup (Option 1)\nOption 1 enables you to configure room-specific scenes for your Hue switches, allowing them to activate lights with appropriate Circadian Lighting settings. For each room where you\u0026rsquo;d like to use a Hue switch/dimmer to activate a Circadian Light scene, create a scene named \u0026ldquo;Circadian\u0026rdquo; (with a capital \u0026ldquo;C\u0026rdquo;) within that room. The iConnectHue app on iPhone is a reliable option for creating these scenes. Other apps may not function correctly.\nWithin the chosen room, like the kitchen, add a scene including the bulbs you want to control with your switch. Color and brightness values don\u0026rsquo;t matter as the plugin will overwrite them. After creating the Circadian scene, assign it to your switch (refer to the screenshots below for a visual guide using iConnectHue).\nKitchen Lights Example\nCreated Scene Example\nOnce the scene is set up (only for Option 1), your setup is complete! Option 2 users will also be finished at this point. Restart your Home Assistant instance. Your Circadian Lights will now adjust according to natural light patterns. The \u0026ldquo;Issues\u0026rdquo; section on GitHub provides advanced setup ideas.\nThank you for reading!\nNote: When launching the plugin for the first time, you might need to manually turn on the Switch(es) through your Home Assistant Dashboard. Add entities to your home page, as shown in the image below, and toggle the switch to activate Circadian Lighting for those lights.\nCL Switches on Home Assistant Dashboard\n","date":"2023-03-02T08:08:04Z","image":"https://raw.githubusercontent.com/Nexus-Security/image-1/main/live-better-with-circadian-lighting.jpg","permalink":"https://Nexus-Security.github.io/improve-your-life-with-circadian-lighting/","title":"Improve Your Life with Circadian Lighting"},{"content":"Overview This post aims to provide tips for addressing network security concerns when hosting a publicly accessible web server from your home network, specifically using Unifi networking equipment. While screenshots will feature the Unifi Dream Machine Pro (UDMP), the concepts explained can be adapted to other networking gear.\nDisclosure: As an Amazon Associate, I earn from qualifying purchases. This does not affect pricing for you.\nStrong Passwords/Encryption Keys Using strong passwords and encryption keys for all services, operating systems, logins, databases, etc., is crucial. A password manager (like 1password.com) can simplify generating and securely storing these complex credentials. Strong passwords are essential as they can mitigate security risks even if a hacker breaches your network. For example, if your WordPress database has a strong encryption key, it will take an impractical amount of time for a hacker to brute-force it, rendering access to the network practically useless. This principle applies to operating system root user passwords, application login credentials, database encryption keys, and any resource secured with authentication. The goal is to prevent unauthorized network access, but if that happens, having strong, unique passwords and encryption keys acts as a significant line of defense.\nCreating an Isolated \u0026ldquo;DMZ\u0026rdquo;-like Network DMZ, which stands for \u0026ldquo;demilitarized zone,\u0026rdquo; is an older networking concept where less secure servers and resources are grouped, hoping to prevent hackers from pivoting to the internal network. Although not as common today, the term is still relevant, particularly in certifications like CompTIA Security+. This post utilizes a similar concept by creating isolated subnets with firewall rules on the router to prevent any traffic from the \u0026ldquo;DMZ\u0026rdquo; network to internal networks.\nNote: The screenshots are from the older Unifi settings page, accessible by disabling the new interface in Settings \u0026gt; System Settings.\nStart by creating a new Corporate LAN in Unifi (Settings \u0026gt; Networks) and name it (e.g., \u0026ldquo;DMZ\u0026rdquo;). Assign a VLAN to this subnet (e.g., VLAN 777). This process might differ slightly for other networking gear, but the objective is to establish an isolated subnet.\nNext, assign devices to this new network. In Unifi, this is done by configuring the port profile for the specific port connected to your web host. Other networking devices should have similar settings.\nConfiguring Firewall Rules\nTo set firewall rules, navigate to Settings \u0026gt; Routing \u0026amp; Firewall \u0026gt; Firewall and select LAN IN. Create a new rule with a descriptive name (e.g., Block DMZ -\u0026gt; IoT) and ensure it is Enabled and applied \u0026ldquo;Before predefined rules\u0026rdquo;. Set the Action to \u0026ldquo;Drop\u0026rdquo; and choose \u0026ldquo;All\u0026rdquo; for the IPv4 Protocol. Under Advanced, select all states (New, Established, Invalid, Related) and leave the IPsec option unchanged. For the Source, choose \u0026ldquo;Network\u0026rdquo; as the \u0026ldquo;Source Type\u0026rdquo; and select your DMZ network. Similarly, for Destination, choose \u0026ldquo;Network\u0026rdquo; and select your internal network. This will block all IPv4 traffic from your DMZ network to your internal network, regardless of its state. Keep in mind this also blocks access to your WebHost from your internal networks, including SSH and RDP. If you need to allow specific protocols, see the next section.\nRepeat this rule creation for each internal network defined as a Corporate LAN in Unifi if you have multiple internal networks.\nAllowing Specific Traffic From DMZ to Internal Network Proceed with caution as any adjustments here can introduce security risks. Explore all other options before making these changes to prioritize the protection of your internal resources.\nAdding Firewall Rules\nIf specific protocols need to be open between your WebHost and internal network, modify the firewall rules. The changes include: Action = Accept, Invalid = Unchecked, Source = Address/Port Group.\nFor the Source, it\u0026rsquo;s recommended to specify the WebHost requiring access. Create an IPv4 Address Group with a descriptive name (e.g., WebHost) and add the static IP of the WebHost under \u0026ldquo;Address\u0026rdquo;. Your WebHost should have a static IP; configure it before proceeding if not already done.\nNext, create a Port Group under Source and add the ports needing access from your internal network (e.g., SSH port 22 and RDP port 3389). Under Destination, you can either specify the networks/devices that require access or allow access from any internal network by leaving both boxes set to \u0026ldquo;Any\u0026rdquo;.\nEnable IPS/IDS The Unifi Dream Machine Pro offers excellent security features, including IPS/IDS found under Settings \u0026gt; Threat Management (old GUI) or Settings \u0026gt; Security \u0026gt; Internet Threat Management (new GUI). IPS (Intrusion Prevention System) actively monitors traffic and blocks potentially malicious activity based on defined rules, while IDS (Intrusion Detection System) passively monitors traffic and logs/alerts suspicious activity.\nConsider IPS as an essential security measure for protecting your home network while web hosting. While other hardware or software options might provide this service, Unifi\u0026rsquo;s solution is explored here.\nWithin the IPS/IDS settings, choosing IPS for your Protection Mode is recommended. Be aware that enabling IPS might impact your overall download speed if you\u0026rsquo;re not using a Unifi Dream Machine Pro. The device will indicate the maximum throughput with IPS enabled. The Unifi Dream Machine Pro supports up to 3.5Gbps throughput with IPS enabled.\nWrapping Up Network security, especially when hosting a web server on your home network, is paramount. Implementing the outlined security measures, including setting up an isolated network and enabling IPS on your Unifi Dream Machine Pro (or similar Unifi Gateway device), is highly recommended.\n","date":"2023-02-16T23:53:21Z","image":"https://raw.githubusercontent.com/Nexus-Security/image-1/main/network-hardening-webhosting.jpg","permalink":"https://Nexus-Security.github.io/tips-for-strengthening-your-network-security-for-hosting-websites-at-home/","title":"Tips for Strengthening Your Network Security for Hosting Websites at Home"},{"content":"Introduction I recently worked with a university research lab to create vapor sensors that could provide approximate measurements of ethanol vapor levels in an operant chamber. This project was quite engaging.\nMy lack of experience with Arduino and the limited circuit documentation online led to some challenges. Despite this project being outside my usual area of expertise, I persisted, and the end product turned out well.\nThis project\u0026rsquo;s open-source nature stems from the work of Dr. Brian McCool. He developed the Arduino code (available on GitHub), the general project design and suggested parts list, and the concept of using this project in alcohol research.\nThe Lab This project was developed for Dr. Anna Radke\u0026rsquo;s Reward and Addictive Disorders (RAD) Lab at Miami University.\nThese sensors will be used in an ongoing experiment involving mouse operant chambers. They will provide a visual representation of ethanol vapor pressure within these chambers. This experiment, utilizing these modules, is overseen by Elizabeth Sneddon, who was a PhD candidate at the time and a recipient of the prestigious DSPAN F99/K00 grant from NINDS.\nTo learn more about Dr. Radke, her team, and their projects, I encourage you to visit the RAD Lab\u0026rsquo;s website.\nSide note: I\u0026rsquo;d also like to highlight a research paper from the RAD Lab that I co-authored, which can be found on PubMed.\nFirst Successful Unit\nThe Parts Per Module Quick List: Arduino Uno R3 Arduino Proto Shield for Arduino Kit (Stackable) Clear Enclosure for Arduino (adafruit.com) Standard LCD 16x2 screen (options available on adafruit.com and Amazon) i2c/SPI character LCD backpack MQ-3 alcohol gas sensor (sparkfun.com) Power supply (see notes below) Standard USB printer cable 500Ohm Trim Potentiometer (rat operant boxes); 1k-2kOhm Trim Potentiometer (mouse operant boxes) - a kit containing both is recommended Wiring: a kit with various wire types is suggested Optional: Breadboard (helpful for testing but not mandatory) Optional: Soldering iron kit (required for soldering) and solder sucker kit (recommended for fixing mistakes) Power supply notes: While a 5V 2A power supply might be considered, I encountered issues with voltage drops. Using a standard USB printer cable with a USB power block (minimum 5V and 2A) is highly recommended. A USB printer cable is also necessary for program transfer to the Arduino.\nThis list is adapted from Dr. Brian McCool\u0026rsquo;s recommendations.\nDisclosure: As an Amazon Associate, I earn from qualifying purchases. Your support through these links helps maintain this blog, and the pricing remains the same for you. Thank you!\nThe Circuit The circuit diagram below, drawn by my father (a radio and broadcast engineer), illustrates the project\u0026rsquo;s wiring. It can be a valuable resource for anyone building this project. Drawn by RW 09/04/2021\nArduino with MQ-3B Sensor Module Wiring Diagram\nAccess the diagram here: https://drive.google.com/file/d/1gVgyCg9lm8LaurHVQtRFqXGifgDlCiEb/view?usp=sharing\nAssembling the Arduino The following steps outline the assembly process, though the order can be adjusted as needed.\nThis was my first Arduino project, so I am by no means an expert. While I found the process enjoyable overall, there were moments of frustration. Unfortunately, I didn\u0026rsquo;t document the process as thoroughly as I should have. I\u0026rsquo;ll aim to provide more comprehensive visuals if I have the opportunity to build more units in the future.\nPreparing the Protoshield and Connecting to the Mainboard Assembling the Protoshield requires soldering. The components included with the Protoshield should be arranged as shown in the image below. Pay close attention to the placement of capacitors, resistors, switches/buttons, and LEDs. Soldering should be done from the board\u0026rsquo;s underside.\nAlign and solder the black pinouts as illustrated in the image above, also from the bottom (see image below for clarity). These will be used to connect the Protoshield to the Mainboard by inserting the pins into the corresponding pinouts.\nYour assembled Protoshield and Mainboard should resemble the first image.\nConnect the LCD Backpack to the LCD Screen The backpack connects to the screen using the 16-pin connector. Solder all 16 pins to the front of the screen using the short ends of the connectors (see image below).\nTip: Attach the 5-screw pins to the backpack before soldering it to the screen. Adding them afterward is considerably difficult.\nFlip the assembly over and solder all 16 pins of the backpack as depicted in the image below.\nThe GND, 5V, CLK, and DAT ports are labeled as the READ-OUT BOARD on the diagram.\nWire the Protoshield While I don\u0026rsquo;t have many detailed photos of the wired Protoshield, the image below demonstrates how I connected the 5V and Ground from the Mainboard to their central counterparts on the Protoshield. This allows for easy access when soldering the remaining required 5V and Ground wires.\nUsing a breadboard during the wiring process is highly recommended. As shown in the image below, a breadboard allows you to test your connections before making them permanent. While some components will require soldering (e.g., screen, backpack), it\u0026rsquo;s beneficial to test your wiring on the breadboard first.\nSuccessful Test Readout with Breadboard\nCleaned Up Wiring\nProgramming Download the code provided by Dr. McCool on GitHub. For instructions on uploading the code to your Arduino, refer to this helpful post. Once the connections are made as described and the program is uploaded, your screen should display the voltage reading.\nFine-Tuning the Read-Out The backpack includes a small, circular \u0026ldquo;Contrast\u0026rdquo; potentiometer near the top-right corner. If your screen illuminates but doesn\u0026rsquo;t show a reading, adjust the contrast using this potentiometer. I found it necessary to fine-tune the contrast for each module I built.\nCaution: The potentiometer is fragile and easily damaged, as illustrated in the image below. Exercise caution when adjusting it to avoid breakage. My broken potentiometer required rewiring the module with a new external potentiometer for contrast control. While I don\u0026rsquo;t currently have a wiring diagram for this situation, I recommend replacing the backpack if you encounter this issue.\nBroken Backpack Potentiometer\nWrapping Up Congratulations! I hope this tutorial was beneficial in guiding you through the process of building your own sensor module. I trust that this information will be valuable for both your research and personal endeavors.\nFirst Successful Unit\nFirst Completed Unit\nUndershot of a Successful Build\n","date":"2023-01-17T12:45:04Z","image":"https://raw.githubusercontent.com/Nexus-Security/image-1/main/IMG_0825-min-768x1024.jpeg","permalink":"https://Nexus-Security.github.io/investigating-ethanol-levels-with-the-arduino-mq-3b-sensor-in-behavioral-neuroscience-studies/","title":"Investigating Ethanol Levels with the Arduino MQ-3B Sensor in Behavioral Neuroscience Studies"},{"content":"Some time ago, I transitioned my self-hosted WordPress blog (don\u0026rsquo;t judge!) to a static site generated with Pelican and hosted for free on GitHub Pages.\nThis setup is surprisingly effective: I write all my posts in Markdown, version control them on GitHub, and when I\u0026rsquo;m ready to publish, a simple pull request and merge triggers a CI job. This job uses Pelican to build the site and deploys it automatically. More details about this setup can be found at this other post.\nThe catch? Managing this workflow usually involves a GitHub client, a Markdown editor, and the ability to commit, push, and create pull requests. I didn\u0026rsquo;t think this was realistic on an iPad, especially offline. The official GitHub app is quite limited, and web-based IDEs like VsCode.dev require an internet connection. However, thanks to this blog post by Marco Gomiero, I discovered it is possible!\nWorking Copy: A Mobile Git Powerhouse A core part of this mobile workflow relies on a robust Git client, and Working Copy fits the bill perfectly.\nWorking Copy is arguably the best Git client on iOS. It allows for repository cloning and offline work. Files are accessible through the File explorer, making them available to any iOS app, including a variety of Markdown editors.\nMWeb: A Markdown Editor for iOS (With Caveats) Following Marco\u0026rsquo;s recommendation, I\u0026rsquo;ve started using this editor. However, I haven\u0026rsquo;t fully committed (pun intended). I\u0026rsquo;m still using the free version because of a few issues I\u0026rsquo;ve encountered. I\u0026rsquo;ve reported these to the developer and am waiting to see how they\u0026rsquo;re addressed. If MWeb doesn\u0026rsquo;t work out, I have my sights set on alternatives like Textastic, iWriter Pro, and maybe iA Writer (although its price tag sets a high bar!).\nStreamlining with iOS Shortcuts and Automations Another trick I adopted from Marco is leveraging iOS Shortcuts.\nI\u0026rsquo;ve customized the shortcuts to accommodate Pelican instead of Hugo and to align with my personal blogging conventions. Here\u0026rsquo;s what my setup looks like:\n\u0026ldquo;New Blog Post\u0026rdquo;: Creates a new branch in my repository and generates a Markdown file adhering to my naming conventions. The file is pre-populated with standard Pelican Markdown headers. \u0026ldquo;Blog Photo\u0026rdquo;: Allows me to select a photo from my library, converts it to JPEG (handling those pesky .heic files from my iPhone), and places it in the designated folder within my repository. Wrapping Up You\u0026rsquo;ve just read my first blog post written entirely on my iPad from the comfort of my sofa. If you have any recommendations for a solid iOS Markdown editor or any general questions, feel free to share in the comments below.\nmigrated\n","date":"2023-01-05T09:50:59.893+01:00","image":"https://raw.githubusercontent.com/Nexus-Security/image-1/main/2024227/editing-markdown-with-mweb.jpeg","permalink":"https://Nexus-Security.github.io/writing-a-blog-post-using-an-ipad/","title":"Writing a blog post using an iPad"},{"content":"The robots.txt file, a longstanding standard, instructs web robots like search engine crawlers on which areas of a website they can and cannot access.\nFor instance:\n1 2 User-agent: * Disallow: /private/ This snippet prevents all robots from accessing the /private/ directory.\n1 2 User-agent: Googlebot Disallow: /users/ This one specifically instructs Googlebot to avoid the /users/ directory.\nThe AI Conundrum The existing robots.txt standard falls short in the age of AI. Current capabilities only allow us to permit or restrict crawling, but we lack the ability to define more nuanced rules.\nWe need more granular controls, such as:\nIndexing: Can a web crawler index the content? Caching: Can a web crawler cache the content? LLM Training: Can the content be used for language model training? Summarising: Can a web crawler summarise the content? And more\u0026hellip; These capabilities were not always present, and website owners should have the right to decide how their content is utilized in these ways.\nThe Need for Enforcement Robust rules are meaningless without enforcement. The robots.txt file seems insufficient to prevent misuse by certain entities.\nA case in point is Perplexity AI, a company recently found out that has been accused of using a fake user agent to bypass robots.txt restrictions, effectively impersonating a human user. This alleged violation has been confirmed by Wired and by MacStories.\nIn Conclusion Clearly, rules need teeth. We need regulatory bodies to address content owners\u0026rsquo; complaints and impose penalties on companies like Perplexity AI that disregard the rules. Smaller creators often lack the resources to pursue legal action against larger entities.\nAs with any tool, it\u0026rsquo;s the application that matters. AI holds the potential for positive innovation, but not at the cost of exploiting the work and rights of others.\nDisclaimer The cover image was generated using AI – a testament to its capabilities, even if imperfect, and certainly surpassing my own artistic skills. The article\u0026rsquo;s content, however, is entirely human-generated.\n","date":"2022-11-25T10:01:08.457+01:00","image":"https://raw.githubusercontent.com/Nexus-Security/image-1/main/2024227/we-need-new-robots.png","permalink":"https://Nexus-Security.github.io/we-require-an-updated-robots.txt-file-and-rules-to-ensure-compliance-with-it/","title":"We require an updated robots.txt file and rules to ensure compliance with it"},{"content":"The organization OpenAI focuses on developing and promoting AI technology with a beneficial impact on humanity. A prime example of their work is the GPT series, which includes the well-known ChatGPT, a sophisticated language model designed to produce human-like text based on provided prompts. This blog post will illustrate how to utilize the OpenAI API for ChatGPT, complete with Python code examples.\nPrerequisites Before getting started, you\u0026rsquo;ll need an OpenAI account and an API key. Visit OpenAI API website to sign up for an account. Your API key, accessible from the API dashboard, will be required for interaction with the API.\nAdditionally, the openai Python package is necessary. Install it via pip:\n1 pip install openai Example: Generating Text with ChatGPT With your API key and the openai package ready, you can begin generating text with ChatGPT via the OpenAI API. Here\u0026rsquo;s a simple example:\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 import openai import os openai.api_key = os.environ[\u0026#34;OPENAI_API_KEY\u0026#34;] prompt = \u0026#34;What is the meaning of life?\u0026#34; response = openai.Completion.create( engine=\u0026#34;davinci\u0026#34;, prompt=prompt, max_tokens=50, ) answer = response.choices[0].text.strip() print(answer) Let\u0026rsquo;s break down this example. First, we import the necessary openai package and set our API key using openai.api_key. We define a prompt variable, which contains the text we want ChatGPT to expand upon. The openai.Completion.create() function sends this prompt to the OpenAI API and receives the generated text response. Lastly, we extract and print the generated text.\nWithin the openai.Completion.create() function, these parameters are defined:\nengine: This specifies the GPT model to be used. In this instance, \u0026ldquo;davinci,\u0026rdquo; the most capable and resource-intensive model, is employed. prompt: The text prompt for the GPT model to work with. max_tokens: This sets a limit on the number of tokens (words and punctuation) in the generated response. These parameters can be adjusted to meet specific requirements, such as selecting different GPT models, modifying the prompt, or changing the max_tokens value.\nConclusion Through the openai Python package, using the OpenAI API for ChatGPT becomes straightforward. The ability to generate human-like text opens up various possibilities, including chatbots, language translation, and content creation. The OpenAI API empowers you to elevate your AI projects to new heights.\nDisclaimer and attribution Just in case it wasn\u0026rsquo;t clear by now\u0026hellip; I must admit: all of the above has been generated by ChatGPT! This, of course, was just an experiment, and I do not plan to regularly generate my blog posts with this tool. Should I conduct further experiments, I will always include this disclaimer at the end. I hope you found this an interesting read!\n","date":"2022-11-06T01:06:53.681+01:00","image":"https://raw.githubusercontent.com/Nexus-Security/image-1/main/2024227/python-chatgpt.jpeg","permalink":"https://Nexus-Security.github.io/using-the-openai-api-for-chatgpt-in-python-a-step-by-step-guide/","title":"Using the OpenAI API for ChatGPT in Python: A Step-by-Step Guide"},{"content":"I\u0026rsquo;ve recently gotten back into reading independent blogs. I use NetNewsWire, a free and open-source RSS reader for macOS and iOS, to keep up with them. Whenever I come across an interesting article, perhaps on Mastodon, I make it a point to subscribe to the blog\u0026rsquo;s RSS feed.\nI\u0026rsquo;ve gathered quite a list of blogs I follow over the past several weeks, and I wanted to share some of them with you. I\u0026rsquo;m trying to prioritize personal blogs over mainstream news because I find the personal touch makes them more engaging and distinctive.\nA Few of My Favorite Blogs Here are a few of the blogs I follow and highly recommend. I strive to read everything from the blogs I follow, but these are the ones I find myself returning to most often:\nBrain Barking Author: dr. Wouter Groeneveld This blog delves into tech, games, and food.\nJeff Triplett\u0026rsquo;s Micro.blog Author: Jeff Triplett Jeff, a DjangoCon organizer and Python Software Foundation Fellow, shares his insights on Django, Python, and other tech-related subjects.\nJuha-Matti Santala Author: Juha-Matti Santala Juha-Matti is a prolific writer whose work spans tech, life, and the craft of writing. He often inspires me to write more.\nTerence Eden\u0026rsquo;s Blog Author Terence Eden Terence covers a range of topics, including tech, privacy, and other intriguing subjects. He frequently writes about unique gadgets and services, and I always come away learning something new from his posts. As a bonus, he\u0026rsquo;s a genuinely kind person!\nMy Complete Blogroll You can find my complete list of blogs at this address: https://gist.github.com/andreagrandi/3226460db523a4c60e72f22e9ad896e9 Alternatively, you can download the OPML file here.\nShare Your Blog! I\u0026rsquo;m always searching for new and interesting blogs to follow, especially indie ones. If you have a blog you believe is worth reading, I\u0026rsquo;d love to hear about it! You can reach me on Mastodon or leave a comment on the gist on GitHub.\n","date":"2022-11-05T18:04:20.231+01:00","image":"https://raw.githubusercontent.com/Nexus-Security/image-1/main/2024227/cover-blog-list.webp","permalink":"https://Nexus-Security.github.io/check-out-my-handpicked-selection-of-independent-blogs/","title":"Check out my handpicked selection of independent blogs"},{"content":"Using a cloud storage service like Google Drive, Dropbox, or OneDrive? It\u0026rsquo;s important to know that your files are not encrypted by default. This means your provider can access and potentially share them. If a security breach occurs, your data could be at risk.\nCryptomator is a tool that can help. It automatically encrypts your files before they\u0026rsquo;re uploaded to the cloud.\nHow does it work? Cryptomator is free, open-source software that creates an encrypted vault for your files on your computer. Utilizing strong encryption algorithms (256-bit AES), Cryptomator secures your data within this vault.\nIt\u0026rsquo;s important to note that Cryptomator itself doesn\u0026rsquo;t upload your files. Your cloud provider\u0026rsquo;s application handles that:\nYou save files to the Cryptomator vault. Cryptomator encrypts these files and places them in a folder synced with your cloud provider. Your cloud provider application uploads the encrypted files. Accessing your files is simple. Just open your mounted vault, and Cryptomator automatically decrypts them for you.\nAfter installation, a new volume appears in your file manager, allowing you to interact with your files as if they were stored locally. Your cloud provider only sees the encrypted versions:\nWhat systems are supported? Cryptomator is widely compatible, with support for Windows, MacOS, Linux, Android, and iOS. Use it on your desktop, laptop, smartphone, or tablet. The only requirement is that your cloud provider offers an app for your chosen platform to ensure your encrypted files stay in sync.\nFor instance, I use it with pCloud to secure my files, accessing them seamlessly from my Mac, iPhone, and iPad. While pCloud offers its own encryption, it comes at a price. I believe it\u0026rsquo;s more secure if they don\u0026rsquo;t hold the encryption keys, just in case a security incident occurs.\nHow much does it cost? Cryptomator is both free and open-source, so there\u0026rsquo;s no cost to use it. The only exception is the iOS app, which offers additional features like file writing for a small fee. However, you can still use it for free to read files, and there\u0026rsquo;s a demo version available.\nConclusion If you value the safety and security of your files stored in the cloud, Cryptomator is a strong option to consider. Its ease of use, open-source nature, and robust encryption provide peace of mind. Even if your cloud provider experiences a breach, your encrypted data remains protected, inaccessible without the encryption key.\n","date":"2022-09-22T04:26:37.927+01:00","image":"https://raw.githubusercontent.com/Nexus-Security/image-1/main/2024227/man-vault.png","permalink":"https://Nexus-Security.github.io/cryptomator-allows-you-to-encrypt-files-end-to-end-in-any-cloud-storage/","title":"Cryptomator allows you to encrypt files end-to-end in any cloud storage"},{"content":"You\u0026rsquo;ll notice my blog has a fresh look. I recently switched from Pelican to Hugo, and I\u0026rsquo;m thrilled with the outcome. Although familiar with Hugo, this project provided my first real opportunity to use it. Thanks to the support of a few friends, the migration only took a few days.\nWhat Prompted the Migration? Pelican has been my trusted static site generator for a long time, consistently delivering excellent results. However, lately, I\u0026rsquo;ve been grappling with PIP dependency issues. For instance, one plugin\u0026rsquo;s requirement of an outdated sub-dependency version clashed with other plugins. Troubleshooting this turned out to be a complex endeavor, leading me to explore Hugo as an alternative.\nAdding to my decision, I realized my current theme was no longer supported. The developer had transitioned to Hugo, leaving me with an obsolete theme.\nFinally, the JavaScript library I relied on for indexing and search functionality was also stagnant in terms of updates and support.\nThe Migration Process Both Pelican and Hugo are static site generators that use Markdown for content. This commonality simplified the migration process considerably. However, due to differences in Markdown syntax and front matter (metadata within Markdown files) between the two, I opted to create a script to streamline the conversion.\nThe complete script is available here: https://gist.github.com/andreagrandi/0a7bf6e217d6561b00b6a5de6211ddaa. Keep in mind that it\u0026rsquo;s tailored to my specific blog structure in both Pelican and Hugo. While it may not directly apply to your situation, it can serve as a useful starting point for your own Pelican-to-Hugo migration.\nContent migration aside, a significant chunk of time was devoted to selecting and customizing a new theme. My limited CSS knowledge meant a steep learning curve. While I\u0026rsquo;m still refining the appearance, I\u0026rsquo;m content with the current state.\nOne less thing to worry about is the search functionality, which is seamlessly integrated into the new theme.\nFinally, I rewrote the CircleCI configuration file to accommodate Hugo instead of Pelican. This involved some trial and error but was ultimately successful.\nReflections on the Migration The entire experience has been positive. My site is now easier to manage, and dependency issues are a thing of the past. I\u0026rsquo;m pleased with the new theme, even though I have minor adjustments planned (like addressing the non-functional comments section). Looking back, I wish I had made the switch sooner.\nI hope you\u0026rsquo;re enjoying the refreshed look! Please don\u0026rsquo;t hesitate to reach out if you come across any issues.\n","date":"2022-09-18T23:28:40.384+01:00","image":"https://raw.githubusercontent.com/Nexus-Security/image-1/main/2024227/migrating-from-pelican-to-hugo.webp","permalink":"https://Nexus-Security.github.io/switching-from-pelican-to-hugo/","title":"Switching from Pelican to Hugo"},{"content":" Introduction My previous post detailed how to connect to a Unifi router using HTTPS, eliminating the \u0026ldquo;Your connection is not private\u0026rdquo; warning. However, that method relies on an external DNS server like CloudFlare to resolve your Unifi router\u0026rsquo;s address. This introduces latency and requires opening external ports, increasing potential security risks.\nThis post presents a solution for maintaining secure, external accessibility while also enabling local HTTPS access without opening external ports. This approach is particularly well-suited for a robust homelab environment.\nThe Solution: \u0026ldquo;Split-Brain DNS\u0026rdquo; Split-Brain DNS utilizes two sets of DNS servers to accurately resolve traffic both internally and externally. This allows for the correct resolution of internal resources from both inside and outside the network.\nIn my setup, PiHole acts as the internal DNS server, handling internal DNS requests for internal resources. CloudFlare manages external resolution for these internal resources. For instance, if my local network is 10.10.10.0/24 with my Unifi Dream Machine Pro (UDMP) at 10.10.10.1 and my MacBook Pro (MBP) at 10.10.10.50, requests from the MBP to the UDMP are resolved directly by PiHole (see the first Terminal image). However, if my MBP is on a different network, the external DNS server would direct traffic to my network\u0026rsquo;s public IP, which would then be routed by the reverse proxy to reach the UDMP (see the second Terminal image).\nFirst Image: Executing dig unifi.white.fm on an internal machine using PiHole as the DNS server resolves unifi.white.fm directly to the reverse proxy within my network.\nSecond Image: Executing dig unifi.white.fm from an external machine resolves unifi.white.fm to my dynamic DNS host (DuckDNS), which points to my network\u0026rsquo;s public IP. Port forwarding rules would then direct the traffic to the reverse proxy and subsequently to the UDMP, if enabled.\nIn both scenarios, my browser establishes a secure HTTPS connection to my Unifi Machine using Let\u0026rsquo;s Encrypt certificates. Before delving into the specific setup, let\u0026rsquo;s review the tools I employ to achieve this.\nTools PiHole PiHole serves as the internal DNS server in this setup. While there are various ways to implement PiHole within a network, my configuration involves multiple instances across different subnets and VLANs for granular control over resolution.\nIt\u0026rsquo;s important to note that PiHole isn\u0026rsquo;t the only DNS server capable of achieving this. Its appeal lies in its straightforward yet powerful DNS blocking features. If network-wide tracking and ad-blocking aren\u0026rsquo;t requirements, alternative tools might be more suitable. This tutorial will utilize PiHole for demonstration purposes.\nUnnecessary Note: Each PiHole instance in my network is configured for anonymous resolution using Oblivious DNS Over HTTPS (ODOH). For containerized instances, I leverage the mschirrmeister/odoh-resolver container (for more details, see the GitHub repository).\nNginx Proxy Manager PiHole is configured to resolve internal resource requests to Nginx Proxy Manager, which handles traffic routing to the desired resource and provides HTTPS protection through Let\u0026rsquo;s Encrypt certificates. My previous post provides a guide on setting up Nginx Proxy Manager with Docker.\nSimilar to PiHole, alternative reverse proxies can achieve this outcome. I opt for Nginx Proxy Manager due to its user-friendly nature.\nInternal Routing With at least one DNS server and a reverse proxy operational, we can establish the internal component of our split-brain DNS. In this tutorial, my network is 10.99.100.0/24, with an Nginx Proxy Manager instance at 10.99.100.10 and a CyberChef Docker container at 10.99.100.82 with a GUI accessible on port 3000 (i.e., http://10.99.100.82:3000).\nNginx Proxy Manager receives client communication on ports 80 and 443, forwarding traffic to a specified host and port. For CyberChef, this would be 10.99.100.82 on port 3000. The first step is configuring Nginx Proxy Manager to route traffic for the host cyberchef.white.fm, which we will define in our DNS server. In Nginx Proxy Manager, navigate to Hosts and click Add Proxy Host. Input your chosen domain (e.g., cyberchef.white.fm). In most cases, you\u0026rsquo;ll want to retain the default Scheme setting of http, as the initial traffic isn\u0026rsquo;t encrypted. Add the host\u0026rsquo;s IP address (e.g., 10.99.100.82) and the communication port (e.g., port 3000). While enabling Block Common Exploits and Websockets Support is recommended, these options should be explored and tested based on your specific host and requirements. The image below illustrates this page configured as described.\nOn the same page, navigate to SSL and select Request a new SSL Certificate from the dropdown menu. This automatically requests a certificate from Let\u0026rsquo;s Encrypt based on the hostname (e.g., cyberchef.white.fm) provided earlier. If you possess an existing certificate or prefer a wildcard certificate for all internal resources, those options are also available. Enable the options based on your host\u0026rsquo;s requirements. The image below provides an example configuration for this screen.\nAfter the certificate request is finalized, click Save. A new Proxy Host will appear in your Nginx Proxy Manager.\nIn PiHole, select Local DNS from the left panel, then choose DNS Records. Under Domain, enter the intended domain target from Nginx Proxy Manager (e.g., cyberchef.white.fm) and the IP address of your Nginx Proxy Manager host (e.g., 10.99.100.10), which differs from the target host. Click Add. The image below depicts this configuration.\nWith these configurations saved, ensure your client utilizes PiHole (or your chosen DNS server) as its DNS server.\nIf you\u0026rsquo;re using Unifi with the latest User Interface, this setting is found under Settings \u0026gt; Networks \u0026gt; Name_of_Your_Network \u0026gt; DHCP \u0026gt; DHCP Service Management: Show options \u0026gt; DHCP DNS Server: Enable.\nEnable this setting and enter your PiHole IP(s). After saving, renew your client\u0026rsquo;s DHCP lease (or reconnect if static) to utilize PiHole as its DNS server. You can also manually assign the DNS server on your client for a more localized change. The Unifi settings are shown in the image below.\nWith the new DNS server(s) added, attempt to connect to your host via HTTPS (e.g., https://cyberchef.white.fm). A successful configuration results in the secure HTTPS connection indicator in your browser (see the image below).\nA dig command on cyberchef.white.fm will resolve to the IP address of our Nginx Proxy Manager (IP 10.99.100.10). See the image below.\nAttempting to capture data transmitted over HTTPS reveals encryption. The screenshot below shows a WireShark capture of data sent from my PC to an InfluxDB instance on my network using this method. All communication with InfluxDB is now encrypted. Various data sources, including Unifi Poller, Varken, and Home Assistant, now transmit data to InfluxDB over HTTPS as well. See the image below.\nWhile this method of implementing HTTPS support is not a comprehensive security solution, it enhances privacy and reduces the likelihood of malicious actors on your network accessing sensitive information.\nExcellent work! You\u0026rsquo;ve successfully configured the internal half of your \u0026ldquo;split-brain DNS.\u0026rdquo; Repeat these steps to add additional hosts to both Nginx Proxy Manager and PiHole. This setup extends to the Nginx Proxy Manager web page, Unifi router, PiHole web interface, ESXi servers, Proxmox servers, or any Docker containers running within your environment. The possibilities are vast.\nExternal Routing Prepare Your Domain Note: This section is an excerpt from my post: Hosting Your Own Site with Traefik and Wordpress. This outlines the external half of the \u0026ldquo;split-brain DNS\u0026rdquo; and is optional if you don\u0026rsquo;t need external access to your services and Nginx Proxy Manager.\nAdd DNS records for your domain (e.g., whitematter.tech) that point to your server\u0026rsquo;s public IP address. A Dynamic DNS service like duckdns.org is recommended for automatically updating your changing public IP address if you lack a static IP. The image below demonstrates CloudFlare pointing my domain to my DuckDNS address (replace \u0026ldquo;Content\u0026rdquo; with your public IP if not using DuckDNS).\nAdd a CNAME record for each host you want to access externally. For example, a CNAME record for cyberchef would point cyberchef.white.fm to your dynamic DNS host or public IP.\nNext, forward ports 80 and 443 to your Nginx Proxy Manager host. Consult your networking equipment\u0026rsquo;s documentation for instructions.\nOnce ports 80 and 443 are forwarded, any host added to CloudFlare (or your DNS service) will be externally accessible.\n","date":"2022-08-05T11:39:46Z","image":"https://raw.githubusercontent.com/Nexus-Security/image-1/main/https.webp","permalink":"https://Nexus-Security.github.io/internal-https-with-lets-encrypt-using-split-brain-dns/","title":"Internal HTTPS with Let's Encrypt using 'Split-Brain DNS'"},{"content":"Introduction This guide explains how to route specific network traffic through a VPN client on pfSense using Unifi\u0026rsquo;s policy-based routing. This setup allows you to manage your devices and subnets with Unifi\u0026rsquo;s Network app while benefiting from pfSense\u0026rsquo;s VPN capabilities. The result is a secure and private network experience without sacrificing speed.\nThis guide focuses solely on setting up policy-based routing and assumes you have already configured a VPN client on pfSense and segmented your Unifi network with subnets and VLANs.\nRequirements A Unifi Gateway (at least two WAN ports) An active VPN Client A pfSense router (with an unused port connected to the Unifi Gateway) Overview The final setup involves a few key components working together.\nOn pfSense, you\u0026rsquo;ll create a dedicated subnet (e.g., 10.99.3.0/24) and an interface (e.g., 10.99.3.15) to act as the gateway for Unifi\u0026rsquo;s WAN2. This pfSense subnet\u0026rsquo;s traffic will be directed through your VPN client, encrypting it before sending it out via pfSense\u0026rsquo;s WAN connection (e.g., 10.99.1.0/24 subnet on Unifi) and eventually exiting through Unifi\u0026rsquo;s WAN1 (your ISP connection).\nHere\u0026rsquo;s a simplified representation of the traffic flow:\nClient device \u0026ndash;policy-based routing\u0026ndash;\u0026gt; Unifi WAN2 (10.99.3.10) \u0026ndash;\u0026gt; pfSense subnet (10.99.3.0/24) (via pfSense Interface [10.99.3.15]) \u0026ndash;\u0026gt; VPN Gateway (traffic encrypted) \u0026ndash;\u0026gt; pfSense WAN (10.99.1.15) \u0026ndash;\u0026gt; Unifi \u0026ldquo;pfSense WAN\u0026rdquo; subnet (10.99.1.0/24) \u0026ndash;\u0026gt; Unifi WAN1 (ISP)\nPreparing pfSense This guide assumes you\u0026rsquo;ve already set up a VPN client on pfSense and configured the necessary NAT rules.\nSetting the Interface Create a new interface and subnet for Unifi\u0026rsquo;s WAN2 connection: Interfaces \u0026gt; Assignments \u0026gt; select an available Network port \u0026gt; Add. Configure the new interface: Enable the interface and assign a static IP address within a subnet that won\u0026rsquo;t clash with existing Unifi/pfSense subnets (e.g., 10.99.3.15/24). Ensure the CIDR allows communication between Unifi WAN2 and the pfSense address. Outbound NAT Rules Switch the outbound NAT mode to Hybrid: Firewall \u0026gt; NAT \u0026gt; Outbound \u0026gt; select Hybrid \u0026gt; Save. Create rules to direct the Unifi subnet traffic to the VPN. You\u0026rsquo;ll need rules for both regular traffic and ISAKMP traffic: Localhost to VPN rule, ISAKMP Localhost to VPN rule, LAN (Subnet) to VPN rule, ISAKMP LAN (Subnet) to VPN rule. Adjust the rules with your specific VPN interface, subnets, and gateway information. Firewall in pfSense Create an allow rule for the new interface: Firewall \u0026gt; Rules \u0026gt; select your new interface \u0026gt; Add \u0026gt; allow traffic from any source/port to any destination/port \u0026gt; under Advanced, set the gateway to your VPN gateway \u0026gt; Save. Create a block rule for the new subnet to prevent unencrypted traffic leaks: Firewall \u0026gt; Rules \u0026gt; select your new interface \u0026gt; Add \u0026gt; under Source, input your subnet (e.g., 10.99.3.0/24) \u0026gt; Save. Preparing Unifi Assign WAN2 Configure Unifi\u0026rsquo;s WAN2: Settings \u0026gt; Internet \u0026gt; select the second WAN option. Connect the cable from your pfSense interface to the WAN2 port. Set a static IP address within the designated subnet (e.g., 10.99.3.10) and use the pfSense static address as the gateway (e.g., 10.99.3.15) \u0026gt; Save. Policy-Based Routing Set up the routing rule: Settings \u0026gt; Traffic Management \u0026gt; Routes \u0026gt; Create New Route. Define the traffic to be routed. For instance, select your IoT subnet as the source and WAN2 as the destination. All traffic routed through WAN2 will now be encrypted through your VPN.\n","date":"2022-08-01T06:02:23Z","image":"https://raw.githubusercontent.com/Nexus-Security/image-1/main/policy-route-unifi-pfsense-pia.png","permalink":"https://Nexus-Security.github.io/utilizing-policy-based-routing-with-unifi-pia-and-pfsense-directing-my-iot-external-traffic-through-pia-vpn/","title":"Utilizing Policy Based Routing with Unifi, PIA, and pfSense: Directing My IoT External Traffic through PIA VPN"},{"content":"Google, the former search king, isn\u0026rsquo;t cutting it for many users these days.\nSearch results are overrun with ads and aren\u0026rsquo;t optimized for users or advertisers, sometimes even including malware. It\u0026rsquo;s clear their priority is pays.\nSo why is Google still the most popular search engine? It aggressively ensures it\u0026rsquo;s the URL_PLACEHOLDER_4, and many users are either set in their ways or unaware of alternatives.\nKagi My Experience After hearing praise for Kagi, particularly from Mastodon users, I decided to try their 100-query trial. Don\u0026rsquo;t just take my word for it – test it yourself!\nMy initial impression was fantastic. I found what I needed within the first few results, a welcome change from Google.\nGoogle often forces users to wade through ads, outdated but well-ranked pages, and irrelevant links before finding relevant results. The problem is, you only realize this after wasting time clicking and reading.\nKagi saves me time with its superior results and respects my privacy by avoiding tracking and ads.\nThe customization features helps tailor results to my specific needs.\nWhy Kagi? Privacy: Kagi offers an ad-free, tracking-free search experience. Your searches remain private. While it utilizes data from established search engines (Google, Bing, Brave, etc.), queries are anonymized before transmission. Kagi then aggregates and personalizes these results.\nPersonalized Results: Users can fine-tune their results by excluding specific websites or adjusting the ranking of others. Tired of a particular clickbait site? Block it! Prefer prioritizing Wikipedia results? You can do that too.\nLenses: Further refine your search results using lenses. For instance, the \u0026ldquo;Recipe\u0026rdquo; lens focuses on recipes, while the \u0026ldquo;Discussions\u0026rdquo; lens targets forum discussions.\nInstant Answers: Some searches provide immediate, comprehensive answers directly within the results page, eliminating the need to click through. For example, searching for \u0026lsquo;Python for loop\u0026rsquo; displays:\nSummarizer: Although still experimental, the AI-powered Kagi Summarizer is a game-changer (sorry, AI skeptics!). It generates summaries of various public resources, including web pages, podcasts, and YouTube videos. This feature saves me significant time depending on my search needs.\nKagi and Safari Safari users like myself face limitations with adding custom search engines. To circumvent this, Kagi\u0026rsquo;s developers created a Safari extension. This extension intercepts queries sent to your default engine, redirecting them to Kagi. A word of advice: If privacy is a concern, consider switching your default engine to DuckDuckGo or another trusted alternative to prevent data leaks to Google.\nPricing Kagi offers three paid plans:\nStarter: $5/month for 300 queries/month Professional: $10/month for unlimited queries and AI tools like Kagi Summarizer Ultimate: $25/month, similar to Professional but with early access to upcoming features I currently use the Professional plan.\nFinal Thoughts Hopefully, I\u0026rsquo;ve sparked your interest enough to try Kagi\u0026rsquo;s trial. I\u0026rsquo;m eager to hear your thoughts.\nnews\n","date":"2022-07-21T23:39:07.542+01:00","image":"https://raw.githubusercontent.com/Nexus-Security/image-1/main/2024227/kagi-landing-page.jpeg","permalink":"https://Nexus-Security.github.io/kagi-is-a-paid-search-engine-that-provides-excellent-value-for-the-cost/","title":"Kagi is a paid search engine that provides excellent value for the cost"},{"content":"Introduction This tutorial explains how to run a Dockerized MacOS on Unraid using the sickcodes/Docker-OSX project. This project allows for MacOS security research within containers on Linux and Windows.\nThis tutorial focuses on utilizing Unraid\u0026rsquo;s Docker-Compose functionality for container management and building a special VNC-compatible image necessary for Unraid. If you\u0026rsquo;re not using Unraid, the default image might be a better fit.\nThe standard sickcodes/Docker-OSX image on Docker Hub relies on KVM for screen rendering. While this tutorial doesn\u0026rsquo;t offer native NoVNC support like other Unraid containers (yet), it allows connection to the container via a VNC client. A solution for native NoVNC support is currently in progress.\nThis guide offers a concise solution for getting started with a functional and maintained container.\nBuilding the VNC-compatible Image on Unraid Connect to your Unraid server through SSH or the web portal\u0026rsquo;s Terminal. Execute the following commands to download the required Dockerfile and build the image:\n1 2 3 4 5 6 mkdir /var/tmp/macos cd /var/tmp/macos wget https://raw.githubusercontent.com/sickcodes/Docker-OSX/master/vnc-version/Dockerfile docker build -t docker-osx-vnc . Initial image building attempts may encounter invalid key errors for numerous packages and prerequisites. If encountered, add the following lines to the Dockerfile after the \u0026ldquo;ARG MIRROR_COUNT=10\u0026rdquo; line:\n1 2 3 4 5 # Disable signature checks for invalid key errors RUN sudo sed -i \u0026#39;s/SigLevel = Required DatabaseOptional/SigLevel = Never/g\u0026#39; /etc/pacman.conf RUN if [[ \u0026#34;${LINUX}\u0026#34; == true ]]; then \\ sudo pacman -Syu linux libguestfs --noconfirm \\ ; fi Open the Dockerfile:\n1 nano /var/tmp/macos/Dockerfile Paste the lines, save the file (CTRL + X), then rebuild the image:\n1 docker build -t docker-osx-vnc . The image should now build without errors.\nCreating a Docker-Compose stack on Unraid Docker-Compose simplifies container management and provides granular control. This guide leverages this tool for its ease of configuration transfer and sharing capabilities.\nEnsure you have the \u0026ldquo;Docker Compose Manager\u0026rdquo; plugin (by dcflachs) installed from Unraid\u0026rsquo;s Community Applications. At the bottom of the Docker tab, select ADD NEW STACK and name it (e.g., \u0026ldquo;MacOS\u0026rdquo;). Click the gear icon next to the stack name, choose EDIT STACK, then select COMPOSE FILE. Paste the following configuration into the provided text box:\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 version: \u0026#39;3\u0026#39; services: macos: container_name: \u0026#39;macos\u0026#39; image: \u0026#39;docker-osx-vnc:monterey\u0026#39; privileged: true devices: - /dev/kvm - /dev/snd - /dev/null ports: - \u0026#39;8888:5999\u0026#39; - \u0026#39;50922:10022\u0026#39; environment: - \u0026#39;USERNAME=user\u0026#39; - \u0026#39;PASSWORD=pass\u0026#39; - \u0026#39;DISPLAY=${DISPLAY:-:0.0}\u0026#39; Save the changes and click COMPOSE UP to start the MacOS container. The new container will appear alongside your other containers in the Unraid GUI, providing access to logs and console.\nRunning the container without Docker-Compose To run the container without Docker-Compose, execute the following command via SSH or the GUI Terminal:\n1 docker run --device /dev/kvm --device /dev/snd -p 8888:5999 -p 50922:10022 -d --privileged docker-osx-vnc:latest Connecting to the MacOS Container with VNC Protocol Before connecting, obtain the generated VNC password. On Unraid, locate the macos container, click it, and select \u0026quot;Console\u0026quot;.\nIn the new Terminal window, type:\n1 cat vncpasswd_file The output is your container\u0026rsquo;s VNC password.\nWindows\nDownload and install a VNC client that supports TigerVNC (e.g., TightVNC Client). Launch the client and enter your container\u0026rsquo;s IP address followed by \u0026quot;::8888\u0026quot;. Provide the VNC password when prompted.\nMacOS\nUtilize the built-in VNC client by pressing CMD+K and entering vnc://IP_OF_MACOS_DOCKER:8888. Enter the VNC password when prompted.\n","date":"2022-07-04T20:41:21Z","image":"https://raw.githubusercontent.com/Nexus-Security/image-1/main/dockermac.jpg","permalink":"https://Nexus-Security.github.io/running-dockerized-macos-on-unraid-a-step-by-step-guide/","title":"Running Dockerized MacOS on Unraid: A Step-by-Step Guide"},{"content":"From time to time, discussions about AI coding tools dominate my Mastodon and LinkedIn feeds, with opinions ranging from complete adoration to utter disapproval.\nSo, are these tools inherently good or bad? As with most things, the answer lies somewhere in the middle: it all comes down to how you use them.\nThe Current Landscape Recent years have witnessed significant advancements in Large Language Models (also known as Generative AI), leading to the development of \u0026ldquo;AI tools\u0026rdquo; designed to assist developers with coding tasks.\nTools like GitHub Copilot and ChatGPT are becoming increasingly popular among developers. They unlock new possibilities, such as enabling us to write code in unfamiliar languages, and streamline existing tasks that would typically take longer.\nThe Hallucination Hazard Before delving into the pros and cons, it\u0026rsquo;s crucial to acknowledge that LLMs can hallucinate. This means that under certain conditions (which can arise unexpectedly), the model might generate inaccurate or fabricated information.\nEven ChatGPT acknowledges this limitation, stating at the bottom of each page:\nChatGPT can make mistakes. Consider checking important information.\nA Case for Good Usage Prior to ChatGPT, my go-to resource for coding conundrums was Google.\nExample: \u0026ldquo;How do I find duplicated SQL rows with the same field?\u0026rdquo;\nI would search for a solution, read numerous blog posts, try to understand if a solution was feasible, and then refine my search to find code examples (typically on Stack Overflow) that I would need to adapt to my existing codebase after consulting documentation.\nPosing the same question to ChatGPT yielded a correct (at least in my experience) SQL query accompanied by an explanation of the syntax used. I tested this query on a local database, adjusting field names accordingly, and it produced the desired result.\nWhile I still needed to integrate the generated code into my project (renaming fields, incorporating it within a larger context, etc.), it undoubtedly saved me a significant amount of time compared to my previous workflow.\nThink of AI tools as driving assistance systems: they are not meant to drive for you, but they can assist in maintaining a safe distance from other vehicles, staying within the speed limit, and warning you about potential hazards while overtaking.\nIn Conclusion AI tools have the potential to be incredibly useful for coding tasks. However, like any tool, it\u0026rsquo;s our responsibility to use them appropriately and ensure the accuracy of the final output. Remaining vigilant in preventing the introduction of erroneous information or malfunctioning code is paramount.\n","date":"2022-06-22T10:08:55.787+01:00","image":"https://raw.githubusercontent.com/Nexus-Security/image-1/main/2024227/ai-tools-cover.webp","permalink":"https://Nexus-Security.github.io/is-the-use-of-ai-tools-for-coding-beneficial-or-detrimental/","title":"Is the use of AI tools for coding beneficial or detrimental?"}]