<!doctype html><html lang=en dir=ltr><head><meta charset=utf-8><meta name=viewport content='width=device-width,initial-scale=1'><meta name=description content="In the realm of traditional programming, programmers meticulously craft each software instruction, leaving no room for learning from data. Conversely, machine learning, a fascinating branch of computer science, empowers computers to learn and extract knowledge from data using statistical methods, without the need for explicit programming.\nThis reinforcement learning tutorial will demonstrate how to utilize PyTorch to train a reinforcement learning neural network to master the game Flappy Bird. Before delving into the implementation, let&rsquo;s establish a foundation by exploring some essential building blocks.\n"><title>Learning How to Play Flappy Bird: A Guide to Reinforcement Learning</title>
<link rel=canonical href=https://Nexus-Security.github.io/learning-how-to-play-flappy-bird-a-guide-to-reinforcement-learning/><link rel=stylesheet href=/scss/style.min.0304c6baf04e01a8fe70693791cb744d56a3578a3120a8796cefc66825aa39c7.css><meta property='og:title' content="Learning How to Play Flappy Bird: A Guide to Reinforcement Learning"><meta property='og:description' content="In the realm of traditional programming, programmers meticulously craft each software instruction, leaving no room for learning from data. Conversely, machine learning, a fascinating branch of computer science, empowers computers to learn and extract knowledge from data using statistical methods, without the need for explicit programming.\nThis reinforcement learning tutorial will demonstrate how to utilize PyTorch to train a reinforcement learning neural network to master the game Flappy Bird. Before delving into the implementation, let&rsquo;s establish a foundation by exploring some essential building blocks.\n"><meta property='og:url' content='https://Nexus-Security.github.io/learning-how-to-play-flappy-bird-a-guide-to-reinforcement-learning/'><meta property='og:site_name' content='Nexus Security'><meta property='og:type' content='article'><meta property='article:section' content='Post'><meta property='article:published_time' content='2022-10-01T00:00:00+00:00'><meta property='article:modified_time' content='2022-10-01T00:00:00+00:00'><meta name=twitter:title content="Learning How to Play Flappy Bird: A Guide to Reinforcement Learning"><meta name=twitter:description content="In the realm of traditional programming, programmers meticulously craft each software instruction, leaving no room for learning from data. Conversely, machine learning, a fascinating branch of computer science, empowers computers to learn and extract knowledge from data using statistical methods, without the need for explicit programming.\nThis reinforcement learning tutorial will demonstrate how to utilize PyTorch to train a reinforcement learning neural network to master the game Flappy Bird. Before delving into the implementation, let&rsquo;s establish a foundation by exploring some essential building blocks.\n"><link rel="shortcut icon" href=/fav.ico></head><body class=article-page><script>(function(){const e="StackColorScheme";localStorage.getItem(e)||localStorage.setItem(e,"light")})()</script><script>(function(){const t="StackColorScheme",e=localStorage.getItem(t),n=window.matchMedia("(prefers-color-scheme: dark)").matches===!0;e=="dark"||e==="auto"&&n?document.documentElement.dataset.scheme="dark":document.documentElement.dataset.scheme="light"})()</script><div class="container main-container flex on-phone--column extended"><aside class="sidebar left-sidebar sticky"><button class="hamburger hamburger--spin" type=button id=toggle-menu aria-label="Toggle Menu">
<span class=hamburger-box><span class=hamburger-inner></span></span></button><ol class=menu-social><li><a href=https://github.com/Nexus-Security target=_blank title=GitHub rel=me><svg xmlns:xlink="http://www.w3.org/1999/xlink" width="150" height="150" viewBox="0 0 150 150"><defs><filter id="alpha" filterUnits="objectBoundingBox" x="0" y="0" width="100%" height="100%"><feColorMatrix type="matrix" in="SourceGraphic" values="0 0 0 0 1 0 0 0 0 1 0 0 0 0 1 0 0 0 1 0"/></filter><mask id="mask0"><g filter="url(#alpha)"><rect x="0" y="0" width="150" height="150" style="fill:#000;fill-opacity:.2;stroke:none"/></g></mask><clipPath id="clip1"><rect x="0" y="0" width="150" height="150"/></clipPath><g id="surface5" clip-path="url(#clip1)"><path style="stroke:none;fill-rule:nonzero;fill:#fff;fill-opacity:1" d="M10.9375 83.867188c0-12.109376 3.46875-21.816407 10.925781-30.535157.71875-.875.925781000000001-2.058593.550781000000001-3.125C21.6875 48.125 21.40625 33.070312 26.488281 17.675781c4.78125 1.207031 16.300781 5.050781 31.65625 16.398438C58.9375 34.65625 59.957031 34.84375 60.894531 34.554688 64.824219 33.382812 71.851562 32.6875 79.6875 32.6875S94.550781 33.386719 98.476562 34.554688C99.417969 34.835938 100.433594 34.65625 101.226562 34.074219 113.96875 24.65625 124.011719 20.445312 129.8125 18.582031c-.476562000000001-1.882812-1.019531-3.746093-1.625-5.589843-4.78125 1.207031-16.300781 5.050781-31.648438 16.394531C95.746094 29.96875 94.730469 30.144531 93.789062 29.867188c-3.925781-1.171876-10.953124-1.875-18.789062-1.875-7.832031.0-14.855469.703124000000003-18.792969 1.871093C55.265625 30.144531 54.246094 29.964844 53.457031 29.382812c-15.355469-11.34375-26.875-15.1875-31.65625-16.402343C16.71875 28.382812 17 43.4375 17.726562 45.523438 18.101562 46.589844 17.890625 47.777344 17.175781 48.648438 9.71875 57.367188 6.25 67.070312 6.25 79.179688c0 23.515624 7.175781 37.921874 17.417969 46.550781C16.007812 116.851562 10.9375 103.582031 10.9375 83.867188zm0 0"/></g><mask id="mask1"><g filter="url(#alpha)"><rect x="0" y="0" width="150" height="150" style="fill:#000;fill-opacity:.101961;stroke:none"/></g></mask><clipPath id="clip2"><rect x="0" y="0" width="150" height="150"/></clipPath><g id="surface8" clip-path="url(#clip2)"><path style="stroke:none;fill-rule:nonzero;fill:#000;fill-opacity:1" d="M32.34375 78.5c3.945312-4 9.382812-5.074219 16.039062-5.074219C51.925781 73.425781 55.824219 73.730469 60.023438 74.054688c4.71875.375 9.589843.71875 14.84375.746093000000002C80.335938 74.917969 85.210938 74.425781 89.976562 74.054688 102.070312 73.125 111.605469 72.367188 117.648438 78.492188c4.527343 4.601562 6.890624 10.328124 7.21875 16.945312C124.925781 94.398438 125 93.382812 125 92.261719c0-7.285157-2.476562-13.492188-7.351562-18.449219-6.042969-6.117188-15.578126-5.367188-27.671876-4.4375C85.210938 69.742188 80.34375 70.238281 74.867188 70.117188 69.914062 70.0625 64.960938 69.8125 60.023438 69.375 55.824219 69.050781 51.929688 68.742188 48.382812 68.742188c-6.65625.0-12.09375 1.070312-16.039062 5.078124C27.476562 78.769531 25 84.976562 25 92.261719 25 93.375 25.074219 94.398438 25.132812 95.445312 25.460938 88.820312 27.820312 83.101562 32.34375 78.5zm0 0"/></g><linearGradient id="linear0" gradientUnits="userSpaceOnUse" x1="10.768" y1="13.86" x2="18.633" y2="21.724" gradientTransform="matrix(6.25,0,0,6.25,0,0)"><stop offset="0" style="stop-color:#000;stop-opacity:.101961"/><stop offset="1" style="stop-color:#000;stop-opacity:0"/></linearGradient><linearGradient id="linear1" gradientUnits="userSpaceOnUse" x1="4.004" y1="18.529" x2="4.047" y2="18.573" gradientTransform="matrix(6.25,0,0,6.25,0,0)"><stop offset="0" style="stop-color:#000;stop-opacity:.101961"/><stop offset="1" style="stop-color:#000;stop-opacity:0"/></linearGradient><linearGradient id="linear2" gradientUnits="userSpaceOnUse" x1=".617" y1="5.771" x2="23.69" y2="16.53" gradientTransform="matrix(6.25,0,0,6.25,0,0)"><stop offset="0" style="stop-color:#fff;stop-opacity:.2"/><stop offset="1" style="stop-color:#fff;stop-opacity:0"/></linearGradient></defs><g id="surface1"><path style="stroke:none;fill-rule:nonzero;fill:#303c42;fill-opacity:1" d="M138.570312 45.789062C139.492188 39.613281 138.867188 23.738281 133.167969 8.292969 132.660156 6.945312 131.300781 6.113281 129.867188 6.273438 129.269531 6.34375 115.1875 8.164062 94.039062 23.46875 89.445312 22.363281 82.632812 21.742188 75 21.742188s-14.4375.625-19.042969 1.726562C34.800781 8.15625 20.71875 6.34375 20.117188 6.273438 18.6875 6.117188 17.332031 6.949219 16.820312 8.292969 11.117188 23.738281 10.5 39.613281 11.425781 45.789062 3.742188 55.289062.0 66.230469.0 79.179688c0 41.632812 21.824219 64.558593 61.457031 64.558593L75 143.75 88.539062 143.738281C128.167969 143.738281 150 120.8125 150 79.179688c0-12.949219-3.742188-23.890626-11.429688-33.390626zm0 0"/><path style="stroke:none;fill-rule:nonzero;fill:#5c6671;fill-opacity:1" d="M88.539062 137.488281 75 137.5 61.457031 137.488281C36.273438 137.488281 6.25 127.367188 6.25 79.179688c0-12.109376 3.46875-21.816407 10.925781-30.535157.71875-.875.925781000000001-2.058593.550781000000001-3.125C17 43.4375 16.71875 28.382812 21.800781 12.988281c4.78125 1.207031 16.300781 5.050781 31.65625 16.398438C54.25 29.96875 55.269531 30.15625 56.207031 29.867188c3.9375-1.171876 10.960938-1.875 18.792969-1.875 7.835938.0 14.863281.703124000000003 18.789062 1.871093C94.730469 30.140625 95.746094 29.964844 96.539062 29.382812c15.347657-11.34375 26.867188-15.1875 31.648438-16.394531 5.082031 15.394531 4.804688 30.449219 4.082031 32.53125C131.867188 46.585938 132.078125 47.785156 132.820312 48.648438 140.273438 57.367188 143.75 67.070312 143.75 79.179688c0 48.1875-30.023438 58.308593-55.210938 58.308593zm0 0"/><use xlink:href="#surface5" mask="url(#mask0)"/><path style="stroke:none;fill-rule:nonzero;fill:#303c42;fill-opacity:1" d="M89.488281 63.144531C84.886719 63.507812 80.125 63.875 75.226562 63.875H74.78125C69.875 63.875 65.117188 63.5 60.519531 63.144531 47.898438 62.164062 35.988281 61.230469 27.894531 69.429688 21.824219 75.59375 18.75 83.28125 18.75 92.261719 18.75 128.238281 46.429688 131.25 75.226562 131.25c28.34375.0 56.023438-3.011719 56.023438-38.988281.0-8.988281-3.074219-16.667969-9.144531-22.835938-8.085938-8.195312-20.003907-7.269531-32.617188-6.28125zm0 0"/><path style="stroke:none;fill-rule:nonzero;fill:#d9cfcc;fill-opacity:1" d="M74.773438 125C42.492188 125 25 119.78125 25 92.261719c0-7.285157 2.476562-13.492188 7.34375-18.449219 3.945312-4 9.382812-5.074219 16.039062-5.074219C51.925781 68.738281 55.824219 69.042969 60.023438 69.367188c4.71875.375 9.589843.71875 14.84375.746093000000002C80.335938 70.230469 85.210938 69.738281 89.976562 69.367188c12.09375-.929687999999999 21.628907-1.6875 27.671876 4.4375C122.523438 78.769531 125 84.976562 125 92.261719 125 119.78125 107.507812 125 74.773438 125zm0 0"/><use xlink:href="#surface8" mask="url(#mask1)"/><path style="stroke:none;fill-rule:nonzero;fill:#303c42;fill-opacity:1" d="M50 75c-7.125.0-12.5 9.40625-12.5 21.875S42.875 118.75 50 118.75s12.5-9.40625 12.5-21.875S57.125 75 50 75zm0 0"/><path style="stroke:none;fill-rule:nonzero;fill:#6d4c41;fill-opacity:1" d="M50 112.5c-2.550781.0-6.25-6.085938-6.25-15.625S47.449219 81.25 50 81.25s6.25 6.085938 6.25 15.625S52.550781 112.5 50 112.5zm0 0"/><path style="stroke:none;fill-rule:nonzero;fill:#303c42;fill-opacity:1" d="M1e2 75c-7.125.0-12.5 9.40625-12.5 21.875S92.875 118.75 1e2 118.75s12.5-9.40625 12.5-21.875S107.125 75 1e2 75zm0 0"/><path style="stroke:none;fill-rule:nonzero;fill:#6d4c41;fill-opacity:1" d="M1e2 112.5c-2.550781.0-6.25-6.085938-6.25-15.625S97.449219 81.25 1e2 81.25s6.25 6.085938 6.25 15.625S102.550781 112.5 1e2 112.5zm0 0"/><path style="stroke:none;fill-rule:nonzero;fill:#a1887f;fill-opacity:1" d="M52.21875 90.511719c0 1.539062-1.246094 2.789062-2.789062 2.789062-1.539063.0-2.785157-1.25-2.785157-2.789062.0-1.539063 1.246094-2.785157 2.785157-2.785157 1.542968.0 2.789062 1.246094 2.789062 2.785157zm0 0"/><path style="stroke:none;fill-rule:nonzero;fill:#a1887f;fill-opacity:1" d="M102.21875 90.511719c0 1.539062-1.246094 2.789062-2.789062 2.789062-1.539063.0-2.785157-1.25-2.785157-2.789062.0-1.539063 1.246094-2.785157 2.785157-2.785157 1.542968.0 2.789062 1.246094 2.789062 2.785157zm0 0"/><path style="stroke:none;fill-rule:nonzero;fill:url(#linear0)" d="M125 72.8125 124.894531 72.855469C129.039062 78.382812 131.25 84.851562 131.25 92.261719c0 35.976562-27.679688 38.988281-56.023438 38.988281-16.664062.0-32.914062-1.054688-43.683593-8.875l13.386719 13.386719c5.507812 1.207031 11.121093 1.71875 16.527343 1.71875L75 137.5 88.539062 137.488281c22.992188.0 49.941407-8.53125 54.472657-46.664062zm0 0"/><path style="stroke:none;fill-rule:nonzero;fill:url(#linear1)" d="M25.070312 115.800781 25 115.832031 25.269531 116.101562zm0 0"/><path style="stroke:none;fill-rule:nonzero;fill:url(#linear2)" d="M138.570312 45.789062C139.492188 39.613281 138.867188 23.738281 133.167969 8.292969 132.660156 6.945312 131.300781 6.113281 129.867188 6.273438 129.269531 6.34375 115.1875 8.164062 94.039062 23.46875 89.445312 22.363281 82.632812 21.742188 75 21.742188s-14.4375.625-19.042969 1.726562C34.800781 8.15625 20.71875 6.34375 20.117188 6.273438 18.6875 6.117188 17.332031 6.949219 16.820312 8.292969 11.117188 23.738281 10.5 39.613281 11.425781 45.789062 3.742188 55.289062.0 66.230469.0 79.179688c0 41.632812 21.824219 64.558593 61.457031 64.558593L75 143.75 88.539062 143.738281C128.167969 143.738281 150 120.8125 150 79.179688c0-12.949219-3.742188-23.890626-11.429688-33.390626zm0 0"/></g></svg></a></li><li><a href=mailto:0x000216@gmail.com target=_blank title=Email rel=me><svg width="512" height="512" viewBox="0 0 512 512"><path d="M33.994 73.934C27.135 76.021 21.866 80.426 18.364 87 16.567 90.374 16.5 96.438 16.5 256V421.5l2.158 4C21.184 430.18 25.101 434.027 30 436.636 33.003 438.235 36.304 438.546 53.246 438.825L72.993 439.15l.253-137.978L73.5 163.194 90 175.45C99.075 182.191 140.17 212.762 181.321 243.386l74.821 55.68 21.179-15.752C288.97 274.65 330 244.021 368.5 215.248l70-52.312L438.754 301.043 439.007 439.15 458.754 438.825C476.726 438.529 478.859 438.306 482.5 436.342 487.18 433.816 491.027 429.899 493.636 425 495.433 421.626 495.5 415.562 495.5 256 495.5 96.438 495.433 90.374 493.636 87 488.565 77.48 482.099 73.799 469.397 73.202L460.293 72.774 454.397 77.337C439.883 88.566 256.423 221.935 255.772 221.73 254.976 221.479 78.972 93.309 62.177 80.75L51.812 73 44.156 73.086C39.945 73.133 35.372 73.515 33.994 73.934" stroke="none" fill="#d34c3c" fill-rule="evenodd"/><path d="M54.528 74.911C55.612 75.963 74.953 90.25 97.507 106.661c22.555 16.412 67.228 48.964 99.275 72.339s58.594 42.604 58.993 42.731C256.549 221.978 454.147 78.147 457.472 74.916 459.416 73.028 456.501 73 256 73 55.677 73 52.586 73.029 54.528 74.911M73 300.878V439H256 439V300.941C439 190.749 438.748 163.038 437.75 163.654 437.063 164.078 405.45 187.632 367.5 215.994c-37.95 28.363-78.528 58.655-90.173 67.316l-21.173 15.748-66.827-49.716C152.572 221.999 111.925 191.776 99 182.18s-24.062-17.892-24.75-18.436C73.251 162.954 73 190.519 73 300.878" stroke="none" fill="#e5e5e5" fill-rule="evenodd"/></svg></a></li><li><a href=https://nexus-security.github.io/index.xml target=_blank title=RSS rel=me><svg id="Layer_1" xmlns:xlink="http://www.w3.org/1999/xlink" viewBox="0 0 512 512" style="enable-background:new 0 0 512 512"><path style="fill:#f78c20" d="M78.333 355.334C35.14 355.334.0 390.474.0 433.667S35.14 512 78.333 512s78.333-35.14 78.333-78.333-35.14-78.333-78.333-78.333z"/><g><path style="fill:#ffa929" d="M78.333 381.445c-28.795.0-52.222 23.427-52.222 52.222s23.427 52.222 52.222 52.222 52.222-23.427 52.222-52.222-23.427-52.222-52.222-52.222z"/><path style="fill:#ffa929" d="M477.918 264.861c-21.843-51.641-53.111-98.019-92.936-137.842-39.824-39.824-86.201-71.093-137.843-92.935C193.669 11.468 136.874.0 78.333.0c-4.807.0-8.704 3.897-8.704 8.704v85.519c0 4.807 3.897 8.704 8.704 8.704 182.37.0 330.74 148.369 330.74 330.74.0 4.807 3.897 8.704 8.704 8.704h85.52c4.807.0 8.704-3.897 8.704-8.704C512 375.126 500.533 318.331 477.918 264.861z"/><path style="fill:#ffa929" d="M78.333 163.853c-4.807.0-8.704 3.897-8.704 8.704v95.74c0 4.807 3.897 8.704 8.704 8.704 86.386.0 156.666 70.281 156.666 156.666.0 4.807 3.897 8.704 8.704 8.704h95.74c4.807.0 8.704-3.897 8.704-8.704.0-72.07-28.065-139.826-79.027-190.787-50.961-50.961-118.717-79.027-190.787-79.027z"/></g><g><path style="fill:#f78c20" d="M78.333 242.186c-2.918.0-5.817.076-8.704.206v25.905c0 4.807 3.897 8.704 8.704 8.704 86.386.0 156.666 70.281 156.666 156.666.0 4.807 3.897 8.704 8.704 8.704h25.905c.129-2.886.206-5.786.206-8.704.0-105.752-85.729-191.481-191.481-191.481z"/><path style="fill:#f78c20" d="M78.333 68.113c-2.91.0-5.81.042-8.704.11v26.001c0 4.807 3.897 8.704 8.704 8.704 182.37.0 330.74 148.369 330.74 330.74.0 4.807 3.897 8.704 8.704 8.704h26.001c.067-2.894.11-5.793.11-8.704C443.887 231.777 280.223 68.113 78.333 68.113z"/></g><g/><g/><g/><g/><g/><g/><g/><g/><g/><g/><g/><g/><g/><g/><g/></svg></a></li><li><a href=https://x.com/0x000216 target=_blank title=Twitter rel=me><svg xmlns:xlink="http://www.w3.org/1999/xlink" width="150" height="150" viewBox="0 0 150 150"><g id="surface1"><path style="stroke:none;fill-rule:nonzero;fill:#03a9f4;fill-opacity:1" d="M150 33.179688c-5.640625 2.460937-11.609375 4.09375-17.71875 4.855468 6.4375-3.820312 11.25-9.867187 13.527344-16.996094-6.027344 3.574219-12.621094 6.089844-19.5 7.441407-8.628906-9.210938-22.007813-12.214844-33.746094-7.574219-11.738281 4.636719-19.449219 15.980469-19.445312 28.601562.0 2.4375.203124000000003 4.78125.710937000000001 7.015626C49.085938 55.308594 26.03125 43.609375 10.445312 24.355469 2.25 38.402344 6.386719 56.398438 19.894531 65.457031 15.027344 65.324219 10.261719 64.027344 6 61.667969V62.007812c.015625 14.636719 10.304688 27.25 24.636719 30.214844-2.628907.691406000000001-5.339844 1.03125-8.0625 1.011719C20.621094 93.269531 18.667969 93.09375 16.753906 92.710938c4.070313 12.507812 15.585938 21.089843 28.734375 21.421874-10.886719 8.511719-24.308593 13.128907-38.128906 13.113282C4.835938 127.246094 2.417969 127.132812.0 126.824219c14.0625 9.0625 30.445312 13.855469 47.175781 13.800781 56.585938.0 87.523438-46.875 87.523438-87.507812.0-1.359376-.046875-2.671876-.113281000000001-3.972657C140.652344 44.804688 145.875 39.394531 150 33.179688zm0 0"/></g></svg></a></li><li><a href=https://www.minds.com/0x000216/ target=_blank title=Minds rel=me><svg id="Capa_1" xmlns:xlink="http://www.w3.org/1999/xlink" viewBox="0 0 512 512" style="enable-background:new 0 0 512 512"><g><g><path style="fill:#ffe1b2" d="M256 33.085C245.078 13.38 224.079.0 2e2.0c-23.781.0-45.57 13.293-56.594 34.184C115.711 41.602 96 66.977 96 96c0 .059.0.113.0.172-9.977 7.512-16 19.301-16 31.828.0 1.316.078 2.637.234 3.992C60.211 145.266 48 167.758 48 192c0 14.07 4.039 27.543 11.719 39.262C57.273 236.512 56 242.207 56 248c0 2.738.281 5.445.828 8.098C36.672 267.308 24 288.539 24 312c0 27.973 18.305 52.34 44.109 60.785C65.398 378.828 64 385.324 64 392c0 21.098 13.805 39.508 33.539 45.727C103.891 466.746 129.828 488 160 488c4.617.0 9.227-.512 13.766-1.527C181.992 502 198.141 512 216 512c16.687.0 31.396-8.567 40-21.523V33.085z"/></g><g><g><path style="fill:#ffb980" d="M264 256c-4.422.0-8-3.582-8-8 0-22.055-17.945-40-40-40-8.008.0-15.734 2.355-22.336 6.812-3.023 2.043-7.055 1.781-9.797-.652-3.156-2.809-8.477-6.16-15.867-6.16-4.422.0-8-3.582-8-8s3.578-8 8-8c7.711.0 15.234 2.293 21.719 6.539C197.773 194.246 206.758 192 216 192c30.875.0 56 25.121 56 56C272 252.418 268.422 256 264 256z"/></g></g><g><g><path style="fill:#ffb980" d="M120 120c18.977.0 36.875 7.312 50.414 20.594 3.141 3.09 8.203 3.047 11.312-.109 3.094-3.152 3.047-8.219-.109-11.312C165.07 112.941 143.187 104 120 104c-13.046.0-25.395 2.93-36.542 8.046C81.253 117.019 80 122.423 80 128c0 1.316.078 2.637.234 3.992-.094.062-.173.139-.267.202C91.423 124.501 105.193 120 120 120z"/></g></g><g><g><path style="fill:#ffb980" d="M216 360c0-4.418-3.578-8-8-8s-8 3.582-8 8c0 17.645-14.352 32-32 32-14.211.0-26.82-9.648-30.664-23.465-.703-2.512-2.578-4.523-5.039-5.395-2.453-.871-5.188-.492-7.305 1.02C114.094 371.906 101.305 376 88 376c-6.948.0-13.625-1.149-19.894-3.207-2.214 4.939-3.501 10.19-3.916 15.586C71.714 390.73 79.711 392 88 392c13.297.0 26.187-3.266 37.773-9.52C133.969 397.894 150.141 408 168 408c26.469.0 48-21.531 48-48z"/></g></g><g><path style="fill:#fdc88e" d="M488 312c0-23.461-12.672-44.691-32.828-55.902.547-2.652.828-5.359.828-8.098.0-5.793-1.273-11.488-3.719-16.738C459.961 219.543 464 206.07 464 192c0-24.242-12.211-46.734-32.234-60.008.156-1.355.234-2.676.234-3.992.0-12.527-6.023-24.316-16-31.828.0-.059.0-.113.0-.172.0-29.023-19.711-54.398-47.406-61.816C357.57 13.293 335.781.0 312 0c-24.08.0-45.078 13.38-56 33.085v457.391C264.604 503.433 279.313 512 296 512c17.859.0 34.008-10 42.234-25.527C342.773 487.488 347.383 488 352 488c30.172.0 56.109-21.254 62.461-50.273C434.195 431.508 448 413.097 448 392c0-6.676-1.398-13.172-4.109-19.215C469.695 364.34 488 339.973 488 312z"/></g><g><path style="fill:#f8ab6b" d="M272.008 151.199C272 151.465 272 151.734 272 152c0 26.469 21.531 48 48 48s48-21.531 48-48c0-4.418-3.578-8-8-8s-8 3.582-8 8c0 17.645-14.352 32-32 32s-32-14.355-32-32c0-2.184.219-4.359.656-6.465.492-2.395-.133-4.883-1.703-6.754-1.57-1.871-4.016-3.066-6.352-2.859-.453.012-.891.059-.602.078-13.234.0-24-10.766-24-24v31.813C260.673 147.348 266.061 149.988 272.008 151.199z"/></g><g><path style="fill:#f8ab6b" d="M296 328c9.242.0 18.219-2.246 26.281-6.539C328.765 325.707 336.289 328 344 328c4.422.0 8-3.582 8-8s-3.578-8-8-8c-7.391.0-12.711-3.352-15.867-6.16-2.742-2.434-6.766-2.695-9.797-.656C311.726 309.644 304 312 296 312c-22.055.0-40-17.945-40-40v39.116C266.174 321.517 280.337 328 296 328z"/></g><g><g><path style="fill:#f8ab6b" d="M431.765 131.992c.156-1.355.234-2.676.234-3.992.0-5.577-1.253-10.981-3.458-15.954C417.395 106.93 405.046 104 392 104c-4.422.0-8 3.582-8 8s3.578 8 8 8c14.807.0 28.577 4.501 40.032 12.194C431.939 132.131 431.859 132.054 431.765 131.992z"/></g></g><g><g><path style="fill:#f8ab6b" d="M447.81 388.38c-.415-5.396-1.702-10.647-3.916-15.586C437.624 374.85 430.948 376 424 376c-13.578.0-26.594-4.266-37.641-12.332-2.07-1.5-4.719-1.93-7.133-1.168-2.43.77-4.344 2.648-5.164 5.059C369.101 382.176 355.414 392 340 392c-4.422.0-8 3.582-8 8s3.578 8 8 8c18.875.0 35.961-10.191 45.094-26.156C396.976 388.512 410.258 392 424 392 432.288 392 440.285 390.73 447.81 388.38z"/></g></g></g><g/><g/><g/><g/><g/><g/><g/><g/><g/><g/><g/><g/><g/><g/><g/></svg></a></li><li><a href=https://www.buymeacoffee.com/0x000216 target=_blank title=Coffee rel=me><svg id="Capa_1" xmlns:xlink="http://www.w3.org/1999/xlink" viewBox="0 0 340 340" style="enable-background:new 0 0 340 340"><g id="XMLID_18_"><polygon id="XMLID_138_" style="fill:#dedde0" points="76.429,290 80,340 170,340 170,290"/><polygon id="XMLID_169_" style="fill:#dedde0" points="170,80 61.429,80 65,130 170,130"/><polygon id="XMLID_197_" style="fill:#acabb1" points="170,290 170,340 260,340 263.571,290"/><polygon id="XMLID_221_" style="fill:#acabb1" points="170,80 170,130 275,130 278.571,80"/><path id="XMLID_222_" style="fill:#ffda44" d="M170 260c-22.091.0-40-22.386-40-50s17.909-50 40-50v-30H65 50l10 160h16.429H170V260z"/><path id="XMLID_33_" style="fill:#ff9811" d="M170 130v30c22.091.0 40 22.386 40 50s-17.909 50-40 50v30h93.571H280l10-160h-15H170z"/><path id="XMLID_223_" style="fill:#50412e" d="M210 210c0-27.614-17.909-50-40-50v1e2c22.091.0 40-22.386 40-50z"/><path id="XMLID_224_" style="fill:#786145" d="M130 210c0 27.614 17.909 50 40 50V160c-22.091.0-40 22.386-40 50z"/><polygon id="XMLID_225_" style="fill:#50412e" points="278.571,80 300,80 300,40 260,40 260,0 80,0 80,40 40,40 40,80 61.429,80 170,80"/></g><g/><g/><g/><g/><g/><g/><g/><g/><g/><g/><g/><g/><g/><g/><g/></svg></a></li></ol><ol class=menu id=main-menu><li><a href=/><svg class="icon icon-tabler icon-tabler-home" width="24" height="24" viewBox="0 0 24 24" stroke-width="2" stroke="currentcolor" fill="none" stroke-linecap="round" stroke-linejoin="round"><path stroke="none" d="M0 0h24v24H0z"/><polyline points="5 12 3 12 12 3 21 12 19 12"/><path d="M5 12v7a2 2 0 002 2h10a2 2 0 002-2v-7"/><path d="M9 21v-6a2 2 0 012-2h2a2 2 0 012 2v6"/></svg>
<span>Home</span></a></li><li><a href=/about/><svg class="icon icon-tabler icon-tabler-user" width="24" height="24" viewBox="0 0 24 24" stroke-width="2" stroke="currentcolor" fill="none" stroke-linecap="round" stroke-linejoin="round"><path stroke="none" d="M0 0h24v24H0z"/><circle cx="12" cy="7" r="4"/><path d="M6 21v-2a4 4 0 014-4h4a4 4 0 014 4v2"/></svg>
<span>About Us</span></a></li><li><a href=/termsofservice/><svg class="icon icon-tabler icon-tabler-pencil" width="24" height="24" viewBox="0 0 24 24" stroke-width="2" stroke="currentcolor" fill="none" stroke-linecap="round" stroke-linejoin="round"><path stroke="none" d="M0 0h24v24H0z" fill="none"/><path d="M4 20h4L18.5 9.5a1.5 1.5.0 00-4-4L4 16v4"/><line x1="13.5" y1="6.5" x2="17.5" y2="10.5"/></svg>
<span>Terms Of Service</span></a></li><li><a href=/privacypolicy/><svg class="icon icon-tabler icon-tabler-archive" width="24" height="24" viewBox="0 0 24 24" stroke-width="2" stroke="currentcolor" fill="none" stroke-linecap="round" stroke-linejoin="round"><path stroke="none" d="M0 0h24v24H0z"/><rect x="3" y="4" width="18" height="4" rx="2"/><path d="M5 8v10a2 2 0 002 2h10a2 2 0 002-2V8"/><line x1="10" y1="12" x2="14" y2="12"/></svg>
<span>Privacy Policy</span></a></li><li><a href=/disclaimer/><svg class="icon icon-tabler icon-tabler-link" width="24" height="24" viewBox="0 0 24 24" stroke-width="2" stroke="currentcolor" fill="none" stroke-linecap="round" stroke-linejoin="round"><path stroke="none" d="M0 0h24v24H0z"/><path d="M10 14a3.5 3.5.0 005 0l4-4a3.5 3.5.0 00-5-5l-.5.5"/><path d="M14 10a3.5 3.5.0 00-5 0l-4 4a3.5 3.5.0 005 5l.5-.5"/></svg>
<span>Disclaimer</span></a></li><li><a href=/search/><svg class="icon icon-tabler icon-tabler-search" width="24" height="24" viewBox="0 0 24 24" stroke-width="2" stroke="currentcolor" fill="none" stroke-linecap="round" stroke-linejoin="round"><path stroke="none" d="M0 0h24v24H0z"/><circle cx="10" cy="10" r="7"/><line x1="21" y1="21" x2="15" y2="15"/></svg>
<span>Search</span></a></li><li><a href=/contact/><svg class="icon icon-tabler icon-tabler-mail" width="44" height="44" viewBox="0 0 24 24" stroke-width="1.5" stroke="currentcolor" fill="none" stroke-linecap="round" stroke-linejoin="round"><path stroke="none" d="M0 0h24v24H0z" fill="none"/><rect x="3" y="5" width="18" height="14" rx="2"/><polyline points="3 7 12 13 21 7"/></svg>
<span>Contact Us</span></a></li><li class=menu-bottom-section><ol class=menu><li id=dark-mode-toggle><svg class="icon icon-tabler icon-tabler-toggle-left" width="24" height="24" viewBox="0 0 24 24" stroke-width="2" stroke="currentcolor" fill="none" stroke-linecap="round" stroke-linejoin="round"><path stroke="none" d="M0 0h24v24H0z"/><circle cx="8" cy="12" r="2"/><rect x="2" y="6" width="20" height="12" rx="6"/></svg>
<svg class="icon icon-tabler icon-tabler-toggle-right" width="24" height="24" viewBox="0 0 24 24" stroke-width="2" stroke="currentcolor" fill="none" stroke-linecap="round" stroke-linejoin="round"><path stroke="none" d="M0 0h24v24H0z"/><circle cx="16" cy="12" r="2"/><rect x="2" y="6" width="20" height="12" rx="6"/></svg>
<span>Dark Mode</span></li></ol></li></ol></aside><aside class="sidebar right-sidebar sticky"><section class="widget archives"><div class=widget-icon><svg class="icon icon-tabler icon-tabler-hash" width="24" height="24" viewBox="0 0 24 24" stroke-width="2" stroke="currentcolor" fill="none" stroke-linecap="round" stroke-linejoin="round"><path stroke="none" d="M0 0h24v24H0z"/><line x1="5" y1="9" x2="19" y2="9"/><line x1="5" y1="15" x2="19" y2="15"/><line x1="11" y1="4" x2="7" y2="20"/><line x1="17" y1="4" x2="13" y2="20"/></svg></div><h2 class="widget-title section-title">Table of contents</h2><div class=widget--toc><nav id=TableOfContents><ol><li><a href=#learnable-parameters-and-hyperparameters>Learnable Parameters and Hyperparameters</a></li><li><a href=#supervised-unsupervised-and-reinforcement-learning-algorithms>Supervised, Unsupervised, and Reinforcement Learning Algorithms</a><ol><li><a href=#active-passive-and-inverse-reinforcement-learning>Active, Passive, and Inverse Reinforcement Learning</a></li></ol></li><li><a href=#generalization-overfitting-and-underfitting>Generalization, Overfitting, and Underfitting</a></li><li><a href=#scalability>Scalability</a></li><li><a href=#neural-networks>Neural Networks</a></li><li><a href=#convolutional-neural-network>Convolutional Neural Network</a></li><li><a href=#applications>Applications</a></li><li><a href=#reinforcement-learning>Reinforcement Learning</a></li><li><a href=#deep-q-learning>Deep Q-learning</a></li><li><a href=#deep-q-learning-example-using-flappy-bird>Deep Q-learning Example Using Flappy Bird</a></li><li><a href=#deep-reinforcement-learning-2d-3d-and-even-real-life>Deep Reinforcement Learning: 2D, 3D, and Even Real Life</a></li></ol></nav></div></section></aside><main class="main full-width"><article class=main-article><header class=article-header><div class=article-details><div class=article-title-wrapper><h2 class=article-title><a href=/learning-how-to-play-flappy-bird-a-guide-to-reinforcement-learning/>Learning How to Play Flappy Bird: A Guide to Reinforcement Learning</a></h2></div><footer class=article-time><div><svg class="icon icon-tabler icon-tabler-calendar-time" width="56" height="56" viewBox="0 0 24 24" stroke-width="2" stroke="currentcolor" fill="none" stroke-linecap="round" stroke-linejoin="round"><path stroke="none" d="M0 0h24v24H0z"/><path d="M11.795 21H5a2 2 0 01-2-2V7a2 2 0 012-2h12a2 2 0 012 2v4"/><circle cx="18" cy="18" r="4"/><path d="M15 3v4"/><path d="M7 3v4"/><path d="M3 11h16"/><path d="M18 16.496V18l1 1"/></svg>
<time class=article-time--published>Oct 01, 2022</time></div><div><svg class="icon icon-tabler icon-tabler-clock" width="24" height="24" viewBox="0 0 24 24" stroke-width="2" stroke="currentcolor" fill="none" stroke-linecap="round" stroke-linejoin="round"><path stroke="none" d="M0 0h24v24H0z"/><circle cx="12" cy="12" r="9"/><polyline points="12 7 12 12 15 15"/></svg>
<time class=article-time--reading>20 minute read</time></div></footer></div></header><section class=article-content><p>In the realm of traditional programming, programmers meticulously craft each software instruction, leaving no room for learning from data. Conversely, machine learning, a fascinating branch of computer science, empowers computers to learn and extract knowledge from data using statistical methods, without the need for explicit programming.</p><p>This reinforcement learning tutorial will demonstrate how to utilize PyTorch to train a reinforcement learning neural network to master the game Flappy Bird. Before delving into the implementation, let&rsquo;s establish a foundation by exploring some essential building blocks.</p><p>Machine learning algorithms can be broadly categorized into two groups: traditional learning algorithms and deep learning algorithms. Traditional learning algorithms, typically characterized by fewer learnable parameters, possess a limited learning capacity compared to their deep learning counterparts.</p><p>Furthermore, traditional learning algorithms lack the ability to perform <em>feature extraction</em>. This limitation necessitates artificial intelligence specialists to meticulously design a suitable data representation, which is then fed into the learning algorithm. Examples of traditional machine learning techniques include SVM, random forest, decision tree, and $k$-means. In contrast, the deep neural network takes center stage as the central algorithm in deep learning.</p><p>Deep neural networks excel in their ability to process raw data, such as images, directly, eliminating the need for artificial intelligence specialists to handcraft data representations. Instead, the neural network autonomously discovers the optimal representation during the training process.</p><p>While many deep learning techniques have existed for a considerable time, recent hardware advancements, particularly in the realm of GPUs spearheaded by Nvidia, have significantly accelerated deep learning research and development, enabling rapid experimentation.</p><h2 id=learnable-parameters-and-hyperparameters>Learnable Parameters and Hyperparameters</h2><p>Machine learning algorithms comprise two types of parameters: learnable parameters, which are fine-tuned during the training process, and non-learnable parameters, which are predetermined before training. These pre-set parameters are referred to as <em>hyperparameters</em>.</p><p><em>Grid search</em> stands as a widely employed method for determining the optimal hyperparameters. This brute-force technique involves exhaustively evaluating all possible hyperparameter combinations within a defined range and selecting the combination that maximizes a predefined metric.</p><h2 id=supervised-unsupervised-and-reinforcement-learning-algorithms>Supervised, Unsupervised, and Reinforcement Learning Algorithms</h2><p>One approach to classifying learning algorithms is to differentiate between supervised and unsupervised algorithms. However, it&rsquo;s important to note that reinforcement learning exists on a spectrum between these two categories.</p><p>In supervised learning, we deal with $ (x_i, y_i) $ pairs, where $ x_i $ represents the input to the algorithm and $ y_i $ represents the desired output. The objective is to identify a function that accurately maps $ x_i $ to $ y_i $.</p><p>To optimize the learnable parameters and establish a function that effectively maps $ x_i $ to $ y_i $, a loss function and an optimizer are required. The optimizer&rsquo;s role is to minimize the loss function. A commonly used loss function is the mean squared error (MSE):</p><p>[MSE = \sum_{i=1}^{n} (y_i - \widehat{y_i} )^2]</p><p>In this equation, $ y_i $ represents the ground truth label, while $ \widehat{y_i} $ denotes the predicted label. <em>Stochastic gradient descent</em> has gained significant popularity as an optimizer in deep learning. Numerous variations, such as Adam, Adadelta, and Adagrad, strive to enhance the performance of stochastic gradient descent.</p><p>Unsupervised algorithms, on the other hand, aim to uncover underlying structures within data without relying on explicit labels. One such algorithm is $k$-means, which seeks to identify optimal clusters within a dataset. To illustrate, consider an image comprising 300 data points. The $k$-means algorithm successfully identifies the inherent structure in the data and assigns a cluster label, represented by a distinct color, to each data point.</p><div data-testid=image-container class="X3Bg-O9c _1xIzGpI7"><figure class=_1sK7TguO><picture class=!contents data-testid=picture><source srcset="https://assets.toptal.io/images?url=https%3A%2F%2Fuploads.toptal.io%2Fblog%2Fimage%2F126765%2Ftoptal-blog-image-1533299134417-048def7b1a0c743b621872fda0b252a7.png&amp;width=360, https://assets.toptal.io/images?url=https%3A%2F%2Fuploads.toptal.io%2Fblog%2Fimage%2F126765%2Ftoptal-blog-image-1533299134417-048def7b1a0c743b621872fda0b252a7.png&amp;width=360&amp;dpr=2 2x" media="(max-width: 360px)"><source srcset="https://assets.toptal.io/images?url=https%3A%2F%2Fuploads.toptal.io%2Fblog%2Fimage%2F126765%2Ftoptal-blog-image-1533299134417-048def7b1a0c743b621872fda0b252a7.png&amp;width=480, https://assets.toptal.io/images?url=https%3A%2F%2Fuploads.toptal.io%2Fblog%2Fimage%2F126765%2Ftoptal-blog-image-1533299134417-048def7b1a0c743b621872fda0b252a7.png&amp;width=480&amp;dpr=2 2x" media="(min-width: 360.1px) and (max-width: 480px)"><source srcset="https://assets.toptal.io/images?url=https%3A%2F%2Fuploads.toptal.io%2Fblog%2Fimage%2F126765%2Ftoptal-blog-image-1533299134417-048def7b1a0c743b621872fda0b252a7.png&amp;width=768, https://assets.toptal.io/images?url=https%3A%2F%2Fuploads.toptal.io%2Fblog%2Fimage%2F126765%2Ftoptal-blog-image-1533299134417-048def7b1a0c743b621872fda0b252a7.png&amp;width=768&amp;dpr=2 2x" media="(min-width: 480.1px) and (max-width: 768px)"><source srcset="https://assets.toptal.io/images?url=https%3A%2F%2Fuploads.toptal.io%2Fblog%2Fimage%2F126765%2Ftoptal-blog-image-1533299134417-048def7b1a0c743b621872fda0b252a7.png&amp;width=1024, https://assets.toptal.io/images?url=https%3A%2F%2Fuploads.toptal.io%2Fblog%2Fimage%2F126765%2Ftoptal-blog-image-1533299134417-048def7b1a0c743b621872fda0b252a7.png&amp;width=1024&amp;dpr=2 2x" media="(min-width: 768.1px) and (max-width: 1024px)"><source srcset="https://assets.toptal.io/images?url=https%3A%2F%2Fuploads.toptal.io%2Fblog%2Fimage%2F126765%2Ftoptal-blog-image-1533299134417-048def7b1a0c743b621872fda0b252a7.png&amp;width=1440, https://assets.toptal.io/images?url=https%3A%2F%2Fuploads.toptal.io%2Fblog%2Fimage%2F126765%2Ftoptal-blog-image-1533299134417-048def7b1a0c743b621872fda0b252a7.png&amp;width=1440&amp;dpr=2 2x" media="(min-width: 1024.1px) and (max-width: 1440px)"><source srcset="https://assets.toptal.io/images?url=https%3A%2F%2Fuploads.toptal.io%2Fblog%2Fimage%2F126765%2Ftoptal-blog-image-1533299134417-048def7b1a0c743b621872fda0b252a7.png&amp;width=1920, https://assets.toptal.io/images?url=https%3A%2F%2Fuploads.toptal.io%2Fblog%2Fimage%2F126765%2Ftoptal-blog-image-1533299134417-048def7b1a0c743b621872fda0b252a7.png&amp;width=1920&amp;dpr=2 2x" media="(min-width: 1440.1px) and (max-width: 1920px)"><source srcset="https://assets.toptal.io/images?url=https%3A%2F%2Fuploads.toptal.io%2Fblog%2Fimage%2F126765%2Ftoptal-blog-image-1533299134417-048def7b1a0c743b621872fda0b252a7.png" media="(min-width: 1920.1px)"><img alt="Color-divided clusters of data points. The clusters were found by an unsupervised algorithm" src="https://assets.toptal.io/images?url=https%3A%2F%2Fuploads.toptal.io%2Fblog%2Fimage%2F126765%2Ftoptal-blog-image-1533299134417-048def7b1a0c743b621872fda0b252a7.png" loading=lazy decoding=async class=Djd3YO_2></picture></figure></div><p>Reinforcement learning distinguishes itself through the use of rewards - sparse, time-delayed labels. An agent interacts with its environment by taking actions, which in turn alter the environment&rsquo;s state. Based on these actions, the agent receives new observations and rewards. Observations represent the stimuli that the agent perceives from the environment, encompassing sights, sounds, smells, and more.</p><p>Rewards are presented to the agent upon taking actions, serving as indicators of the action&rsquo;s effectiveness. Through the process of perceiving observations and rewards, the agent progressively learns how to navigate and interact optimally within its environment. We will explore this concept in greater detail shortly.</p><h3 id=active-passive-and-inverse-reinforcement-learning>Active, Passive, and Inverse Reinforcement Learning</h3><p>Within the realm of reinforcement learning, several distinct approaches exist. In this tutorial, we focus on active reinforcement learning, where the agent actively interacts with the environment to learn. In contrast, passive reinforcement learning treats rewards merely as additional observations, with decisions being made based on a predetermined fixed policy.</p><p>Inverse reinforcement learning takes a different route by attempting to reconstruct the reward function given a history of actions and their corresponding rewards in various states.</p><h2 id=generalization-overfitting-and-underfitting>Generalization, Overfitting, and Underfitting</h2><p>In machine learning, a specific configuration of parameters and hyperparameters is referred to as a model. Machine learning experiments generally consist of two phases: training and testing.</p><p>During training, learnable parameters are adjusted using a training dataset. In the testing phase, these parameters are frozen, and the model&rsquo;s performance is evaluated based on its ability to make accurate predictions on unseen data. Generalization refers to the model&rsquo;s capacity to perform well on novel, previously unencountered examples or tasks after being trained on a specific dataset.</p><p>If a model is overly simplistic relative to the complexity of the data, it may struggle to fit the training data adequately, leading to poor performance on both the training and test datasets. This phenomenon is known as <em>underfitting</em>.</p><p>Conversely, when a model exhibits excellent performance on the training data but performs poorly on the test data, it is indicative of <em>overfitting</em>. Overfitting occurs when a model becomes overly tailored to the training data, failing to generalize well to unseen examples.</p><p>The following image illustrates the concepts of underfitting and overfitting in comparison to a balanced scenario where the prediction function aligns well with the overall data distribution.</p><div data-testid=image-container class="X3Bg-O9c _1xIzGpI7"><figure class=_1sK7TguO><picture class=!contents data-testid=picture><source srcset="https://assets.toptal.io/images?url=https%3A%2F%2Fuploads.toptal.io%2Fblog%2Fimage%2F126767%2Ftoptal-blog-image-1533299202728-8a23c7d09ca8c2f4b7935d93d3e5b61c.png&amp;width=360, https://assets.toptal.io/images?url=https%3A%2F%2Fuploads.toptal.io%2Fblog%2Fimage%2F126767%2Ftoptal-blog-image-1533299202728-8a23c7d09ca8c2f4b7935d93d3e5b61c.png&amp;width=360&amp;dpr=2 2x" media="(max-width: 360px)"><source srcset="https://assets.toptal.io/images?url=https%3A%2F%2Fuploads.toptal.io%2Fblog%2Fimage%2F126767%2Ftoptal-blog-image-1533299202728-8a23c7d09ca8c2f4b7935d93d3e5b61c.png&amp;width=480, https://assets.toptal.io/images?url=https%3A%2F%2Fuploads.toptal.io%2Fblog%2Fimage%2F126767%2Ftoptal-blog-image-1533299202728-8a23c7d09ca8c2f4b7935d93d3e5b61c.png&amp;width=480&amp;dpr=2 2x" media="(min-width: 360.1px) and (max-width: 480px)"><source srcset="https://assets.toptal.io/images?url=https%3A%2F%2Fuploads.toptal.io%2Fblog%2Fimage%2F126767%2Ftoptal-blog-image-1533299202728-8a23c7d09ca8c2f4b7935d93d3e5b61c.png&amp;width=768, https://assets.toptal.io/images?url=https%3A%2F%2Fuploads.toptal.io%2Fblog%2Fimage%2F126767%2Ftoptal-blog-image-1533299202728-8a23c7d09ca8c2f4b7935d93d3e5b61c.png&amp;width=768&amp;dpr=2 2x" media="(min-width: 480.1px) and (max-width: 768px)"><source srcset="https://assets.toptal.io/images?url=https%3A%2F%2Fuploads.toptal.io%2Fblog%2Fimage%2F126767%2Ftoptal-blog-image-1533299202728-8a23c7d09ca8c2f4b7935d93d3e5b61c.png&amp;width=1024, https://assets.toptal.io/images?url=https%3A%2F%2Fuploads.toptal.io%2Fblog%2Fimage%2F126767%2Ftoptal-blog-image-1533299202728-8a23c7d09ca8c2f4b7935d93d3e5b61c.png&amp;width=1024&amp;dpr=2 2x" media="(min-width: 768.1px) and (max-width: 1024px)"><source srcset="https://assets.toptal.io/images?url=https%3A%2F%2Fuploads.toptal.io%2Fblog%2Fimage%2F126767%2Ftoptal-blog-image-1533299202728-8a23c7d09ca8c2f4b7935d93d3e5b61c.png&amp;width=1440, https://assets.toptal.io/images?url=https%3A%2F%2Fuploads.toptal.io%2Fblog%2Fimage%2F126767%2Ftoptal-blog-image-1533299202728-8a23c7d09ca8c2f4b7935d93d3e5b61c.png&amp;width=1440&amp;dpr=2 2x" media="(min-width: 1024.1px) and (max-width: 1440px)"><source srcset="https://assets.toptal.io/images?url=https%3A%2F%2Fuploads.toptal.io%2Fblog%2Fimage%2F126767%2Ftoptal-blog-image-1533299202728-8a23c7d09ca8c2f4b7935d93d3e5b61c.png&amp;width=1920, https://assets.toptal.io/images?url=https%3A%2F%2Fuploads.toptal.io%2Fblog%2Fimage%2F126767%2Ftoptal-blog-image-1533299202728-8a23c7d09ca8c2f4b7935d93d3e5b61c.png&amp;width=1920&amp;dpr=2 2x" media="(min-width: 1440.1px) and (max-width: 1920px)"><source srcset="https://assets.toptal.io/images?url=https%3A%2F%2Fuploads.toptal.io%2Fblog%2Fimage%2F126767%2Ftoptal-blog-image-1533299202728-8a23c7d09ca8c2f4b7935d93d3e5b61c.png" media="(min-width: 1920.1px)"><img alt="Underfitting, balanced, and overfitting graphs. A balanced function follows the general trend of the data points well enough, without sticking too closely to individual data points" src="https://assets.toptal.io/images?url=https%3A%2F%2Fuploads.toptal.io%2Fblog%2Fimage%2F126767%2Ftoptal-blog-image-1533299202728-8a23c7d09ca8c2f4b7935d93d3e5b61c.png" loading=lazy decoding=async class=Djd3YO_2></picture></figure></div><h2 id=scalability>Scalability</h2><p>Data plays a pivotal role in the development of robust machine learning models. While traditional learning algorithms may not necessitate extensive datasets, their limited capacity often translates to restricted performance. The accompanying plot highlights the superior scalability of deep learning methods compared to their traditional counterparts.</p><div data-testid=image-container class="X3Bg-O9c _1xIzGpI7"><figure class=_1sK7TguO><picture class=!contents data-testid=picture><source srcset="https://assets.toptal.io/images?url=https%3A%2F%2Fuploads.toptal.io%2Fblog%2Fimage%2F126768%2Ftoptal-blog-image-1533299235752-881e36172d662fb20ea05b44cbee8156.png&amp;width=360, https://assets.toptal.io/images?url=https%3A%2F%2Fuploads.toptal.io%2Fblog%2Fimage%2F126768%2Ftoptal-blog-image-1533299235752-881e36172d662fb20ea05b44cbee8156.png&amp;width=360&amp;dpr=2 2x" media="(max-width: 360px)"><source srcset="https://assets.toptal.io/images?url=https%3A%2F%2Fuploads.toptal.io%2Fblog%2Fimage%2F126768%2Ftoptal-blog-image-1533299235752-881e36172d662fb20ea05b44cbee8156.png&amp;width=480, https://assets.toptal.io/images?url=https%3A%2F%2Fuploads.toptal.io%2Fblog%2Fimage%2F126768%2Ftoptal-blog-image-1533299235752-881e36172d662fb20ea05b44cbee8156.png&amp;width=480&amp;dpr=2 2x" media="(min-width: 360.1px) and (max-width: 480px)"><source srcset="https://assets.toptal.io/images?url=https%3A%2F%2Fuploads.toptal.io%2Fblog%2Fimage%2F126768%2Ftoptal-blog-image-1533299235752-881e36172d662fb20ea05b44cbee8156.png&amp;width=768, https://assets.toptal.io/images?url=https%3A%2F%2Fuploads.toptal.io%2Fblog%2Fimage%2F126768%2Ftoptal-blog-image-1533299235752-881e36172d662fb20ea05b44cbee8156.png&amp;width=768&amp;dpr=2 2x" media="(min-width: 480.1px) and (max-width: 768px)"><source srcset="https://assets.toptal.io/images?url=https%3A%2F%2Fuploads.toptal.io%2Fblog%2Fimage%2F126768%2Ftoptal-blog-image-1533299235752-881e36172d662fb20ea05b44cbee8156.png&amp;width=1024, https://assets.toptal.io/images?url=https%3A%2F%2Fuploads.toptal.io%2Fblog%2Fimage%2F126768%2Ftoptal-blog-image-1533299235752-881e36172d662fb20ea05b44cbee8156.png&amp;width=1024&amp;dpr=2 2x" media="(min-width: 768.1px) and (max-width: 1024px)"><source srcset="https://assets.toptal.io/images?url=https%3A%2F%2Fuploads.toptal.io%2Fblog%2Fimage%2F126768%2Ftoptal-blog-image-1533299235752-881e36172d662fb20ea05b44cbee8156.png&amp;width=1440, https://assets.toptal.io/images?url=https%3A%2F%2Fuploads.toptal.io%2Fblog%2Fimage%2F126768%2Ftoptal-blog-image-1533299235752-881e36172d662fb20ea05b44cbee8156.png&amp;width=1440&amp;dpr=2 2x" media="(min-width: 1024.1px) and (max-width: 1440px)"><source srcset="https://assets.toptal.io/images?url=https%3A%2F%2Fuploads.toptal.io%2Fblog%2Fimage%2F126768%2Ftoptal-blog-image-1533299235752-881e36172d662fb20ea05b44cbee8156.png&amp;width=1920, https://assets.toptal.io/images?url=https%3A%2F%2Fuploads.toptal.io%2Fblog%2Fimage%2F126768%2Ftoptal-blog-image-1533299235752-881e36172d662fb20ea05b44cbee8156.png&amp;width=1920&amp;dpr=2 2x" media="(min-width: 1440.1px) and (max-width: 1920px)"><source srcset="https://assets.toptal.io/images?url=https%3A%2F%2Fuploads.toptal.io%2Fblog%2Fimage%2F126768%2Ftoptal-blog-image-1533299235752-881e36172d662fb20ea05b44cbee8156.png" media="(min-width: 1920.1px)"><img alt="Performance vs. data quantity for deep learning and traditional algorithms.  Neural networks perform better at scale." src="https://assets.toptal.io/images?url=https%3A%2F%2Fuploads.toptal.io%2Fblog%2Fimage%2F126768%2Ftoptal-blog-image-1533299235752-881e36172d662fb20ea05b44cbee8156.png" loading=lazy decoding=async class=Djd3YO_2></picture></figure></div><h2 id=neural-networks>Neural Networks</h2><p>Neural networks are sophisticated structures composed of interconnected layers. A simplified representation of a four-layered neural network is depicted in the image below. The first layer acts as the input layer, responsible for receiving input data, while the last layer serves as the output layer, producing the final output. Sandwiched between the input and output layers are two hidden layers responsible for intermediate computations.</p><div data-testid=image-container class="X3Bg-O9c _1xIzGpI7"><figure class=_1sK7TguO><picture class=!contents data-testid=picture><source srcset="https://assets.toptal.io/images?url=https%3A%2F%2Fuploads.toptal.io%2Fblog%2Fimage%2F126769%2Ftoptal-blog-image-1533299263061-7d93664ad861e6a1c2ef82b359aecbb7.png&amp;width=360, https://assets.toptal.io/images?url=https%3A%2F%2Fuploads.toptal.io%2Fblog%2Fimage%2F126769%2Ftoptal-blog-image-1533299263061-7d93664ad861e6a1c2ef82b359aecbb7.png&amp;width=360&amp;dpr=2 2x" media="(max-width: 360px)"><source srcset="https://assets.toptal.io/images?url=https%3A%2F%2Fuploads.toptal.io%2Fblog%2Fimage%2F126769%2Ftoptal-blog-image-1533299263061-7d93664ad861e6a1c2ef82b359aecbb7.png&amp;width=480, https://assets.toptal.io/images?url=https%3A%2F%2Fuploads.toptal.io%2Fblog%2Fimage%2F126769%2Ftoptal-blog-image-1533299263061-7d93664ad861e6a1c2ef82b359aecbb7.png&amp;width=480&amp;dpr=2 2x" media="(min-width: 360.1px) and (max-width: 480px)"><source srcset="https://assets.toptal.io/images?url=https%3A%2F%2Fuploads.toptal.io%2Fblog%2Fimage%2F126769%2Ftoptal-blog-image-1533299263061-7d93664ad861e6a1c2ef82b359aecbb7.png&amp;width=768, https://assets.toptal.io/images?url=https%3A%2F%2Fuploads.toptal.io%2Fblog%2Fimage%2F126769%2Ftoptal-blog-image-1533299263061-7d93664ad861e6a1c2ef82b359aecbb7.png&amp;width=768&amp;dpr=2 2x" media="(min-width: 480.1px) and (max-width: 768px)"><source srcset="https://assets.toptal.io/images?url=https%3A%2F%2Fuploads.toptal.io%2Fblog%2Fimage%2F126769%2Ftoptal-blog-image-1533299263061-7d93664ad861e6a1c2ef82b359aecbb7.png&amp;width=1024, https://assets.toptal.io/images?url=https%3A%2F%2Fuploads.toptal.io%2Fblog%2Fimage%2F126769%2Ftoptal-blog-image-1533299263061-7d93664ad861e6a1c2ef82b359aecbb7.png&amp;width=1024&amp;dpr=2 2x" media="(min-width: 768.1px) and (max-width: 1024px)"><source srcset="https://assets.toptal.io/images?url=https%3A%2F%2Fuploads.toptal.io%2Fblog%2Fimage%2F126769%2Ftoptal-blog-image-1533299263061-7d93664ad861e6a1c2ef82b359aecbb7.png&amp;width=1440, https://assets.toptal.io/images?url=https%3A%2F%2Fuploads.toptal.io%2Fblog%2Fimage%2F126769%2Ftoptal-blog-image-1533299263061-7d93664ad861e6a1c2ef82b359aecbb7.png&amp;width=1440&amp;dpr=2 2x" media="(min-width: 1024.1px) and (max-width: 1440px)"><source srcset="https://assets.toptal.io/images?url=https%3A%2F%2Fuploads.toptal.io%2Fblog%2Fimage%2F126769%2Ftoptal-blog-image-1533299263061-7d93664ad861e6a1c2ef82b359aecbb7.png&amp;width=1920, https://assets.toptal.io/images?url=https%3A%2F%2Fuploads.toptal.io%2Fblog%2Fimage%2F126769%2Ftoptal-blog-image-1533299263061-7d93664ad861e6a1c2ef82b359aecbb7.png&amp;width=1920&amp;dpr=2 2x" media="(min-width: 1440.1px) and (max-width: 1920px)"><source srcset="https://assets.toptal.io/images?url=https%3A%2F%2Fuploads.toptal.io%2Fblog%2Fimage%2F126769%2Ftoptal-blog-image-1533299263061-7d93664ad861e6a1c2ef82b359aecbb7.png" media="(min-width: 1920.1px)"><img alt="Neural network graph, showing every node of an input layer mapped to every node of Hidden Layer 1, in turn mapped to every node of Hidden Layer 2, and then finally mapped to the output layer, which consists of a single node" src="https://assets.toptal.io/images?url=https%3A%2F%2Fuploads.toptal.io%2Fblog%2Fimage%2F126769%2Ftoptal-blog-image-1533299263061-7d93664ad861e6a1c2ef82b359aecbb7.png" loading=lazy decoding=async class=Djd3YO_2></picture></figure></div><p>Neural networks containing more than one hidden layer are categorized as deep neural networks. Given an input set $ X $, the neural network processes the data and generates an output $ y $. The learning process unfolds through the backpropagation algorithm, which leverages a loss function and an optimizer.</p><p>Backpropagation comprises two key steps: a forward pass and a backward pass. In the forward pass, input data is fed into the neural network, producing an output. The discrepancy between the ground truth and the prediction is quantified as a loss. Subsequently, during the backward pass, the neural network&rsquo;s parameters are adjusted based on the calculated loss to improve accuracy.</p><h2 id=convolutional-neural-network>Convolutional Neural Network</h2><p>A notable variation of the neural network architecture is the <em>convolutional neural network</em>, particularly well-suited for computer vision tasks.</p><p>The convolutional layer, from which the network derives its name, stands as the most crucial component of a convolutional neural network. This layer&rsquo;s parameters consist of learnable filters, often referred to as kernels. Convolutional layers apply convolution operations to the input, effectively reducing the number of learnable parameters. This reduction acts as a form of heuristics, simplifying the neural network&rsquo;s training process and enhancing its efficiency.</p><p>The following illustration demonstrates the operation of a single convolutional kernel within a convolutional layer. The kernel slides across the input image, performing element-wise multiplications and summations to produce a convolved feature map.</p><div data-testid=image-container class="X3Bg-O9c _1xIzGpI7"><figure class=_1sK7TguO><img alt="Animation highlighting a kernel and its operation as an image is processed, and the corresponding convolved feature output" src=https://uploads.toptal.io/blog/image/126770/toptal-blog-image-1533299291351-f3a4d423c8f77667cc817b59ebaaa11f.gif loading=lazy decoding=async class=Djd3YO_2></figure></div><p>ReLU (Rectified Linear Unit) layers play a critical role in introducing non-linearities into the neural network. Non-linearities are essential because they empower the network to model a wide range of functions, extending beyond linear relationships. The ReLU function is defined as follows:</p><p>[ReLU = \max(0, x)]</p><p>In essence, the ReLU function acts as a threshold, effectively suppressing negative values to zero.</p><p>[ReLU = \max(0, x)]</p><p>ReLU is one of many <em>activation functions</em> employed to incorporate non-linearities into neural networks. Other examples include sigmoid and hyperbolic tangent functions. ReLU&rsquo;s popularity stems from its ability to facilitate efficient neural network training compared to its alternatives.</p><p>The plot below visually represents the ReLU function.</p><div data-testid=image-container class="X3Bg-O9c _1xIzGpI7"><figure class=_1sK7TguO><picture class=!contents data-testid=picture><source srcset="https://assets.toptal.io/images?url=https%3A%2F%2Fuploads.toptal.io%2Fblog%2Fimage%2F126784%2Ftoptal-blog-image-1533545672186-41124fc389e89ff10c5b8551c17777cf.png&amp;width=360, https://assets.toptal.io/images?url=https%3A%2F%2Fuploads.toptal.io%2Fblog%2Fimage%2F126784%2Ftoptal-blog-image-1533545672186-41124fc389e89ff10c5b8551c17777cf.png&amp;width=360&amp;dpr=2 2x" media="(max-width: 360px)"><source srcset="https://assets.toptal.io/images?url=https%3A%2F%2Fuploads.toptal.io%2Fblog%2Fimage%2F126784%2Ftoptal-blog-image-1533545672186-41124fc389e89ff10c5b8551c17777cf.png&amp;width=480, https://assets.toptal.io/images?url=https%3A%2F%2Fuploads.toptal.io%2Fblog%2Fimage%2F126784%2Ftoptal-blog-image-1533545672186-41124fc389e89ff10c5b8551c17777cf.png&amp;width=480&amp;dpr=2 2x" media="(min-width: 360.1px) and (max-width: 480px)"><source srcset="https://assets.toptal.io/images?url=https%3A%2F%2Fuploads.toptal.io%2Fblog%2Fimage%2F126784%2Ftoptal-blog-image-1533545672186-41124fc389e89ff10c5b8551c17777cf.png&amp;width=768, https://assets.toptal.io/images?url=https%3A%2F%2Fuploads.toptal.io%2Fblog%2Fimage%2F126784%2Ftoptal-blog-image-1533545672186-41124fc389e89ff10c5b8551c17777cf.png&amp;width=768&amp;dpr=2 2x" media="(min-width: 480.1px) and (max-width: 768px)"><source srcset="https://assets.toptal.io/images?url=https%3A%2F%2Fuploads.toptal.io%2Fblog%2Fimage%2F126784%2Ftoptal-blog-image-1533545672186-41124fc389e89ff10c5b8551c17777cf.png&amp;width=1024, https://assets.toptal.io/images?url=https%3A%2F%2Fuploads.toptal.io%2Fblog%2Fimage%2F126784%2Ftoptal-blog-image-1533545672186-41124fc389e89ff10c5b8551c17777cf.png&amp;width=1024&amp;dpr=2 2x" media="(min-width: 768.1px) and (max-width: 1024px)"><source srcset="https://assets.toptal.io/images?url=https%3A%2F%2Fuploads.toptal.io%2Fblog%2Fimage%2F126784%2Ftoptal-blog-image-1533545672186-41124fc389e89ff10c5b8551c17777cf.png&amp;width=1440, https://assets.toptal.io/images?url=https%3A%2F%2Fuploads.toptal.io%2Fblog%2Fimage%2F126784%2Ftoptal-blog-image-1533545672186-41124fc389e89ff10c5b8551c17777cf.png&amp;width=1440&amp;dpr=2 2x" media="(min-width: 1024.1px) and (max-width: 1440px)"><source srcset="https://assets.toptal.io/images?url=https%3A%2F%2Fuploads.toptal.io%2Fblog%2Fimage%2F126784%2Ftoptal-blog-image-1533545672186-41124fc389e89ff10c5b8551c17777cf.png&amp;width=1920, https://assets.toptal.io/images?url=https%3A%2F%2Fuploads.toptal.io%2Fblog%2Fimage%2F126784%2Ftoptal-blog-image-1533545672186-41124fc389e89ff10c5b8551c17777cf.png&amp;width=1920&amp;dpr=2 2x" media="(min-width: 1440.1px) and (max-width: 1920px)"><source srcset="https://assets.toptal.io/images?url=https%3A%2F%2Fuploads.toptal.io%2Fblog%2Fimage%2F126784%2Ftoptal-blog-image-1533545672186-41124fc389e89ff10c5b8551c17777cf.png" media="(min-width: 1920.1px)"><img alt="A ReLU function, similar to the simple diagonal of an y=x graph, but with all negative x values mapped to zero" src="https://assets.toptal.io/images?url=https%3A%2F%2Fuploads.toptal.io%2Fblog%2Fimage%2F126784%2Ftoptal-blog-image-1533545672186-41124fc389e89ff10c5b8551c17777cf.png" loading=lazy decoding=async class=Djd3YO_2></picture></figure></div><p>As the plot illustrates, the ReLU function effectively rectifies negative values to zero. This <a class=link href=https://www.quora.com/What-is-the-role-of-rectified-linear-ReLU-activation-function-in-CNN target=_blank rel=noopener>helps prevent the vanishing gradient problem</a>. If a gradient vanishes during training, it will have a negligible impact on adjusting the neural network&rsquo;s weights.</p><p>Convolutional neural networks typically comprise a combination of convolutional layers, ReLU layers, and fully connected layers. Fully connected layers establish connections between every neuron in one layer and every neuron in the subsequent layer, as observed in the two hidden layers depicted in the initial neural network image. The final fully connected layer maps the outputs from the preceding layer to a desired number of outputs, which in this case corresponds to the <code>number_of_actions</code>.</p><h2 id=applications>Applications</h2><p>Deep learning has demonstrated remarkable success and consistently outperforms classical machine learning algorithms across various subfields, including computer vision, speech recognition, and reinforcement learning. These deep learning advancements have found practical applications in diverse real-world domains, spanning finance, medicine, entertainment, and more.</p><h2 id=reinforcement-learning>Reinforcement Learning</h2><p>At the heart of reinforcement learning lies the concept of an <em>agent</em>. An agent interacts with its environment by taking actions, receiving observations, and obtaining rewards in return. The primary objective of training an agent is to maximize its cumulative reward.</p><p>Unlike classical machine learning algorithms, where machine learning engineers bear the responsibility of feature extraction, deep learning empowers the creation of end-to-end systems. These systems can process high-dimensional inputs, such as video streams, and autonomously learn optimal strategies for agents to make informed decisions.</p><p>In 2013, DeepMind, an AI startup based in London, achieved a significant breakthrough in training agents directly from high-dimensional sensory inputs. Their seminal paper, <em>Playing Atari with Deep Reinforcement Learning</em>, showcased their success in teaching an artificial neural network to master Atari games by solely observing the screen. Following their acquisition by Google, DeepMind further refined their approach and published an updated paper in <em>Nature</em>: <a class=link href=https://storage.googleapis.com/deepmind-media/dqn/DQNNaturePaper.pdf target=_blank rel=noopener><em>Human-level control through deep reinforcement learning</em></a>.</p><p>Reinforcement learning stands apart from other machine learning paradigms by eschewing the need for a direct supervisor. Instead, it relies solely on a reward signal to guide the agent&rsquo;s learning process. Feedback is inherently delayed, unlike the instantaneous feedback mechanisms employed in supervised learning algorithms. Additionally, data in reinforcement learning is sequential, with the agent&rsquo;s actions influencing the subsequent data it receives.</p><p>To formalize the agent&rsquo;s interaction with the environment, we employ the framework of a Markov decision process, which <a class=link href=https://www.davidsilver.uk/wp-content/uploads/2020/03/MDP.pdf target=_blank rel=noopener>a formal way of modeling this reinforcement learning environment</a>. This mathematical framework comprises a set of states, a set of available actions, and rules (e.g., probabilities) that govern transitions between states.</p><div data-testid=image-container class="X3Bg-O9c _1xIzGpI7"><figure class=_1sK7TguO><picture class=!contents data-testid=picture><source srcset="https://assets.toptal.io/images?url=https%3A%2F%2Fuploads.toptal.io%2Fblog%2Fimage%2F126773%2Ftoptal-blog-image-1533299447769-1194fed7cab736f336ac2bec747e85b7.png&amp;width=360, https://assets.toptal.io/images?url=https%3A%2F%2Fuploads.toptal.io%2Fblog%2Fimage%2F126773%2Ftoptal-blog-image-1533299447769-1194fed7cab736f336ac2bec747e85b7.png&amp;width=360&amp;dpr=2 2x" media="(max-width: 360px)"><source srcset="https://assets.toptal.io/images?url=https%3A%2F%2Fuploads.toptal.io%2Fblog%2Fimage%2F126773%2Ftoptal-blog-image-1533299447769-1194fed7cab736f336ac2bec747e85b7.png&amp;width=480, https://assets.toptal.io/images?url=https%3A%2F%2Fuploads.toptal.io%2Fblog%2Fimage%2F126773%2Ftoptal-blog-image-1533299447769-1194fed7cab736f336ac2bec747e85b7.png&amp;width=480&amp;dpr=2 2x" media="(min-width: 360.1px) and (max-width: 480px)"><source srcset="https://assets.toptal.io/images?url=https%3A%2F%2Fuploads.toptal.io%2Fblog%2Fimage%2F126773%2Ftoptal-blog-image-1533299447769-1194fed7cab736f336ac2bec747e85b7.png&amp;width=768, https://assets.toptal.io/images?url=https%3A%2F%2Fuploads.toptal.io%2Fblog%2Fimage%2F126773%2Ftoptal-blog-image-1533299447769-1194fed7cab736f336ac2bec747e85b7.png&amp;width=768&amp;dpr=2 2x" media="(min-width: 480.1px) and (max-width: 768px)"><source srcset="https://assets.toptal.io/images?url=https%3A%2F%2Fuploads.toptal.io%2Fblog%2Fimage%2F126773%2Ftoptal-blog-image-1533299447769-1194fed7cab736f336ac2bec747e85b7.png&amp;width=1024, https://assets.toptal.io/images?url=https%3A%2F%2Fuploads.toptal.io%2Fblog%2Fimage%2F126773%2Ftoptal-blog-image-1533299447769-1194fed7cab736f336ac2bec747e85b7.png&amp;width=1024&amp;dpr=2 2x" media="(min-width: 768.1px) and (max-width: 1024px)"><source srcset="https://assets.toptal.io/images?url=https%3A%2F%2Fuploads.toptal.io%2Fblog%2Fimage%2F126773%2Ftoptal-blog-image-1533299447769-1194fed7cab736f336ac2bec747e85b7.png&amp;width=1440, https://assets.toptal.io/images?url=https%3A%2F%2Fuploads.toptal.io%2Fblog%2Fimage%2F126773%2Ftoptal-blog-image-1533299447769-1194fed7cab736f336ac2bec747e85b7.png&amp;width=1440&amp;dpr=2 2x" media="(min-width: 1024.1px) and (max-width: 1440px)"><source srcset="https://assets.toptal.io/images?url=https%3A%2F%2Fuploads.toptal.io%2Fblog%2Fimage%2F126773%2Ftoptal-blog-image-1533299447769-1194fed7cab736f336ac2bec747e85b7.png&amp;width=1920, https://assets.toptal.io/images?url=https%3A%2F%2Fuploads.toptal.io%2Fblog%2Fimage%2F126773%2Ftoptal-blog-image-1533299447769-1194fed7cab736f336ac2bec747e85b7.png&amp;width=1920&amp;dpr=2 2x" media="(min-width: 1440.1px) and (max-width: 1920px)"><source srcset="https://assets.toptal.io/images?url=https%3A%2F%2Fuploads.toptal.io%2Fblog%2Fimage%2F126773%2Ftoptal-blog-image-1533299447769-1194fed7cab736f336ac2bec747e85b7.png" media="(min-width: 1920.1px)"><img alt="A Markov decision process graph: States (marked 'S') take actions (marked 'a') which then has various probabilities for which state the agent ends up in next; some followed paths also indicate a reward" src="https://assets.toptal.io/images?url=https%3A%2F%2Fuploads.toptal.io%2Fblog%2Fimage%2F126773%2Ftoptal-blog-image-1533299447769-1194fed7cab736f336ac2bec747e85b7.png" loading=lazy decoding=async class=Djd3YO_2></picture></figure></div><p>The agent possesses the ability to execute actions, thereby altering the environment&rsquo;s state. The outcome of these actions is reflected in the reward, denoted as $ R_t $, a scalar feedback signal indicating the agent&rsquo;s performance at a given time step $t$.</p><div data-testid=image-container class="X3Bg-O9c _1xIzGpI7"><figure class=_1sK7TguO><picture class=!contents data-testid=picture><source srcset="https://assets.toptal.io/images?url=https%3A%2F%2Fuploads.toptal.io%2Fblog%2Fimage%2F126772%2Ftoptal-blog-image-1533299409244-e2e9ad27f83938dbe88f5a036ae83f59.png&amp;width=360, https://assets.toptal.io/images?url=https%3A%2F%2Fuploads.toptal.io%2Fblog%2Fimage%2F126772%2Ftoptal-blog-image-1533299409244-e2e9ad27f83938dbe88f5a036ae83f59.png&amp;width=360&amp;dpr=2 2x" media="(max-width: 360px)"><source srcset="https://assets.toptal.io/images?url=https%3A%2F%2Fuploads.toptal.io%2Fblog%2Fimage%2F126772%2Ftoptal-blog-image-1533299409244-e2e9ad27f83938dbe88f5a036ae83f59.png&amp;width=480, https://assets.toptal.io/images?url=https%3A%2F%2Fuploads.toptal.io%2Fblog%2Fimage%2F126772%2Ftoptal-blog-image-1533299409244-e2e9ad27f83938dbe88f5a036ae83f59.png&amp;width=480&amp;dpr=2 2x" media="(min-width: 360.1px) and (max-width: 480px)"><source srcset="https://assets.toptal.io/images?url=https%3A%2F%2Fuploads.toptal.io%2Fblog%2Fimage%2F126772%2Ftoptal-blog-image-1533299409244-e2e9ad27f83938dbe88f5a036ae83f59.png&amp;width=768, https://assets.toptal.io/images?url=https%3A%2F%2Fuploads.toptal.io%2Fblog%2Fimage%2F126772%2Ftoptal-blog-image-1533299409244-e2e9ad27f83938dbe88f5a036ae83f59.png&amp;width=768&amp;dpr=2 2x" media="(min-width: 480.1px) and (max-width: 768px)"><source srcset="https://assets.toptal.io/images?url=https%3A%2F%2Fuploads.toptal.io%2Fblog%2Fimage%2F126772%2Ftoptal-blog-image-1533299409244-e2e9ad27f83938dbe88f5a036ae83f59.png&amp;width=1024, https://assets.toptal.io/images?url=https%3A%2F%2Fuploads.toptal.io%2Fblog%2Fimage%2F126772%2Ftoptal-blog-image-1533299409244-e2e9ad27f83938dbe88f5a036ae83f59.png&amp;width=1024&amp;dpr=2 2x" media="(min-width: 768.1px) and (max-width: 1024px)"><source srcset="https://assets.toptal.io/images?url=https%3A%2F%2Fuploads.toptal.io%2Fblog%2Fimage%2F126772%2Ftoptal-blog-image-1533299409244-e2e9ad27f83938dbe88f5a036ae83f59.png&amp;width=1440, https://assets.toptal.io/images?url=https%3A%2F%2Fuploads.toptal.io%2Fblog%2Fimage%2F126772%2Ftoptal-blog-image-1533299409244-e2e9ad27f83938dbe88f5a036ae83f59.png&amp;width=1440&amp;dpr=2 2x" media="(min-width: 1024.1px) and (max-width: 1440px)"><source srcset="https://assets.toptal.io/images?url=https%3A%2F%2Fuploads.toptal.io%2Fblog%2Fimage%2F126772%2Ftoptal-blog-image-1533299409244-e2e9ad27f83938dbe88f5a036ae83f59.png&amp;width=1920, https://assets.toptal.io/images?url=https%3A%2F%2Fuploads.toptal.io%2Fblog%2Fimage%2F126772%2Ftoptal-blog-image-1533299409244-e2e9ad27f83938dbe88f5a036ae83f59.png&amp;width=1920&amp;dpr=2 2x" media="(min-width: 1440.1px) and (max-width: 1920px)"><source srcset="https://assets.toptal.io/images?url=https%3A%2F%2Fuploads.toptal.io%2Fblog%2Fimage%2F126772%2Ftoptal-blog-image-1533299409244-e2e9ad27f83938dbe88f5a036ae83f59.png" media="(min-width: 1920.1px)"><img alt="A neural network agent decides which action to take at any step based on observations and rewards." src="https://assets.toptal.io/images?url=https%3A%2F%2Fuploads.toptal.io%2Fblog%2Fimage%2F126772%2Ftoptal-blog-image-1533299409244-e2e9ad27f83938dbe88f5a036ae83f59.png" loading=lazy decoding=async class=Djd3YO_2></picture></figure></div><p>Optimizing for long-term performance necessitates considering not only immediate rewards but also future rewards. The total reward for a single episode starting from time step $t$ is given by $ R_t = r_t + r_{t+1} + r_{t+2} + \ldots + r_n $. However, due to the inherent uncertainty of future predictions, a discounted future reward is employed: $ R_t = r_t +\gamma r_{t+1} + \gamma^2r_{t+2} + \ldots + \gamma^{n-t}r_n = r_t + \gamma R_{t+1} $. Here, $ \gamma $ represents a discount factor that diminishes the significance of rewards as they extend further into the future. The agent&rsquo;s goal is to select actions that maximize this discounted future reward.</p><h2 id=deep-q-learning>Deep Q-learning</h2><p>The $ Q(s, a) $ function, central to deep Q-learning, quantifies the maximum discounted future reward attainable when action $ a $ is executed in state $ s $:</p><p>[Q(s_t, a_t) = \max R_{t+1}]</p><p>The Bellman equation provides a recursive relationship for estimating future rewards: $ Q(s, a) = r + \gamma \max_{a’}Q(s’, a’) $. This equation states that the maximum future reward, given a state $ s $ and action $ a $, is equivalent to the immediate reward plus the maximum future reward attainable from the subsequent state $ s&rsquo; $ after taking the optimal action $ a&rsquo; $.</p><p>Directly approximating Q-values using non-linear functions, such as neural networks, can introduce instability into the learning process. To mitigate this, experience replay is employed. Experience replay involves storing the agent&rsquo;s experiences (state transitions, actions, rewards) during training episodes in a replay memory. Instead of utilizing only the most recent transition, random mini-batches are sampled from the replay memory. This sampling technique disrupts the correlation between consecutive training samples, preventing the neural network from converging to suboptimal local minima.</p><p>Two crucial aspects of deep Q-learning warrant attention: exploration and exploitation. Exploitation entails making the best decision based on the currently available information, while exploration focuses on gathering more information to potentially discover better actions.</p><p>When the algorithm chooses actions based on the neural network&rsquo;s recommendations, it is said to be exploiting the network&rsquo;s learned knowledge. Conversely, when the algorithm opts for random actions, it is exploring new possibilities, potentially enriching the neural network&rsquo;s knowledge base.</p><p>The following excerpt from DeepMind&rsquo;s paper <a class=link href=https://www.cs.toronto.edu/~vmnih/docs/dqn.pdf target=_blank rel=noopener>Playing Atari with Deep Reinforcement Learning</a> outlines the &ldquo;Deep Q-learning algorithm with Experience Replay&rdquo;:</p><div data-testid=image-container class="X3Bg-O9c _1xIzGpI7"><figure class=_1sK7TguO><picture class=!contents data-testid=picture><source srcset="https://assets.toptal.io/images?url=https%3A%2F%2Fuploads.toptal.io%2Fblog%2Fimage%2F126774%2Ftoptal-blog-image-1533299476567-a87ee2d2c9861f1eba3f206fad0419f0.png&amp;width=360, https://assets.toptal.io/images?url=https%3A%2F%2Fuploads.toptal.io%2Fblog%2Fimage%2F126774%2Ftoptal-blog-image-1533299476567-a87ee2d2c9861f1eba3f206fad0419f0.png&amp;width=360&amp;dpr=2 2x" media="(max-width: 360px)"><source srcset="https://assets.toptal.io/images?url=https%3A%2F%2Fuploads.toptal.io%2Fblog%2Fimage%2F126774%2Ftoptal-blog-image-1533299476567-a87ee2d2c9861f1eba3f206fad0419f0.png&amp;width=480, https://assets.toptal.io/images?url=https%3A%2F%2Fuploads.toptal.io%2Fblog%2Fimage%2F126774%2Ftoptal-blog-image-1533299476567-a87ee2d2c9861f1eba3f206fad0419f0.png&amp;width=480&amp;dpr=2 2x" media="(min-width: 360.1px) and (max-width: 480px)"><source srcset="https://assets.toptal.io/images?url=https%3A%2F%2Fuploads.toptal.io%2Fblog%2Fimage%2F126774%2Ftoptal-blog-image-1533299476567-a87ee2d2c9861f1eba3f206fad0419f0.png&amp;width=768, https://assets.toptal.io/images?url=https%3A%2F%2Fuploads.toptal.io%2Fblog%2Fimage%2F126774%2Ftoptal-blog-image-1533299476567-a87ee2d2c9861f1eba3f206fad0419f0.png&amp;width=768&amp;dpr=2 2x" media="(min-width: 480.1px) and (max-width: 768px)"><source srcset="https://assets.toptal.io/images?url=https%3A%2F%2Fuploads.toptal.io%2Fblog%2Fimage%2F126774%2Ftoptal-blog-image-1533299476567-a87ee2d2c9861f1eba3f206fad0419f0.png&amp;width=1024, https://assets.toptal.io/images?url=https%3A%2F%2Fuploads.toptal.io%2Fblog%2Fimage%2F126774%2Ftoptal-blog-image-1533299476567-a87ee2d2c9861f1eba3f206fad0419f0.png&amp;width=1024&amp;dpr=2 2x" media="(min-width: 768.1px) and (max-width: 1024px)"><source srcset="https://assets.toptal.io/images?url=https%3A%2F%2Fuploads.toptal.io%2Fblog%2Fimage%2F126774%2Ftoptal-blog-image-1533299476567-a87ee2d2c9861f1eba3f206fad0419f0.png&amp;width=1440, https://assets.toptal.io/images?url=https%3A%2F%2Fuploads.toptal.io%2Fblog%2Fimage%2F126774%2Ftoptal-blog-image-1533299476567-a87ee2d2c9861f1eba3f206fad0419f0.png&amp;width=1440&amp;dpr=2 2x" media="(min-width: 1024.1px) and (max-width: 1440px)"><source srcset="https://assets.toptal.io/images?url=https%3A%2F%2Fuploads.toptal.io%2Fblog%2Fimage%2F126774%2Ftoptal-blog-image-1533299476567-a87ee2d2c9861f1eba3f206fad0419f0.png&amp;width=1920, https://assets.toptal.io/images?url=https%3A%2F%2Fuploads.toptal.io%2Fblog%2Fimage%2F126774%2Ftoptal-blog-image-1533299476567-a87ee2d2c9861f1eba3f206fad0419f0.png&amp;width=1920&amp;dpr=2 2x" media="(min-width: 1440.1px) and (max-width: 1920px)"><source srcset="https://assets.toptal.io/images?url=https%3A%2F%2Fuploads.toptal.io%2Fblog%2Fimage%2F126774%2Ftoptal-blog-image-1533299476567-a87ee2d2c9861f1eba3f206fad0419f0.png" media="(min-width: 1920.1px)"><img alt="The Deep Q-learning with Experience Replay algorithm in pseudocode" src="https://assets.toptal.io/images?url=https%3A%2F%2Fuploads.toptal.io%2Fblog%2Fimage%2F126774%2Ftoptal-blog-image-1533299476567-a87ee2d2c9861f1eba3f206fad0419f0.png" loading=lazy decoding=async class=Djd3YO_2></picture></figure></div><p>DeepMind coined the term Deep Q-networks (DQN) to refer to convolutional networks trained using their proposed deep Q-learning approach.</p><h2 id=deep-q-learning-example-using-flappy-bird>Deep Q-learning Example Using Flappy Bird</h2><p>Flappy Bird, a mobile game that took the world by storm, was the brainchild of Vietnamese video game artist and programmer Dong Nguyen. In this deceptively simple yet challenging game, the player assumes control of a bird, navigating it through a series of green pipes without colliding with them.</p><p>A screenshot from <a class=link href=https://github.com/sourabhv/FlapPyBird target=_blank rel=noopener>a Flappy Bird clone coded using PyGame</a> is provided below for reference:</p><div data-testid=image-container class="X3Bg-O9c _1xIzGpI7"><figure class=_1sK7TguO><picture class=!contents data-testid=picture><source srcset="https://assets.toptal.io/images?url=https%3A%2F%2Fuploads.toptal.io%2Fblog%2Fimage%2F126775%2Ftoptal-blog-image-1533299510232-fbec7612e4f7ddf670b6cae8de2ceb82.png&amp;width=360, https://assets.toptal.io/images?url=https%3A%2F%2Fuploads.toptal.io%2Fblog%2Fimage%2F126775%2Ftoptal-blog-image-1533299510232-fbec7612e4f7ddf670b6cae8de2ceb82.png&amp;width=360&amp;dpr=2 2x" media="(max-width: 360px)"><source srcset="https://assets.toptal.io/images?url=https%3A%2F%2Fuploads.toptal.io%2Fblog%2Fimage%2F126775%2Ftoptal-blog-image-1533299510232-fbec7612e4f7ddf670b6cae8de2ceb82.png&amp;width=480, https://assets.toptal.io/images?url=https%3A%2F%2Fuploads.toptal.io%2Fblog%2Fimage%2F126775%2Ftoptal-blog-image-1533299510232-fbec7612e4f7ddf670b6cae8de2ceb82.png&amp;width=480&amp;dpr=2 2x" media="(min-width: 360.1px) and (max-width: 480px)"><source srcset="https://assets.toptal.io/images?url=https%3A%2F%2Fuploads.toptal.io%2Fblog%2Fimage%2F126775%2Ftoptal-blog-image-1533299510232-fbec7612e4f7ddf670b6cae8de2ceb82.png&amp;width=768, https://assets.toptal.io/images?url=https%3A%2F%2Fuploads.toptal.io%2Fblog%2Fimage%2F126775%2Ftoptal-blog-image-1533299510232-fbec7612e4f7ddf670b6cae8de2ceb82.png&amp;width=768&amp;dpr=2 2x" media="(min-width: 480.1px) and (max-width: 768px)"><source srcset="https://assets.toptal.io/images?url=https%3A%2F%2Fuploads.toptal.io%2Fblog%2Fimage%2F126775%2Ftoptal-blog-image-1533299510232-fbec7612e4f7ddf670b6cae8de2ceb82.png&amp;width=1024, https://assets.toptal.io/images?url=https%3A%2F%2Fuploads.toptal.io%2Fblog%2Fimage%2F126775%2Ftoptal-blog-image-1533299510232-fbec7612e4f7ddf670b6cae8de2ceb82.png&amp;width=1024&amp;dpr=2 2x" media="(min-width: 768.1px) and (max-width: 1024px)"><source srcset="https://assets.toptal.io/images?url=https%3A%2F%2Fuploads.toptal.io%2Fblog%2Fimage%2F126775%2Ftoptal-blog-image-1533299510232-fbec7612e4f7ddf670b6cae8de2ceb82.png&amp;width=1440, https://assets.toptal.io/images?url=https%3A%2F%2Fuploads.toptal.io%2Fblog%2Fimage%2F126775%2Ftoptal-blog-image-1533299510232-fbec7612e4f7ddf670b6cae8de2ceb82.png&amp;width=1440&amp;dpr=2 2x" media="(min-width: 1024.1px) and (max-width: 1440px)"><source srcset="https://assets.toptal.io/images?url=https%3A%2F%2Fuploads.toptal.io%2Fblog%2Fimage%2F126775%2Ftoptal-blog-image-1533299510232-fbec7612e4f7ddf670b6cae8de2ceb82.png&amp;width=1920, https://assets.toptal.io/images?url=https%3A%2F%2Fuploads.toptal.io%2Fblog%2Fimage%2F126775%2Ftoptal-blog-image-1533299510232-fbec7612e4f7ddf670b6cae8de2ceb82.png&amp;width=1920&amp;dpr=2 2x" media="(min-width: 1440.1px) and (max-width: 1920px)"><source srcset="https://assets.toptal.io/images?url=https%3A%2F%2Fuploads.toptal.io%2Fblog%2Fimage%2F126775%2Ftoptal-blog-image-1533299510232-fbec7612e4f7ddf670b6cae8de2ceb82.png" media="(min-width: 1920.1px)"><img alt="A screenshot from FlapPyBird, a Flappy Bird clone coded using PyGame" src="https://assets.toptal.io/images?url=https%3A%2F%2Fuploads.toptal.io%2Fblog%2Fimage%2F126775%2Ftoptal-blog-image-1533299510232-fbec7612e4f7ddf670b6cae8de2ceb82.png" loading=lazy decoding=async class=Djd3YO_2></picture></figure></div><p>Since its inception, the original Flappy Bird game has been cloned, modified, and adapted for various purposes. One such modification, which we&rsquo;ll be using in this tutorial, strips away the background, sounds, and different bird and pipe styles, focusing solely on the core game mechanics. This modified game engine is sourced from <a class=link href=https://github.com/yenchenlin/DeepLearningFlappyBird target=_blank rel=noopener>this TensorFlow project</a>:</p><div data-testid=image-container class="X3Bg-O9c _1xIzGpI7"><figure class=_1sK7TguO><picture class=!contents data-testid=picture><source srcset="https://assets.toptal.io/images?url=https%3A%2F%2Fuploads.toptal.io%2Fblog%2Fimage%2F126776%2Ftoptal-blog-image-1533299569307-8c50700d44cd999c8bfb081350244c5a.png&amp;width=360, https://assets.toptal.io/images?url=https%3A%2F%2Fuploads.toptal.io%2Fblog%2Fimage%2F126776%2Ftoptal-blog-image-1533299569307-8c50700d44cd999c8bfb081350244c5a.png&amp;width=360&amp;dpr=2 2x" media="(max-width: 360px)"><source srcset="https://assets.toptal.io/images?url=https%3A%2F%2Fuploads.toptal.io%2Fblog%2Fimage%2F126776%2Ftoptal-blog-image-1533299569307-8c50700d44cd999c8bfb081350244c5a.png&amp;width=480, https://assets.toptal.io/images?url=https%3A%2F%2Fuploads.toptal.io%2Fblog%2Fimage%2F126776%2Ftoptal-blog-image-1533299569307-8c50700d44cd999c8bfb081350244c5a.png&amp;width=480&amp;dpr=2 2x" media="(min-width: 360.1px) and (max-width: 480px)"><source srcset="https://assets.toptal.io/images?url=https%3A%2F%2Fuploads.toptal.io%2Fblog%2Fimage%2F126776%2Ftoptal-blog-image-1533299569307-8c50700d44cd999c8bfb081350244c5a.png&amp;width=768, https://assets.toptal.io/images?url=https%3A%2F%2Fuploads.toptal.io%2Fblog%2Fimage%2F126776%2Ftoptal-blog-image-1533299569307-8c50700d44cd999c8bfb081350244c5a.png&amp;width=768&amp;dpr=2 2x" media="(min-width: 480.1px) and (max-width: 768px)"><source srcset="https://assets.toptal.io/images?url=https%3A%2F%2Fuploads.toptal.io%2Fblog%2Fimage%2F126776%2Ftoptal-blog-image-1533299569307-8c50700d44cd999c8bfb081350244c5a.png&amp;width=1024, https://assets.toptal.io/images?url=https%3A%2F%2Fuploads.toptal.io%2Fblog%2Fimage%2F126776%2Ftoptal-blog-image-1533299569307-8c50700d44cd999c8bfb081350244c5a.png&amp;width=1024&amp;dpr=2 2x" media="(min-width: 768.1px) and (max-width: 1024px)"><source srcset="https://assets.toptal.io/images?url=https%3A%2F%2Fuploads.toptal.io%2Fblog%2Fimage%2F126776%2Ftoptal-blog-image-1533299569307-8c50700d44cd999c8bfb081350244c5a.png&amp;width=1440, https://assets.toptal.io/images?url=https%3A%2F%2Fuploads.toptal.io%2Fblog%2Fimage%2F126776%2Ftoptal-blog-image-1533299569307-8c50700d44cd999c8bfb081350244c5a.png&amp;width=1440&amp;dpr=2 2x" media="(min-width: 1024.1px) and (max-width: 1440px)"><source srcset="https://assets.toptal.io/images?url=https%3A%2F%2Fuploads.toptal.io%2Fblog%2Fimage%2F126776%2Ftoptal-blog-image-1533299569307-8c50700d44cd999c8bfb081350244c5a.png&amp;width=1920, https://assets.toptal.io/images?url=https%3A%2F%2Fuploads.toptal.io%2Fblog%2Fimage%2F126776%2Ftoptal-blog-image-1533299569307-8c50700d44cd999c8bfb081350244c5a.png&amp;width=1920&amp;dpr=2 2x" media="(min-width: 1440.1px) and (max-width: 1920px)"><source srcset="https://assets.toptal.io/images?url=https%3A%2F%2Fuploads.toptal.io%2Fblog%2Fimage%2F126776%2Ftoptal-blog-image-1533299569307-8c50700d44cd999c8bfb081350244c5a.png" media="(min-width: 1920.1px)"><img alt="A screenshot of DeepLearningFlappyBird, a fork of the clone with simplified graphics" src="https://assets.toptal.io/images?url=https%3A%2F%2Fuploads.toptal.io%2Fblog%2Fimage%2F126776%2Ftoptal-blog-image-1533299569307-8c50700d44cd999c8bfb081350244c5a.png" loading=lazy decoding=async class=Djd3YO_2></picture></figure></div><p>However, instead of relying on TensorFlow, the deep reinforcement learning framework in this tutorial is built using PyTorch. PyTorch, known for its speed and flexibility, provides a powerful platform for deep learning experimentation, offering tensors, dynamic neural networks in Python, and robust GPU acceleration.</p><p>The neural network architecture employed in this tutorial mirrors the one used by DeepMind in their paper <em>Human-level control through deep reinforcement learning</em>.</p><div class=responsive-table><div class=table-wrapper><table><tbody><tr><th>Layer</th><th>Input</th><th>Filter size</th><th>Stride</th><th>Number of filters</th><th>Activation</th><th>Output</th></tr><tr><td>conv1</td><td>84x84x4</td><td>8x8</td><td>4</td><td>32</td><td>ReLU</td><td>20x20x32</td></tr><tr><td>conv2</td><td>20x20x32</td><td>4x4</td><td>2</td><td>64</td><td>ReLU</td><td>9x9x64</td></tr><tr><td>conv3</td><td>9x9x64</td><td>3x3</td><td>1</td><td>64</td><td>ReLU</td><td>7x7x64</td></tr><tr><td>fc4</td><td>7x7x64</td><td></td><td></td><td>512</td><td>ReLU</td><td>512</td></tr><tr><td>fc5</td><td>512</td><td></td><td></td><td>2</td><td>Linear</td><td>2</td></tr></tbody></table></div></div><p>The network consists of three convolutional layers followed by two fully connected layers. ReLU activation is applied to each layer except the last, which utilizes linear activation. The network&rsquo;s output comprises two values, representing the two possible actions available to the player: &ldquo;Fly up&rdquo; and &ldquo;Do nothing.&rdquo;</p><p>The input to the network consists of four consecutive 84x84 grayscale images. An example of four such images, which would be fed to the neural network, is shown below.</p><p>You&rsquo;ll observe that the images are rotated. This rotation stems from the output format of the modified game engine. However, this rotation does not hinder the neural network&rsquo;s performance during training or testing.</p><div data-testid=image-container class="X3Bg-O9c _1xIzGpI7"><figure class=_1sK7TguO><picture class=!contents data-testid=picture><source srcset="https://assets.toptal.io/images?url=https%3A%2F%2Fuploads.toptal.io%2Fblog%2Fimage%2F126777%2Ftoptal-blog-image-1533299620001-9ab9c78be9ac9a40587454aa9ab3e611.png&amp;width=360, https://assets.toptal.io/images?url=https%3A%2F%2Fuploads.toptal.io%2Fblog%2Fimage%2F126777%2Ftoptal-blog-image-1533299620001-9ab9c78be9ac9a40587454aa9ab3e611.png&amp;width=360&amp;dpr=2 2x" media="(max-width: 360px)"><source srcset="https://assets.toptal.io/images?url=https%3A%2F%2Fuploads.toptal.io%2Fblog%2Fimage%2F126777%2Ftoptal-blog-image-1533299620001-9ab9c78be9ac9a40587454aa9ab3e611.png&amp;width=480, https://assets.toptal.io/images?url=https%3A%2F%2Fuploads.toptal.io%2Fblog%2Fimage%2F126777%2Ftoptal-blog-image-1533299620001-9ab9c78be9ac9a40587454aa9ab3e611.png&amp;width=480&amp;dpr=2 2x" media="(min-width: 360.1px) and (max-width: 480px)"><source srcset="https://assets.toptal.io/images?url=https%3A%2F%2Fuploads.toptal.io%2Fblog%2Fimage%2F126777%2Ftoptal-blog-image-1533299620001-9ab9c78be9ac9a40587454aa9ab3e611.png&amp;width=768, https://assets.toptal.io/images?url=https%3A%2F%2Fuploads.toptal.io%2Fblog%2Fimage%2F126777%2Ftoptal-blog-image-1533299620001-9ab9c78be9ac9a40587454aa9ab3e611.png&amp;width=768&amp;dpr=2 2x" media="(min-width: 480.1px) and (max-width: 768px)"><source srcset="https://assets.toptal.io/images?url=https%3A%2F%2Fuploads.toptal.io%2Fblog%2Fimage%2F126777%2Ftoptal-blog-image-1533299620001-9ab9c78be9ac9a40587454aa9ab3e611.png&amp;width=1024, https://assets.toptal.io/images?url=https%3A%2F%2Fuploads.toptal.io%2Fblog%2Fimage%2F126777%2Ftoptal-blog-image-1533299620001-9ab9c78be9ac9a40587454aa9ab3e611.png&amp;width=1024&amp;dpr=2 2x" media="(min-width: 768.1px) and (max-width: 1024px)"><source srcset="https://assets.toptal.io/images?url=https%3A%2F%2Fuploads.toptal.io%2Fblog%2Fimage%2F126777%2Ftoptal-blog-image-1533299620001-9ab9c78be9ac9a40587454aa9ab3e611.png&amp;width=1440, https://assets.toptal.io/images?url=https%3A%2F%2Fuploads.toptal.io%2Fblog%2Fimage%2F126777%2Ftoptal-blog-image-1533299620001-9ab9c78be9ac9a40587454aa9ab3e611.png&amp;width=1440&amp;dpr=2 2x" media="(min-width: 1024.1px) and (max-width: 1440px)"><source srcset="https://assets.toptal.io/images?url=https%3A%2F%2Fuploads.toptal.io%2Fblog%2Fimage%2F126777%2Ftoptal-blog-image-1533299620001-9ab9c78be9ac9a40587454aa9ab3e611.png&amp;width=1920, https://assets.toptal.io/images?url=https%3A%2F%2Fuploads.toptal.io%2Fblog%2Fimage%2F126777%2Ftoptal-blog-image-1533299620001-9ab9c78be9ac9a40587454aa9ab3e611.png&amp;width=1920&amp;dpr=2 2x" media="(min-width: 1440.1px) and (max-width: 1920px)"><source srcset="https://assets.toptal.io/images?url=https%3A%2F%2Fuploads.toptal.io%2Fblog%2Fimage%2F126777%2Ftoptal-blog-image-1533299620001-9ab9c78be9ac9a40587454aa9ab3e611.png" media="(min-width: 1920.1px)"><img alt="Four consecutive black-and-white frames of the Flappy Bird clone, as fed directly to a neural network" src="https://assets.toptal.io/images?url=https%3A%2F%2Fuploads.toptal.io%2Fblog%2Fimage%2F126777%2Ftoptal-blog-image-1533299620001-9ab9c78be9ac9a40587454aa9ab3e611.png" loading=lazy decoding=async class=Djd3YO_2></picture></figure></div><p>Furthermore, the images are cropped to exclude the floor, as it is irrelevant to the task at hand. Pixels representing the pipes and the bird are white, while background pixels are black.</p><p>The following code snippet showcases the definition of the neural network. The network&rsquo;s weights are initialized using a uniform distribution $\mathcal{U}(-0.01, 0.01)$, while the bias terms are set to 0.01. Although various weight initializations <a class=link href=https://pytorch.org/docs/stable/_modules/torch/nn/init.html target=_blank rel=noopener>Several different initializations</a> were explored, including Xavier uniform, Xavier normal, Kaiming uniform, Kaiming normal, uniform, and normal, the chosen initialization proved to result in the fastest convergence and training time for this particular task. The size of the neural network is 6.8 MB.</p><div class=highlight><div class=chroma><table class=lntable><tr><td class=lntd><pre tabindex=0 class=chroma><code><span class=lnt> 1
</span><span class=lnt> 2
</span><span class=lnt> 3
</span><span class=lnt> 4
</span><span class=lnt> 5
</span><span class=lnt> 6
</span><span class=lnt> 7
</span><span class=lnt> 8
</span><span class=lnt> 9
</span><span class=lnt>10
</span><span class=lnt>11
</span><span class=lnt>12
</span><span class=lnt>13
</span><span class=lnt>14
</span><span class=lnt>15
</span><span class=lnt>16
</span><span class=lnt>17
</span><span class=lnt>18
</span><span class=lnt>19
</span><span class=lnt>20
</span><span class=lnt>21
</span><span class=lnt>22
</span><span class=lnt>23
</span><span class=lnt>24
</span><span class=lnt>25
</span><span class=lnt>26
</span><span class=lnt>27
</span><span class=lnt>28
</span><span class=lnt>29
</span><span class=lnt>30
</span><span class=lnt>31
</span><span class=lnt>32
</span><span class=lnt>33
</span><span class=lnt>34
</span><span class=lnt>35
</span><span class=lnt>36
</span></code></pre></td><td class=lntd><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=k>class</span> <span class=nc>NeuralNetwork</span><span class=p>(</span><span class=n>nn</span><span class=o>.</span><span class=n>Module</span><span class=p>):</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>    <span class=k>def</span> <span class=fm>__init__</span><span class=p>(</span><span class=bp>self</span><span class=p>):</span>
</span></span><span class=line><span class=cl>        <span class=nb>super</span><span class=p>(</span><span class=n>NeuralNetwork</span><span class=p>,</span> <span class=bp>self</span><span class=p>)</span><span class=o>.</span><span class=fm>__init__</span><span class=p>()</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>        <span class=bp>self</span><span class=o>.</span><span class=n>number_of_actions</span> <span class=o>=</span> <span class=mi>2</span>
</span></span><span class=line><span class=cl>        <span class=bp>self</span><span class=o>.</span><span class=n>gamma</span> <span class=o>=</span> <span class=mf>0.99</span>
</span></span><span class=line><span class=cl>        <span class=bp>self</span><span class=o>.</span><span class=n>final_epsilon</span> <span class=o>=</span> <span class=mf>0.0001</span>
</span></span><span class=line><span class=cl>        <span class=bp>self</span><span class=o>.</span><span class=n>initial_epsilon</span> <span class=o>=</span> <span class=mf>0.1</span>
</span></span><span class=line><span class=cl>        <span class=bp>self</span><span class=o>.</span><span class=n>number_of_iterations</span> <span class=o>=</span> <span class=mi>2000000</span>
</span></span><span class=line><span class=cl>        <span class=bp>self</span><span class=o>.</span><span class=n>replay_memory_size</span> <span class=o>=</span> <span class=mi>10000</span>
</span></span><span class=line><span class=cl>        <span class=bp>self</span><span class=o>.</span><span class=n>minibatch_size</span> <span class=o>=</span> <span class=mi>32</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>        <span class=bp>self</span><span class=o>.</span><span class=n>conv1</span> <span class=o>=</span> <span class=n>nn</span><span class=o>.</span><span class=n>Conv2d</span><span class=p>(</span><span class=mi>4</span><span class=p>,</span> <span class=mi>32</span><span class=p>,</span> <span class=mi>8</span><span class=p>,</span> <span class=mi>4</span><span class=p>)</span>
</span></span><span class=line><span class=cl>        <span class=bp>self</span><span class=o>.</span><span class=n>relu1</span> <span class=o>=</span> <span class=n>nn</span><span class=o>.</span><span class=n>ReLU</span><span class=p>(</span><span class=n>inplace</span><span class=o>=</span><span class=kc>True</span><span class=p>)</span>
</span></span><span class=line><span class=cl>        <span class=bp>self</span><span class=o>.</span><span class=n>conv2</span> <span class=o>=</span> <span class=n>nn</span><span class=o>.</span><span class=n>Conv2d</span><span class=p>(</span><span class=mi>32</span><span class=p>,</span> <span class=mi>64</span><span class=p>,</span> <span class=mi>4</span><span class=p>,</span> <span class=mi>2</span><span class=p>)</span>
</span></span><span class=line><span class=cl>        <span class=bp>self</span><span class=o>.</span><span class=n>relu2</span> <span class=o>=</span> <span class=n>nn</span><span class=o>.</span><span class=n>ReLU</span><span class=p>(</span><span class=n>inplace</span><span class=o>=</span><span class=kc>True</span><span class=p>)</span>
</span></span><span class=line><span class=cl>        <span class=bp>self</span><span class=o>.</span><span class=n>conv3</span> <span class=o>=</span> <span class=n>nn</span><span class=o>.</span><span class=n>Conv2d</span><span class=p>(</span><span class=mi>64</span><span class=p>,</span> <span class=mi>64</span><span class=p>,</span> <span class=mi>3</span><span class=p>,</span> <span class=mi>1</span><span class=p>)</span>
</span></span><span class=line><span class=cl>        <span class=bp>self</span><span class=o>.</span><span class=n>relu3</span> <span class=o>=</span> <span class=n>nn</span><span class=o>.</span><span class=n>ReLU</span><span class=p>(</span><span class=n>inplace</span><span class=o>=</span><span class=kc>True</span><span class=p>)</span>
</span></span><span class=line><span class=cl>        <span class=bp>self</span><span class=o>.</span><span class=n>fc4</span> <span class=o>=</span> <span class=n>nn</span><span class=o>.</span><span class=n>Linear</span><span class=p>(</span><span class=mi>3136</span><span class=p>,</span> <span class=mi>512</span><span class=p>)</span>
</span></span><span class=line><span class=cl>        <span class=bp>self</span><span class=o>.</span><span class=n>relu4</span> <span class=o>=</span> <span class=n>nn</span><span class=o>.</span><span class=n>ReLU</span><span class=p>(</span><span class=n>inplace</span><span class=o>=</span><span class=kc>True</span><span class=p>)</span>
</span></span><span class=line><span class=cl>        <span class=bp>self</span><span class=o>.</span><span class=n>fc5</span> <span class=o>=</span> <span class=n>nn</span><span class=o>.</span><span class=n>Linear</span><span class=p>(</span><span class=mi>512</span><span class=p>,</span> <span class=bp>self</span><span class=o>.</span><span class=n>number_of_actions</span><span class=p>)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>    <span class=k>def</span> <span class=nf>forward</span><span class=p>(</span><span class=bp>self</span><span class=p>,</span> <span class=n>x</span><span class=p>):</span>
</span></span><span class=line><span class=cl>        <span class=n>out</span> <span class=o>=</span> <span class=bp>self</span><span class=o>.</span><span class=n>conv1</span><span class=p>(</span><span class=n>x</span><span class=p>)</span>
</span></span><span class=line><span class=cl>        <span class=n>out</span> <span class=o>=</span> <span class=bp>self</span><span class=o>.</span><span class=n>relu1</span><span class=p>(</span><span class=n>out</span><span class=p>)</span>
</span></span><span class=line><span class=cl>        <span class=n>out</span> <span class=o>=</span> <span class=bp>self</span><span class=o>.</span><span class=n>conv2</span><span class=p>(</span><span class=n>out</span><span class=p>)</span>
</span></span><span class=line><span class=cl>        <span class=n>out</span> <span class=o>=</span> <span class=bp>self</span><span class=o>.</span><span class=n>relu2</span><span class=p>(</span><span class=n>out</span><span class=p>)</span>
</span></span><span class=line><span class=cl>        <span class=n>out</span> <span class=o>=</span> <span class=bp>self</span><span class=o>.</span><span class=n>conv3</span><span class=p>(</span><span class=n>out</span><span class=p>)</span>
</span></span><span class=line><span class=cl>        <span class=n>out</span> <span class=o>=</span> <span class=bp>self</span><span class=o>.</span><span class=n>relu3</span><span class=p>(</span><span class=n>out</span><span class=p>)</span>
</span></span><span class=line><span class=cl>        <span class=n>out</span> <span class=o>=</span> <span class=n>out</span><span class=o>.</span><span class=n>view</span><span class=p>(</span><span class=n>out</span><span class=o>.</span><span class=n>size</span><span class=p>()[</span><span class=mi>0</span><span class=p>],</span> <span class=o>-</span><span class=mi>1</span><span class=p>)</span>
</span></span><span class=line><span class=cl>        <span class=n>out</span> <span class=o>=</span> <span class=bp>self</span><span class=o>.</span><span class=n>fc4</span><span class=p>(</span><span class=n>out</span><span class=p>)</span>
</span></span><span class=line><span class=cl>        <span class=n>out</span> <span class=o>=</span> <span class=bp>self</span><span class=o>.</span><span class=n>relu4</span><span class=p>(</span><span class=n>out</span><span class=p>)</span>
</span></span><span class=line><span class=cl>        <span class=n>out</span> <span class=o>=</span> <span class=bp>self</span><span class=o>.</span><span class=n>fc5</span><span class=p>(</span><span class=n>out</span><span class=p>)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>        <span class=k>return</span> <span class=n>out</span>
</span></span></code></pre></td></tr></table></div></div><p>Within the constructor, you&rsquo;ll notice the definition of hyperparameters. Hyperparameter optimization is not performed in this particular blog post. Instead, the hyperparameter values are largely adopted from DeepMind&rsquo;s papers, with some adjustments made to suit the complexity of Flappy Bird compared to the Atari games used in their research.</p><p>Specifically, the epsilon value, which governs the exploration-exploitation trade-off, is significantly reduced compared to DeepMind&rsquo;s paper. DeepMind uses an epsilon of one, while this implementation utilizes a value of 0.1. This reduction is motivated by the observation that higher epsilon values lead to excessive flapping, often resulting in the bird colliding with the upper screen boundary.</p><p>The reinforcement learning code operates in two modes: training and testing. During testing, the algorithm&rsquo;s performance is evaluated to assess how effectively it has learned to play the game. However, before testing, the neural network must first be trained. This involves defining a loss function to minimize and an optimizer to guide the minimization process. In this implementation, the <em>Adam</em> optimization algorithm is employed in conjunction with the mean squared error loss function:</p><div class=highlight><div class=chroma><table class=lntable><tr><td class=lntd><pre tabindex=0 class=chroma><code><span class=lnt>1
</span><span class=lnt>2
</span></code></pre></td><td class=lntd><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=n>optimizer</span> <span class=o>=</span> <span class=n>optim</span><span class=o>.</span><span class=n>Adam</span><span class=p>(</span><span class=n>model</span><span class=o>.</span><span class=n>parameters</span><span class=p>(),</span> <span class=n>lr</span><span class=o>=</span><span class=mf>1e-6</span><span class=p>)</span>
</span></span><span class=line><span class=cl><span class=n>criterion</span> <span class=o>=</span> <span class=n>nn</span><span class=o>.</span><span class=n>MSELoss</span><span class=p>()</span>
</span></span></code></pre></td></tr></table></div></div><p>Next, an instance of the game is created:</p><div class=highlight><div class=chroma><table class=lntable><tr><td class=lntd><pre tabindex=0 class=chroma><code><span class=lnt>1
</span></code></pre></td><td class=lntd><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=n>game_state</span> <span class=o>=</span> <span class=n>GameState</span><span class=p>()</span>
</span></span></code></pre></td></tr></table></div></div><p>A Python list is used to implement the replay memory:</p><div class=highlight><div class=chroma><table class=lntable><tr><td class=lntd><pre tabindex=0 class=chroma><code><span class=lnt>1
</span></code></pre></td><td class=lntd><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=n>replay_memory</span> <span class=o>=</span> <span class=p>[]</span>
</span></span></code></pre></td></tr></table></div></div><p>The initial state is then initialized. Actions are represented as two-dimensional tensors:</p><ul><li>[1, 0] corresponds to the &ldquo;Do nothing&rdquo; action.</li><li>[0, 1] corresponds to the &ldquo;Fly up&rdquo; action.</li></ul><p>The <code>frame_step</code> method provides the next game screen, the reward obtained, and a boolean flag indicating whether the current state is terminal (game over). A reward of <code>0.1</code> is awarded for each move the bird makes without crashing, as long as it is not passing through a pipe. Successfully passing through a pipe grants a reward of <code>1</code>, while crashing results in a penalty of <code>-1</code>.</p><p>The <code>resize_and_bgr2gray</code> function handles cropping the floor, resizing the game screen to an 84x84 image, and converting the color space from BGR to grayscale. The <code>image_to_tensor</code> function converts the processed image to a PyTorch tensor and transfers it to GPU memory if CUDA is available. Finally, the last four consecutive screens are concatenated to create the input for the neural network.</p><div class=highlight><div class=chroma><table class=lntable><tr><td class=lntd><pre tabindex=0 class=chroma><code><span class=lnt>1
</span><span class=lnt>2
</span><span class=lnt>3
</span><span class=lnt>4
</span><span class=lnt>5
</span><span class=lnt>6
</span></code></pre></td><td class=lntd><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=n>action</span> <span class=o>=</span> <span class=n>torch</span><span class=o>.</span><span class=n>zeros</span><span class=p>([</span><span class=n>model</span><span class=o>.</span><span class=n>number_of_actions</span><span class=p>],</span> <span class=n>dtype</span><span class=o>=</span><span class=n>torch</span><span class=o>.</span><span class=n>float32</span><span class=p>)</span>
</span></span><span class=line><span class=cl><span class=n>action</span><span class=p>[</span><span class=mi>0</span><span class=p>]</span> <span class=o>=</span> <span class=mi>1</span>
</span></span><span class=line><span class=cl><span class=n>image_data</span><span class=p>,</span> <span class=n>reward</span><span class=p>,</span> <span class=n>terminal</span> <span class=o>=</span> <span class=n>game_state</span><span class=o>.</span><span class=n>frame_step</span><span class=p>(</span><span class=n>action</span><span class=p>)</span>
</span></span><span class=line><span class=cl><span class=n>image_data</span> <span class=o>=</span> <span class=n>resize_and_bgr2gray</span><span class=p>(</span><span class=n>image_data</span><span class=p>)</span>
</span></span><span class=line><span class=cl><span class=n>image_data</span> <span class=o>=</span> <span class=n>image_to_tensor</span><span class=p>(</span><span class=n>image_data</span><span class=p>)</span>
</span></span><span class=line><span class=cl><span class=n>state</span> <span class=o>=</span> <span class=n>torch</span><span class=o>.</span><span class=n>cat</span><span class=p>((</span><span class=n>image_data</span><span class=p>,</span> <span class=n>image_data</span><span class=p>,</span> <span class=n>image_data</span><span class=p>,</span> <span class=n>image_data</span><span class=p>))</span><span class=o>.</span><span class=n>unsqueeze</span><span class=p>(</span><span class=mi>0</span><span class=p>)</span>
</span></span></code></pre></td></tr></table></div></div><p>The initial epsilon value is set as follows:</p><div class=highlight><div class=chroma><table class=lntable><tr><td class=lntd><pre tabindex=0 class=chroma><code><span class=lnt>1
</span></code></pre></td><td class=lntd><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=n>epsilon</span> <span class=o>=</span> <span class=n>model</span><span class=o>.</span><span class=n>initial_epsilon</span>
</span></span></code></pre></td></tr></table></div></div><p>The core training loop is then executed. Comments embedded within the code provide insights into its operation, and you can compare it to the Deep Q-learning with Experience Replay algorithm described earlier.</p><p>The algorithm samples mini-batches of experiences from the replay memory and updates the neural network&rsquo;s parameters accordingly. Actions are chosen using an <em>epsilon-greedy exploration</em> strategy, where epsilon <a class=link href=https://en.wikipedia.org/wiki/Simulated_annealing target=_blank rel=noopener>annealed</a> over time. The loss function being minimized is $ L = \frac{1}{2}\left[\max_{a’}Q(s’, a’) - Q(s, a)\right]^2 $. $ Q(s, a) $ represents the target Q-value calculated using the Bellman equation, while $ \max_{a’}Q(s’, a’) $ is the predicted Q-value obtained from the neural network. The neural network outputs two Q-values, one for each possible action, and the algorithm selects the action associated with the higher Q-value.</p><div class=highlight><div class=chroma><table class=lntable><tr><td class=lntd><pre tabindex=0 class=chroma><code><span class=lnt> 1
</span><span class=lnt> 2
</span><span class=lnt> 3
</span><span class=lnt> 4
</span><span class=lnt> 5
</span><span class=lnt> 6
</span><span class=lnt> 7
</span><span class=lnt> 8
</span><span class=lnt> 9
</span><span class=lnt>10
</span><span class=lnt>11
</span><span class=lnt>12
</span><span class=lnt>13
</span><span class=lnt>14
</span><span class=lnt>15
</span><span class=lnt>16
</span><span class=lnt>17
</span><span class=lnt>18
</span><span class=lnt>19
</span><span class=lnt>20
</span><span class=lnt>21
</span><span class=lnt>22
</span><span class=lnt>23
</span><span class=lnt>24
</span><span class=lnt>25
</span><span class=lnt>26
</span><span class=lnt>27
</span><span class=lnt>28
</span><span class=lnt>29
</span><span class=lnt>30
</span><span class=lnt>31
</span><span class=lnt>32
</span><span class=lnt>33
</span><span class=lnt>34
</span><span class=lnt>35
</span><span class=lnt>36
</span><span class=lnt>37
</span><span class=lnt>38
</span><span class=lnt>39
</span><span class=lnt>40
</span><span class=lnt>41
</span><span class=lnt>42
</span><span class=lnt>43
</span><span class=lnt>44
</span><span class=lnt>45
</span><span class=lnt>46
</span><span class=lnt>47
</span><span class=lnt>48
</span><span class=lnt>49
</span><span class=lnt>50
</span><span class=lnt>51
</span><span class=lnt>52
</span><span class=lnt>53
</span><span class=lnt>54
</span><span class=lnt>55
</span><span class=lnt>56
</span><span class=lnt>57
</span><span class=lnt>58
</span><span class=lnt>59
</span><span class=lnt>60
</span><span class=lnt>61
</span><span class=lnt>62
</span><span class=lnt>63
</span><span class=lnt>64
</span><span class=lnt>65
</span><span class=lnt>66
</span><span class=lnt>67
</span><span class=lnt>68
</span><span class=lnt>69
</span><span class=lnt>70
</span><span class=lnt>71
</span><span class=lnt>72
</span><span class=lnt>73
</span><span class=lnt>74
</span><span class=lnt>75
</span><span class=lnt>76
</span><span class=lnt>77
</span><span class=lnt>78
</span><span class=lnt>79
</span><span class=lnt>80
</span><span class=lnt>81
</span><span class=lnt>82
</span></code></pre></td><td class=lntd><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=k>while</span> <span class=n>iteration</span> <span class=o>&lt;</span> <span class=n>model</span><span class=o>.</span><span class=n>number_of_iterations</span><span class=p>:</span>
</span></span><span class=line><span class=cl>    <span class=c1># get output from the neural network</span>
</span></span><span class=line><span class=cl>    <span class=n>output</span> <span class=o>=</span> <span class=n>model</span><span class=p>(</span><span class=n>state</span><span class=p>)[</span><span class=mi>0</span><span class=p>]</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>    <span class=c1># initialize action</span>
</span></span><span class=line><span class=cl>    <span class=n>action</span> <span class=o>=</span> <span class=n>torch</span><span class=o>.</span><span class=n>zeros</span><span class=p>([</span><span class=n>model</span><span class=o>.</span><span class=n>number_of_actions</span><span class=p>],</span> <span class=n>dtype</span><span class=o>=</span><span class=n>torch</span><span class=o>.</span><span class=n>float32</span><span class=p>)</span>
</span></span><span class=line><span class=cl>    <span class=k>if</span> <span class=n>torch</span><span class=o>.</span><span class=n>cuda</span><span class=o>.</span><span class=n>is_available</span><span class=p>():</span>  <span class=c1># put on GPU if CUDA is available</span>
</span></span><span class=line><span class=cl>        <span class=n>action</span> <span class=o>=</span> <span class=n>action</span><span class=o>.</span><span class=n>cuda</span><span class=p>()</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>    <span class=c1># epsilon greedy exploration</span>
</span></span><span class=line><span class=cl>    <span class=n>random_action</span> <span class=o>=</span> <span class=n>random</span><span class=o>.</span><span class=n>random</span><span class=p>()</span> <span class=o>&lt;=</span> <span class=n>epsilon</span>
</span></span><span class=line><span class=cl>    <span class=k>if</span> <span class=n>random_action</span><span class=p>:</span>
</span></span><span class=line><span class=cl>        <span class=nb>print</span><span class=p>(</span><span class=s2>&#34;Performed random action!&#34;</span><span class=p>)</span>
</span></span><span class=line><span class=cl>    <span class=n>action_index</span> <span class=o>=</span> <span class=p>[</span><span class=n>torch</span><span class=o>.</span><span class=n>randint</span><span class=p>(</span><span class=n>model</span><span class=o>.</span><span class=n>number_of_actions</span><span class=p>,</span> <span class=n>torch</span><span class=o>.</span><span class=n>Size</span><span class=p>([]),</span> <span class=n>dtype</span><span class=o>=</span><span class=n>torch</span><span class=o>.</span><span class=n>int</span><span class=p>)</span>
</span></span><span class=line><span class=cl>                    <span class=k>if</span> <span class=n>random_action</span>
</span></span><span class=line><span class=cl>                    <span class=k>else</span> <span class=n>torch</span><span class=o>.</span><span class=n>argmax</span><span class=p>(</span><span class=n>output</span><span class=p>)][</span><span class=mi>0</span><span class=p>]</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>    <span class=k>if</span> <span class=n>torch</span><span class=o>.</span><span class=n>cuda</span><span class=o>.</span><span class=n>is_available</span><span class=p>():</span>  <span class=c1># put on GPU if CUDA is available</span>
</span></span><span class=line><span class=cl>        <span class=n>action_index</span> <span class=o>=</span> <span class=n>action_index</span><span class=o>.</span><span class=n>cuda</span><span class=p>()</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>    <span class=n>action</span><span class=p>[</span><span class=n>action_index</span><span class=p>]</span> <span class=o>=</span> <span class=mi>1</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>    <span class=c1># get next state and reward</span>
</span></span><span class=line><span class=cl>    <span class=n>image_data_1</span><span class=p>,</span> <span class=n>reward</span><span class=p>,</span> <span class=n>terminal</span> <span class=o>=</span> <span class=n>game_state</span><span class=o>.</span><span class=n>frame_step</span><span class=p>(</span><span class=n>action</span><span class=p>)</span>
</span></span><span class=line><span class=cl>    <span class=n>image_data_1</span> <span class=o>=</span> <span class=n>resize_and_bgr2gray</span><span class=p>(</span><span class=n>image_data_1</span><span class=p>)</span>
</span></span><span class=line><span class=cl>    <span class=n>image_data_1</span> <span class=o>=</span> <span class=n>image_to_tensor</span><span class=p>(</span><span class=n>image_data_1</span><span class=p>)</span>
</span></span><span class=line><span class=cl>    <span class=n>state_1</span> <span class=o>=</span> <span class=n>torch</span><span class=o>.</span><span class=n>cat</span><span class=p>((</span><span class=n>state</span><span class=o>.</span><span class=n>squeeze</span><span class=p>(</span><span class=mi>0</span><span class=p>)[</span><span class=mi>1</span><span class=p>:,</span> <span class=p>:,</span> <span class=p>:],</span> <span class=n>image_data_1</span><span class=p>))</span><span class=o>.</span><span class=n>unsqueeze</span><span class=p>(</span><span class=mi>0</span><span class=p>)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>    <span class=n>action</span> <span class=o>=</span> <span class=n>action</span><span class=o>.</span><span class=n>unsqueeze</span><span class=p>(</span><span class=mi>0</span><span class=p>)</span>
</span></span><span class=line><span class=cl>    <span class=n>reward</span> <span class=o>=</span> <span class=n>torch</span><span class=o>.</span><span class=n>from_numpy</span><span class=p>(</span><span class=n>np</span><span class=o>.</span><span class=n>array</span><span class=p>([</span><span class=n>reward</span><span class=p>],</span> <span class=n>dtype</span><span class=o>=</span><span class=n>np</span><span class=o>.</span><span class=n>float32</span><span class=p>))</span><span class=o>.</span><span class=n>unsqueeze</span><span class=p>(</span><span class=mi>0</span><span class=p>)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>    <span class=c1># save transition to replay memory</span>
</span></span><span class=line><span class=cl>    <span class=n>replay_memory</span><span class=o>.</span><span class=n>append</span><span class=p>((</span><span class=n>state</span><span class=p>,</span> <span class=n>action</span><span class=p>,</span> <span class=n>reward</span><span class=p>,</span> <span class=n>state_1</span><span class=p>,</span> <span class=n>terminal</span><span class=p>))</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>    <span class=c1># if replay memory is full, remove the oldest transition</span>
</span></span><span class=line><span class=cl>    <span class=k>if</span> <span class=nb>len</span><span class=p>(</span><span class=n>replay_memory</span><span class=p>)</span> <span class=o>&gt;</span> <span class=n>model</span><span class=o>.</span><span class=n>replay_memory_size</span><span class=p>:</span>
</span></span><span class=line><span class=cl>        <span class=n>replay_memory</span><span class=o>.</span><span class=n>pop</span><span class=p>(</span><span class=mi>0</span><span class=p>)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>    <span class=c1># epsilon annealing</span>
</span></span><span class=line><span class=cl>    <span class=n>epsilon</span> <span class=o>=</span> <span class=n>epsilon_decrements</span><span class=p>[</span><span class=n>iteration</span><span class=p>]</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>    <span class=c1># sample random minibatch</span>
</span></span><span class=line><span class=cl>    <span class=n>minibatch</span> <span class=o>=</span> <span class=n>random</span><span class=o>.</span><span class=n>sample</span><span class=p>(</span><span class=n>replay_memory</span><span class=p>,</span> <span class=nb>min</span><span class=p>(</span><span class=nb>len</span><span class=p>(</span><span class=n>replay_memory</span><span class=p>),</span> <span class=n>model</span><span class=o>.</span><span class=n>minibatch_size</span><span class=p>))</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>    <span class=c1># unpack minibatch</span>
</span></span><span class=line><span class=cl>    <span class=n>state_batch</span> <span class=o>=</span> <span class=n>torch</span><span class=o>.</span><span class=n>cat</span><span class=p>(</span><span class=nb>tuple</span><span class=p>(</span><span class=n>d</span><span class=p>[</span><span class=mi>0</span><span class=p>]</span> <span class=k>for</span> <span class=n>d</span> <span class=ow>in</span> <span class=n>minibatch</span><span class=p>))</span>
</span></span><span class=line><span class=cl>    <span class=n>action_batch</span> <span class=o>=</span> <span class=n>torch</span><span class=o>.</span><span class=n>cat</span><span class=p>(</span><span class=nb>tuple</span><span class=p>(</span><span class=n>d</span><span class=p>[</span><span class=mi>1</span><span class=p>]</span> <span class=k>for</span> <span class=n>d</span> <span class=ow>in</span> <span class=n>minibatch</span><span class=p>))</span>
</span></span><span class=line><span class=cl>    <span class=n>reward_batch</span> <span class=o>=</span> <span class=n>torch</span><span class=o>.</span><span class=n>cat</span><span class=p>(</span><span class=nb>tuple</span><span class=p>(</span><span class=n>d</span><span class=p>[</span><span class=mi>2</span><span class=p>]</span> <span class=k>for</span> <span class=n>d</span> <span class=ow>in</span> <span class=n>minibatch</span><span class=p>))</span>
</span></span><span class=line><span class=cl>    <span class=n>state_1_batch</span> <span class=o>=</span> <span class=n>torch</span><span class=o>.</span><span class=n>cat</span><span class=p>(</span><span class=nb>tuple</span><span class=p>(</span><span class=n>d</span><span class=p>[</span><span class=mi>3</span><span class=p>]</span> <span class=k>for</span> <span class=n>d</span> <span class=ow>in</span> <span class=n>minibatch</span><span class=p>))</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>    <span class=k>if</span> <span class=n>torch</span><span class=o>.</span><span class=n>cuda</span><span class=o>.</span><span class=n>is_available</span><span class=p>():</span>  <span class=c1># put on GPU if CUDA is available</span>
</span></span><span class=line><span class=cl>        <span class=n>state_batch</span> <span class=o>=</span> <span class=n>state_batch</span><span class=o>.</span><span class=n>cuda</span><span class=p>()</span>
</span></span><span class=line><span class=cl>        <span class=n>action_batch</span> <span class=o>=</span> <span class=n>action_batch</span><span class=o>.</span><span class=n>cuda</span><span class=p>()</span>
</span></span><span class=line><span class=cl>        <span class=n>reward_batch</span> <span class=o>=</span> <span class=n>reward_batch</span><span class=o>.</span><span class=n>cuda</span><span class=p>()</span>
</span></span><span class=line><span class=cl>        <span class=n>state_1_batch</span> <span class=o>=</span> <span class=n>state_1_batch</span><span class=o>.</span><span class=n>cuda</span><span class=p>()</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>    <span class=c1># get output for the next state</span>
</span></span><span class=line><span class=cl>    <span class=n>output_1_batch</span> <span class=o>=</span> <span class=n>model</span><span class=p>(</span><span class=n>state_1_batch</span><span class=p>)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>    <span class=c1># set y_j to r_j for terminal state, otherwise to r_j + gamma*max(Q)</span>
</span></span><span class=line><span class=cl>    <span class=n>y_batch</span> <span class=o>=</span> <span class=n>torch</span><span class=o>.</span><span class=n>cat</span><span class=p>(</span><span class=nb>tuple</span><span class=p>(</span><span class=n>reward_batch</span><span class=p>[</span><span class=n>i</span><span class=p>]</span> <span class=k>if</span> <span class=n>minibatch</span><span class=p>[</span><span class=n>i</span><span class=p>][</span><span class=mi>4</span><span class=p>]</span>
</span></span><span class=line><span class=cl>                              <span class=k>else</span> <span class=n>reward_batch</span><span class=p>[</span><span class=n>i</span><span class=p>]</span> <span class=o>+</span> <span class=n>model</span><span class=o>.</span><span class=n>gamma</span> <span class=o>*</span> <span class=n>torch</span><span class=o>.</span><span class=n>max</span><span class=p>(</span><span class=n>output_1_batch</span><span class=p>[</span><span class=n>i</span><span class=p>])</span>
</span></span><span class=line><span class=cl>                              <span class=k>for</span> <span class=n>i</span> <span class=ow>in</span> <span class=nb>range</span><span class=p>(</span><span class=nb>len</span><span class=p>(</span><span class=n>minibatch</span><span class=p>))))</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>    <span class=c1># extract Q-value</span>
</span></span><span class=line><span class=cl>    <span class=n>q_value</span> <span class=o>=</span> <span class=n>torch</span><span class=o>.</span><span class=n>sum</span><span class=p>(</span><span class=n>model</span><span class=p>(</span><span class=n>state_batch</span><span class=p>)</span> <span class=o>*</span> <span class=n>action_batch</span><span class=p>,</span> <span class=n>dim</span><span class=o>=</span><span class=mi>1</span><span class=p>)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>    <span class=c1># PyTorch accumulates gradients by default, so they need to be reset in each pass</span>
</span></span><span class=line><span class=cl>    <span class=n>optimizer</span><span class=o>.</span><span class=n>zero_grad</span><span class=p>()</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>    <span class=c1># returns a new Tensor, detached from the current graph, the result will never require gradient</span>
</span></span><span class=line><span class=cl>    <span class=n>y_batch</span> <span class=o>=</span> <span class=n>y_batch</span><span class=o>.</span><span class=n>detach</span><span class=p>()</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>    <span class=c1># calculate loss</span>
</span></span><span class=line><span class=cl>    <span class=n>loss</span> <span class=o>=</span> <span class=n>criterion</span><span class=p>(</span><span class=n>q_value</span><span class=p>,</span> <span class=n>y_batch</span><span class=p>)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>    <span class=c1># do backward pass</span>
</span></span><span class=line><span class=cl>    <span class=n>loss</span><span class=o>.</span><span class=n>backward</span><span class=p>()</span>
</span></span><span class=line><span class=cl>    <span class=n>optimizer</span><span class=o>.</span><span class=n>step</span><span class=p>()</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>    <span class=c1># set state to be state_1</span>
</span></span><span class=line><span class=cl>    <span class=n>state</span> <span class=o>=</span> <span class=n>state_1</span>
</span></span></code></pre></td></tr></table></div></div><p>With all the components in place, let&rsquo;s visualize the data flow within our neural network:</p><div data-testid=image-container class="X3Bg-O9c _1xIzGpI7"><figure class=_1sK7TguO><picture class=!contents data-testid=picture><source srcset="https://assets.toptal.io/images?url=https%3A%2F%2Fuploads.toptal.io%2Fblog%2Fimage%2F126785%2Ftoptal-blog-image-1533545714614-309f4fa0677e7e86039a56151e6a4a36.png&amp;width=360, https://assets.toptal.io/images?url=https%3A%2F%2Fuploads.toptal.io%2Fblog%2Fimage%2F126785%2Ftoptal-blog-image-1533545714614-309f4fa0677e7e86039a56151e6a4a36.png&amp;width=360&amp;dpr=2 2x" media="(max-width: 360px)"><source srcset="https://assets.toptal.io/images?url=https%3A%2F%2Fuploads.toptal.io%2Fblog%2Fimage%2F126785%2Ftoptal-blog-image-1533545714614-309f4fa0677e7e86039a56151e6a4a36.png&amp;width=480, https://assets.toptal.io/images?url=https%3A%2F%2Fuploads.toptal.io%2Fblog%2Fimage%2F126785%2Ftoptal-blog-image-1533545714614-309f4fa0677e7e86039a56151e6a4a36.png&amp;width=480&amp;dpr=2 2x" media="(min-width: 360.1px) and (max-width: 480px)"><source srcset="https://assets.toptal.io/images?url=https%3A%2F%2Fuploads.toptal.io%2Fblog%2Fimage%2F126785%2Ftoptal-blog-image-1533545714614-309f4fa0677e7e86039a56151e6a4a36.png&amp;width=768, https://assets.toptal.io/images?url=https%3A%2F%2Fuploads.toptal.io%2Fblog%2Fimage%2F126785%2Ftoptal-blog-image-1533545714614-309f4fa0677e7e86039a56151e6a4a36.png&amp;width=768&amp;dpr=2 2x" media="(min-width: 480.1px) and (max-width: 768px)"><source srcset="https://assets.toptal.io/images?url=https%3A%2F%2Fuploads.toptal.io%2Fblog%2Fimage%2F126785%2Ftoptal-blog-image-1533545714614-309f4fa0677e7e86039a56151e6a4a36.png&amp;width=1024, https://assets.toptal.io/images?url=https%3A%2F%2Fuploads.toptal.io%2Fblog%2Fimage%2F126785%2Ftoptal-blog-image-1533545714614-309f4fa0677e7e86039a56151e6a4a36.png&amp;width=1024&amp;dpr=2 2x" media="(min-width: 768.1px) and (max-width: 1024px)"><source srcset="https://assets.toptal.io/images?url=https%3A%2F%2Fuploads.toptal.io%2Fblog%2Fimage%2F126785%2Ftoptal-blog-image-1533545714614-309f4fa0677e7e86039a56151e6a4a36.png&amp;width=1440, https://assets.toptal.io/images?url=https%3A%2F%2Fuploads.toptal.io%2Fblog%2Fimage%2F126785%2Ftoptal-blog-image-1533545714614-309f4fa0677e7e86039a56151e6a4a36.png&amp;width=1440&amp;dpr=2 2x" media="(min-width: 1024.1px) and (max-width: 1440px)"><source srcset="https://assets.toptal.io/images?url=https%3A%2F%2Fuploads.toptal.io%2Fblog%2Fimage%2F126785%2Ftoptal-blog-image-1533545714614-309f4fa0677e7e86039a56151e6a4a36.png&amp;width=1920, https://assets.toptal.io/images?url=https%3A%2F%2Fuploads.toptal.io%2Fblog%2Fimage%2F126785%2Ftoptal-blog-image-1533545714614-309f4fa0677e7e86039a56151e6a4a36.png&amp;width=1920&amp;dpr=2 2x" media="(min-width: 1440.1px) and (max-width: 1920px)"><source srcset="https://assets.toptal.io/images?url=https%3A%2F%2Fuploads.toptal.io%2Fblog%2Fimage%2F126785%2Ftoptal-blog-image-1533545714614-309f4fa0677e7e86039a56151e6a4a36.png" media="(min-width: 1920.1px)"><img alt='Final high-level overview of data flow using our neural network: The input state consists of four consecutive screens. From that, the neural network gives two Q-values for the two possible actions ("do nothing" and "fly up")' src="https://assets.toptal.io/images?url=https%3A%2F%2Fuploads.toptal.io%2Fblog%2Fimage%2F126785%2Ftoptal-blog-image-1533545714614-309f4fa0677e7e86039a56151e6a4a36.png" loading=lazy decoding=async class=Djd3YO_2></picture></figure></div><p>The following is a short gameplay sequence using a trained neural network:</p><div data-testid=image-container class="X3Bg-O9c _1xIzGpI7"><figure class=_1sK7TguO><img alt="An animation of the Flappy Bird clone being played by the resulting trained neural network" src=https://uploads.toptal.io/blog/image/127044/toptal-blog-image-1536217573520-afe1540e3d8b487b0b974f663a555950.gif loading=lazy decoding=async class=Djd3YO_2></figure></div><p>The neural network showcased above was trained on a high-end Nvidia GTX 1080 GPU for a few hours. Training on a CPU-based system would take significantly longer, potentially several days. During training, the game engine&rsquo;s frames per second (FPS) were set to the maximum possible value (999&mldr;999) to expedite the process. In the testing phase, the FPS was limited to 30 to ensure smooth gameplay.</p><p>The chart below depicts the evolution of the maximum Q-value throughout the training iterations, with every 10,000th iteration displayed. Downward spikes indicate instances where the neural network predicts a low future reward for a particular frame, suggesting an imminent crash.</p><div data-testid=image-container class="X3Bg-O9c _1xIzGpI7"><figure class=_1sK7TguO><picture class=!contents data-testid=picture><source srcset="https://assets.toptal.io/images?url=https%3A%2F%2Fuploads.toptal.io%2Fblog%2Fimage%2F126780%2Ftoptal-blog-image-1533299793559-e4a834559b386637833de29c2a18fa44.png&amp;width=360, https://assets.toptal.io/images?url=https%3A%2F%2Fuploads.toptal.io%2Fblog%2Fimage%2F126780%2Ftoptal-blog-image-1533299793559-e4a834559b386637833de29c2a18fa44.png&amp;width=360&amp;dpr=2 2x" media="(max-width: 360px)"><source srcset="https://assets.toptal.io/images?url=https%3A%2F%2Fuploads.toptal.io%2Fblog%2Fimage%2F126780%2Ftoptal-blog-image-1533299793559-e4a834559b386637833de29c2a18fa44.png&amp;width=480, https://assets.toptal.io/images?url=https%3A%2F%2Fuploads.toptal.io%2Fblog%2Fimage%2F126780%2Ftoptal-blog-image-1533299793559-e4a834559b386637833de29c2a18fa44.png&amp;width=480&amp;dpr=2 2x" media="(min-width: 360.1px) and (max-width: 480px)"><source srcset="https://assets.toptal.io/images?url=https%3A%2F%2Fuploads.toptal.io%2Fblog%2Fimage%2F126780%2Ftoptal-blog-image-1533299793559-e4a834559b386637833de29c2a18fa44.png&amp;width=768, https://assets.toptal.io/images?url=https%3A%2F%2Fuploads.toptal.io%2Fblog%2Fimage%2F126780%2Ftoptal-blog-image-1533299793559-e4a834559b386637833de29c2a18fa44.png&amp;width=768&amp;dpr=2 2x" media="(min-width: 480.1px) and (max-width: 768px)"><source srcset="https://assets.toptal.io/images?url=https%3A%2F%2Fuploads.toptal.io%2Fblog%2Fimage%2F126780%2Ftoptal-blog-image-1533299793559-e4a834559b386637833de29c2a18fa44.png&amp;width=1024, https://assets.toptal.io/images?url=https%3A%2F%2Fuploads.toptal.io%2Fblog%2Fimage%2F126780%2Ftoptal-blog-image-1533299793559-e4a834559b386637833de29c2a18fa44.png&amp;width=1024&amp;dpr=2 2x" media="(min-width: 768.1px) and (max-width: 1024px)"><source srcset="https://assets.toptal.io/images?url=https%3A%2F%2Fuploads.toptal.io%2Fblog%2Fimage%2F126780%2Ftoptal-blog-image-1533299793559-e4a834559b386637833de29c2a18fa44.png&amp;width=1440, https://assets.toptal.io/images?url=https%3A%2F%2Fuploads.toptal.io%2Fblog%2Fimage%2F126780%2Ftoptal-blog-image-1533299793559-e4a834559b386637833de29c2a18fa44.png&amp;width=1440&amp;dpr=2 2x" media="(min-width: 1024.1px) and (max-width: 1440px)"><source srcset="https://assets.toptal.io/images?url=https%3A%2F%2Fuploads.toptal.io%2Fblog%2Fimage%2F126780%2Ftoptal-blog-image-1533299793559-e4a834559b386637833de29c2a18fa44.png&amp;width=1920, https://assets.toptal.io/images?url=https%3A%2F%2Fuploads.toptal.io%2Fblog%2Fimage%2F126780%2Ftoptal-blog-image-1533299793559-e4a834559b386637833de29c2a18fa44.png&amp;width=1920&amp;dpr=2 2x" media="(min-width: 1440.1px) and (max-width: 1920px)"><source srcset="https://assets.toptal.io/images?url=https%3A%2F%2Fuploads.toptal.io%2Fblog%2Fimage%2F126780%2Ftoptal-blog-image-1533299793559-e4a834559b386637833de29c2a18fa44.png" media="(min-width: 1920.1px)"><img alt="A chart showing how the maximum Q-value changed during iterations. Starting at zero and with several downward spikes, it shows an overall trend toward a Q-value maximum around 12 or 13 after around a million iterations." src="https://assets.toptal.io/images?url=https%3A%2F%2Fuploads.toptal.io%2Fblog%2Fimage%2F126780%2Ftoptal-blog-image-1533299793559-e4a834559b386637833de29c2a18fa44.png" loading=lazy decoding=async class=Djd3YO_2></picture></figure></div><p>The complete source code and a pretrained model are available <a class=link href=https://github.com/nevenp/dqn_flappy_bird target=_blank rel=noopener>here</a>.</p><h2 id=deep-reinforcement-learning-2d-3d-and-even-real-life>Deep Reinforcement Learning: 2D, 3D, and Even Real Life</h2><p>This PyTorch reinforcement learning tutorial demonstrated how a computer, starting with no prior knowledge of Flappy Bird, can learn to play the game through a trial-and-error approach, mimicking the learning process of a human player.</p><p>Remarkably, the entire algorithm can be implemented in a concise manner using the PyTorch framework. It&rsquo;s worth noting that the techniques presented in this blog post are based on a relatively early paper in the field of deep reinforcement learning. Since then, numerous advancements have been made, leading to faster convergence and improved performance.</p><p>Deep reinforcement learning has transcended the realm of 2D games and found applications in training agents to play 3D games and even control real-world robotic systems.</p><p>Companies at the forefront of AI research, such as <a class=link href=https://deepmind.com/ target=_blank rel=noopener>DeepMind</a>, <a class=link href=https://blogs.microsoft.com/blog/2017/01/13/microsoft-acquires-deep-learning-startup-maluuba-ai-pioneer-yoshua-bengio-advisory-role/ target=_blank rel=noopener>Maluuba</a>, and <a class=link href=https://www.vicarious.com/ target=_blank rel=noopener>Vicarious</a>, are heavily invested in advancing deep reinforcement learning. This technology played a pivotal role in the development of AlphaGo, the AI system that famously defeated Lee Sedol, one of the world&rsquo;s top Go players. At the time, it was widely believed that it would take at least a decade for machines to reach such a level of proficiency in Go.</p><p>The surge of interest and investment in deep reinforcement learning, and artificial intelligence in general, has sparked discussions about the potential emergence of artificial <em>general</em> intelligence (AGI) - human-level or even superhuman intelligence that can be expressed algorithmically and simulated on computers. However, AGI is a topic for another time.</p><hr><p>References:</p><ul><li><a class=link href=https://ai.intel.com/demystifying-deep-reinforcement-learning/ target=_blank rel=noopener>Demystifying Deep Reinforcement Learning</a></li><li><a class=link href=http://cs229.stanford.edu/proj2015/362_report.pdf target=_blank rel=noopener>Deep Reinforcement Learning for Flappy Bird</a></li><li><a class=link href=https://en.wikipedia.org/wiki/Convolutional_neural_network target=_blank rel=noopener>“Convolutional neural network” at Wikipedia</a></li><li><a class=link href=https://en.wikipedia.org/wiki/Reinforcement_learning target=_blank rel=noopener>“Reinforcement learning” at Wikipedia</a></li><li><a class=link href=https://en.wikipedia.org/wiki/Markov_decision_process target=_blank rel=noopener>“Markov decision process” at Wikipedia</a></li><li><a class=link href=http://www0.cs.ucl.ac.uk/staff/d.silver/web/Teaching.html target=_blank rel=noopener>University College London course on RL</a></li></ul></section><footer class=article-footer><section class=article-copyright><svg class="icon icon-tabler icon-tabler-copyright" width="24" height="24" viewBox="0 0 24 24" stroke-width="2" stroke="currentcolor" fill="none" stroke-linecap="round" stroke-linejoin="round"><path stroke="none" d="M0 0h24v24H0z"/><circle cx="12" cy="12" r="9"/><path d="M14.5 9a3.5 4 0 100 6"/></svg>
<span>Licensed under CC BY-NC-SA 4.0</span></section></footer></article><footer class=site-footer><script async src="https://pagead2.googlesyndication.com/pagead/js/adsbygoogle.js?client=ca-pub-3987358164777632" crossorigin=anonymous></script><script>(function(e,t,n,s,o){e[s]=e[s]||[],e[s].push({"gtm.start":(new Date).getTime(),event:"gtm.js"});var a=t.getElementsByTagName(n)[0],i=t.createElement(n),r=s!="dataLayer"?"&l="+s:"";i.async=!0,i.src="https://www.googletagmanager.com/gtm.js?id="+o+r,a.parentNode.insertBefore(i,a)})(window,document,"script","dataLayer","GTM-5X67X4Q4")</script><noscript><iframe src="https://www.googletagmanager.com/ns.html?id=GTM-5X67X4Q4" height=0 width=0 style=display:none;visibility:hidden></iframe></noscript></footer><div class=pswp tabindex=-1 role=dialog aria-hidden=true><div class=pswp__bg></div><div class=pswp__scroll-wrap><div class=pswp__container><div class=pswp__item></div><div class=pswp__item></div><div class=pswp__item></div></div><div class="pswp__ui pswp__ui--hidden"><div class=pswp__top-bar><div class=pswp__counter></div><button class="pswp__button pswp__button--close" title="Close (Esc)"></button>
<button class="pswp__button pswp__button--share" title=Share></button>
<button class="pswp__button pswp__button--fs" title="Toggle fullscreen"></button>
<button class="pswp__button pswp__button--zoom" title="Zoom in/out"></button><div class=pswp__preloader><div class=pswp__preloader__icn><div class=pswp__preloader__cut><div class=pswp__preloader__donut></div></div></div></div></div><div class="pswp__share-modal pswp__share-modal--hidden pswp__single-tap"><div class=pswp__share-tooltip></div></div><button class="pswp__button pswp__button--arrow--left" title="Previous (arrow left)">
</button>
<button class="pswp__button pswp__button--arrow--right" title="Next (arrow right)"></button><div class=pswp__caption><div class=pswp__caption__center></div></div></div></div></div><script src=https://cdn.jsdelivr.net/npm/photoswipe@4.1.3/dist/photoswipe.min.js integrity="sha256-ePwmChbbvXbsO02lbM3HoHbSHTHFAeChekF1xKJdleo=" crossorigin=anonymous defer></script><script src=https://cdn.jsdelivr.net/npm/photoswipe@4.1.3/dist/photoswipe-ui-default.min.js integrity="sha256-UKkzOn/w1mBxRmLLGrSeyB4e1xbrp4xylgAWb3M42pU=" crossorigin=anonymous defer></script><link rel=stylesheet href=https://cdn.jsdelivr.net/npm/photoswipe@4.1.3/dist/default-skin/default-skin.min.css crossorigin=anonymous><link rel=stylesheet href=https://cdn.jsdelivr.net/npm/photoswipe@4.1.3/dist/photoswipe.min.css crossorigin=anonymous></main></div><script src=https://cdn.jsdelivr.net/npm/node-vibrant@3.1.6/dist/vibrant.min.js integrity="sha256-awcR2jno4kI5X0zL8ex0vi2z+KMkF24hUW8WePSA9HM=" crossorigin=anonymous></script><script type=text/javascript src=/ts/main.js defer></script><script>(function(){const e=document.createElement("link");e.href="https://fonts.googleapis.com/css2?family=Lato:wght@300;400;700&display=swap",e.type="text/css",e.rel="stylesheet",document.head.appendChild(e)})()</script></body></html>