<!doctype html><html lang=en dir=ltr><head><meta charset=utf-8><meta name=viewport content='width=device-width,initial-scale=1'><meta name=description content="Let&rsquo;s explore reinforcement learning by tackling a practical problem using popular libraries such as TensorFlow, TensorBoard, Keras, and OpenAI Gym. We&rsquo;ll implement deep $Q$-learning to understand its workings. The code will run on a standard PC, leveraging all CPU cores thanks to TensorFlow&rsquo;s capabilities.\nOur problem is the Mountain Car: Imagine a car on a one-dimensional track between two mountains. The goal is to drive up the right mountain (reaching a flag). However, the car&rsquo;s engine is weak and cannot climb the mountain directly. Success requires building momentum by driving back and forth.\n"><title>Exploring Reinforcement Learning in Depth</title>
<link rel=canonical href=https://Nexus-Security.github.io/exploring-reinforcement-learning-in-depth/><link rel=stylesheet href=/scss/style.min.0304c6baf04e01a8fe70693791cb744d56a3578a3120a8796cefc66825aa39c7.css><meta property='og:title' content="Exploring Reinforcement Learning in Depth"><meta property='og:description' content="Let&rsquo;s explore reinforcement learning by tackling a practical problem using popular libraries such as TensorFlow, TensorBoard, Keras, and OpenAI Gym. We&rsquo;ll implement deep $Q$-learning to understand its workings. The code will run on a standard PC, leveraging all CPU cores thanks to TensorFlow&rsquo;s capabilities.\nOur problem is the Mountain Car: Imagine a car on a one-dimensional track between two mountains. The goal is to drive up the right mountain (reaching a flag). However, the car&rsquo;s engine is weak and cannot climb the mountain directly. Success requires building momentum by driving back and forth.\n"><meta property='og:url' content='https://Nexus-Security.github.io/exploring-reinforcement-learning-in-depth/'><meta property='og:site_name' content='Nexus Security'><meta property='og:type' content='article'><meta property='article:section' content='Post'><meta property='article:published_time' content='2024-05-15T00:00:00+00:00'><meta property='article:modified_time' content='2024-05-15T00:00:00+00:00'><meta name=twitter:title content="Exploring Reinforcement Learning in Depth"><meta name=twitter:description content="Let&rsquo;s explore reinforcement learning by tackling a practical problem using popular libraries such as TensorFlow, TensorBoard, Keras, and OpenAI Gym. We&rsquo;ll implement deep $Q$-learning to understand its workings. The code will run on a standard PC, leveraging all CPU cores thanks to TensorFlow&rsquo;s capabilities.\nOur problem is the Mountain Car: Imagine a car on a one-dimensional track between two mountains. The goal is to drive up the right mountain (reaching a flag). However, the car&rsquo;s engine is weak and cannot climb the mountain directly. Success requires building momentum by driving back and forth.\n"><link rel="shortcut icon" href=/fav.ico></head><body class=article-page><script>(function(){const e="StackColorScheme";localStorage.getItem(e)||localStorage.setItem(e,"light")})()</script><script>(function(){const t="StackColorScheme",e=localStorage.getItem(t),n=window.matchMedia("(prefers-color-scheme: dark)").matches===!0;e=="dark"||e==="auto"&&n?document.documentElement.dataset.scheme="dark":document.documentElement.dataset.scheme="light"})()</script><div class="container main-container flex on-phone--column extended"><aside class="sidebar left-sidebar sticky"><button class="hamburger hamburger--spin" type=button id=toggle-menu aria-label="Toggle Menu">
<span class=hamburger-box><span class=hamburger-inner></span></span></button><ol class=menu-social><li><a href=https://github.com/Nexus-Security target=_blank title=GitHub rel=me><svg xmlns:xlink="http://www.w3.org/1999/xlink" width="150" height="150" viewBox="0 0 150 150"><defs><filter id="alpha" filterUnits="objectBoundingBox" x="0" y="0" width="100%" height="100%"><feColorMatrix type="matrix" in="SourceGraphic" values="0 0 0 0 1 0 0 0 0 1 0 0 0 0 1 0 0 0 1 0"/></filter><mask id="mask0"><g filter="url(#alpha)"><rect x="0" y="0" width="150" height="150" style="fill:#000;fill-opacity:.2;stroke:none"/></g></mask><clipPath id="clip1"><rect x="0" y="0" width="150" height="150"/></clipPath><g id="surface5" clip-path="url(#clip1)"><path style="stroke:none;fill-rule:nonzero;fill:#fff;fill-opacity:1" d="M10.9375 83.867188c0-12.109376 3.46875-21.816407 10.925781-30.535157.71875-.875.925781000000001-2.058593.550781000000001-3.125C21.6875 48.125 21.40625 33.070312 26.488281 17.675781c4.78125 1.207031 16.300781 5.050781 31.65625 16.398438C58.9375 34.65625 59.957031 34.84375 60.894531 34.554688 64.824219 33.382812 71.851562 32.6875 79.6875 32.6875S94.550781 33.386719 98.476562 34.554688C99.417969 34.835938 100.433594 34.65625 101.226562 34.074219 113.96875 24.65625 124.011719 20.445312 129.8125 18.582031c-.476562000000001-1.882812-1.019531-3.746093-1.625-5.589843-4.78125 1.207031-16.300781 5.050781-31.648438 16.394531C95.746094 29.96875 94.730469 30.144531 93.789062 29.867188c-3.925781-1.171876-10.953124-1.875-18.789062-1.875-7.832031.0-14.855469.703124000000003-18.792969 1.871093C55.265625 30.144531 54.246094 29.964844 53.457031 29.382812c-15.355469-11.34375-26.875-15.1875-31.65625-16.402343C16.71875 28.382812 17 43.4375 17.726562 45.523438 18.101562 46.589844 17.890625 47.777344 17.175781 48.648438 9.71875 57.367188 6.25 67.070312 6.25 79.179688c0 23.515624 7.175781 37.921874 17.417969 46.550781C16.007812 116.851562 10.9375 103.582031 10.9375 83.867188zm0 0"/></g><mask id="mask1"><g filter="url(#alpha)"><rect x="0" y="0" width="150" height="150" style="fill:#000;fill-opacity:.101961;stroke:none"/></g></mask><clipPath id="clip2"><rect x="0" y="0" width="150" height="150"/></clipPath><g id="surface8" clip-path="url(#clip2)"><path style="stroke:none;fill-rule:nonzero;fill:#000;fill-opacity:1" d="M32.34375 78.5c3.945312-4 9.382812-5.074219 16.039062-5.074219C51.925781 73.425781 55.824219 73.730469 60.023438 74.054688c4.71875.375 9.589843.71875 14.84375.746093000000002C80.335938 74.917969 85.210938 74.425781 89.976562 74.054688 102.070312 73.125 111.605469 72.367188 117.648438 78.492188c4.527343 4.601562 6.890624 10.328124 7.21875 16.945312C124.925781 94.398438 125 93.382812 125 92.261719c0-7.285157-2.476562-13.492188-7.351562-18.449219-6.042969-6.117188-15.578126-5.367188-27.671876-4.4375C85.210938 69.742188 80.34375 70.238281 74.867188 70.117188 69.914062 70.0625 64.960938 69.8125 60.023438 69.375 55.824219 69.050781 51.929688 68.742188 48.382812 68.742188c-6.65625.0-12.09375 1.070312-16.039062 5.078124C27.476562 78.769531 25 84.976562 25 92.261719 25 93.375 25.074219 94.398438 25.132812 95.445312 25.460938 88.820312 27.820312 83.101562 32.34375 78.5zm0 0"/></g><linearGradient id="linear0" gradientUnits="userSpaceOnUse" x1="10.768" y1="13.86" x2="18.633" y2="21.724" gradientTransform="matrix(6.25,0,0,6.25,0,0)"><stop offset="0" style="stop-color:#000;stop-opacity:.101961"/><stop offset="1" style="stop-color:#000;stop-opacity:0"/></linearGradient><linearGradient id="linear1" gradientUnits="userSpaceOnUse" x1="4.004" y1="18.529" x2="4.047" y2="18.573" gradientTransform="matrix(6.25,0,0,6.25,0,0)"><stop offset="0" style="stop-color:#000;stop-opacity:.101961"/><stop offset="1" style="stop-color:#000;stop-opacity:0"/></linearGradient><linearGradient id="linear2" gradientUnits="userSpaceOnUse" x1=".617" y1="5.771" x2="23.69" y2="16.53" gradientTransform="matrix(6.25,0,0,6.25,0,0)"><stop offset="0" style="stop-color:#fff;stop-opacity:.2"/><stop offset="1" style="stop-color:#fff;stop-opacity:0"/></linearGradient></defs><g id="surface1"><path style="stroke:none;fill-rule:nonzero;fill:#303c42;fill-opacity:1" d="M138.570312 45.789062C139.492188 39.613281 138.867188 23.738281 133.167969 8.292969 132.660156 6.945312 131.300781 6.113281 129.867188 6.273438 129.269531 6.34375 115.1875 8.164062 94.039062 23.46875 89.445312 22.363281 82.632812 21.742188 75 21.742188s-14.4375.625-19.042969 1.726562C34.800781 8.15625 20.71875 6.34375 20.117188 6.273438 18.6875 6.117188 17.332031 6.949219 16.820312 8.292969 11.117188 23.738281 10.5 39.613281 11.425781 45.789062 3.742188 55.289062.0 66.230469.0 79.179688c0 41.632812 21.824219 64.558593 61.457031 64.558593L75 143.75 88.539062 143.738281C128.167969 143.738281 150 120.8125 150 79.179688c0-12.949219-3.742188-23.890626-11.429688-33.390626zm0 0"/><path style="stroke:none;fill-rule:nonzero;fill:#5c6671;fill-opacity:1" d="M88.539062 137.488281 75 137.5 61.457031 137.488281C36.273438 137.488281 6.25 127.367188 6.25 79.179688c0-12.109376 3.46875-21.816407 10.925781-30.535157.71875-.875.925781000000001-2.058593.550781000000001-3.125C17 43.4375 16.71875 28.382812 21.800781 12.988281c4.78125 1.207031 16.300781 5.050781 31.65625 16.398438C54.25 29.96875 55.269531 30.15625 56.207031 29.867188c3.9375-1.171876 10.960938-1.875 18.792969-1.875 7.835938.0 14.863281.703124000000003 18.789062 1.871093C94.730469 30.140625 95.746094 29.964844 96.539062 29.382812c15.347657-11.34375 26.867188-15.1875 31.648438-16.394531 5.082031 15.394531 4.804688 30.449219 4.082031 32.53125C131.867188 46.585938 132.078125 47.785156 132.820312 48.648438 140.273438 57.367188 143.75 67.070312 143.75 79.179688c0 48.1875-30.023438 58.308593-55.210938 58.308593zm0 0"/><use xlink:href="#surface5" mask="url(#mask0)"/><path style="stroke:none;fill-rule:nonzero;fill:#303c42;fill-opacity:1" d="M89.488281 63.144531C84.886719 63.507812 80.125 63.875 75.226562 63.875H74.78125C69.875 63.875 65.117188 63.5 60.519531 63.144531 47.898438 62.164062 35.988281 61.230469 27.894531 69.429688 21.824219 75.59375 18.75 83.28125 18.75 92.261719 18.75 128.238281 46.429688 131.25 75.226562 131.25c28.34375.0 56.023438-3.011719 56.023438-38.988281.0-8.988281-3.074219-16.667969-9.144531-22.835938-8.085938-8.195312-20.003907-7.269531-32.617188-6.28125zm0 0"/><path style="stroke:none;fill-rule:nonzero;fill:#d9cfcc;fill-opacity:1" d="M74.773438 125C42.492188 125 25 119.78125 25 92.261719c0-7.285157 2.476562-13.492188 7.34375-18.449219 3.945312-4 9.382812-5.074219 16.039062-5.074219C51.925781 68.738281 55.824219 69.042969 60.023438 69.367188c4.71875.375 9.589843.71875 14.84375.746093000000002C80.335938 70.230469 85.210938 69.738281 89.976562 69.367188c12.09375-.929687999999999 21.628907-1.6875 27.671876 4.4375C122.523438 78.769531 125 84.976562 125 92.261719 125 119.78125 107.507812 125 74.773438 125zm0 0"/><use xlink:href="#surface8" mask="url(#mask1)"/><path style="stroke:none;fill-rule:nonzero;fill:#303c42;fill-opacity:1" d="M50 75c-7.125.0-12.5 9.40625-12.5 21.875S42.875 118.75 50 118.75s12.5-9.40625 12.5-21.875S57.125 75 50 75zm0 0"/><path style="stroke:none;fill-rule:nonzero;fill:#6d4c41;fill-opacity:1" d="M50 112.5c-2.550781.0-6.25-6.085938-6.25-15.625S47.449219 81.25 50 81.25s6.25 6.085938 6.25 15.625S52.550781 112.5 50 112.5zm0 0"/><path style="stroke:none;fill-rule:nonzero;fill:#303c42;fill-opacity:1" d="M1e2 75c-7.125.0-12.5 9.40625-12.5 21.875S92.875 118.75 1e2 118.75s12.5-9.40625 12.5-21.875S107.125 75 1e2 75zm0 0"/><path style="stroke:none;fill-rule:nonzero;fill:#6d4c41;fill-opacity:1" d="M1e2 112.5c-2.550781.0-6.25-6.085938-6.25-15.625S97.449219 81.25 1e2 81.25s6.25 6.085938 6.25 15.625S102.550781 112.5 1e2 112.5zm0 0"/><path style="stroke:none;fill-rule:nonzero;fill:#a1887f;fill-opacity:1" d="M52.21875 90.511719c0 1.539062-1.246094 2.789062-2.789062 2.789062-1.539063.0-2.785157-1.25-2.785157-2.789062.0-1.539063 1.246094-2.785157 2.785157-2.785157 1.542968.0 2.789062 1.246094 2.789062 2.785157zm0 0"/><path style="stroke:none;fill-rule:nonzero;fill:#a1887f;fill-opacity:1" d="M102.21875 90.511719c0 1.539062-1.246094 2.789062-2.789062 2.789062-1.539063.0-2.785157-1.25-2.785157-2.789062.0-1.539063 1.246094-2.785157 2.785157-2.785157 1.542968.0 2.789062 1.246094 2.789062 2.785157zm0 0"/><path style="stroke:none;fill-rule:nonzero;fill:url(#linear0)" d="M125 72.8125 124.894531 72.855469C129.039062 78.382812 131.25 84.851562 131.25 92.261719c0 35.976562-27.679688 38.988281-56.023438 38.988281-16.664062.0-32.914062-1.054688-43.683593-8.875l13.386719 13.386719c5.507812 1.207031 11.121093 1.71875 16.527343 1.71875L75 137.5 88.539062 137.488281c22.992188.0 49.941407-8.53125 54.472657-46.664062zm0 0"/><path style="stroke:none;fill-rule:nonzero;fill:url(#linear1)" d="M25.070312 115.800781 25 115.832031 25.269531 116.101562zm0 0"/><path style="stroke:none;fill-rule:nonzero;fill:url(#linear2)" d="M138.570312 45.789062C139.492188 39.613281 138.867188 23.738281 133.167969 8.292969 132.660156 6.945312 131.300781 6.113281 129.867188 6.273438 129.269531 6.34375 115.1875 8.164062 94.039062 23.46875 89.445312 22.363281 82.632812 21.742188 75 21.742188s-14.4375.625-19.042969 1.726562C34.800781 8.15625 20.71875 6.34375 20.117188 6.273438 18.6875 6.117188 17.332031 6.949219 16.820312 8.292969 11.117188 23.738281 10.5 39.613281 11.425781 45.789062 3.742188 55.289062.0 66.230469.0 79.179688c0 41.632812 21.824219 64.558593 61.457031 64.558593L75 143.75 88.539062 143.738281C128.167969 143.738281 150 120.8125 150 79.179688c0-12.949219-3.742188-23.890626-11.429688-33.390626zm0 0"/></g></svg></a></li><li><a href=mailto:0x000216@gmail.com target=_blank title=Email rel=me><svg width="512" height="512" viewBox="0 0 512 512"><path d="M33.994 73.934C27.135 76.021 21.866 80.426 18.364 87 16.567 90.374 16.5 96.438 16.5 256V421.5l2.158 4C21.184 430.18 25.101 434.027 30 436.636 33.003 438.235 36.304 438.546 53.246 438.825L72.993 439.15l.253-137.978L73.5 163.194 90 175.45C99.075 182.191 140.17 212.762 181.321 243.386l74.821 55.68 21.179-15.752C288.97 274.65 330 244.021 368.5 215.248l70-52.312L438.754 301.043 439.007 439.15 458.754 438.825C476.726 438.529 478.859 438.306 482.5 436.342 487.18 433.816 491.027 429.899 493.636 425 495.433 421.626 495.5 415.562 495.5 256 495.5 96.438 495.433 90.374 493.636 87 488.565 77.48 482.099 73.799 469.397 73.202L460.293 72.774 454.397 77.337C439.883 88.566 256.423 221.935 255.772 221.73 254.976 221.479 78.972 93.309 62.177 80.75L51.812 73 44.156 73.086C39.945 73.133 35.372 73.515 33.994 73.934" stroke="none" fill="#d34c3c" fill-rule="evenodd"/><path d="M54.528 74.911C55.612 75.963 74.953 90.25 97.507 106.661c22.555 16.412 67.228 48.964 99.275 72.339s58.594 42.604 58.993 42.731C256.549 221.978 454.147 78.147 457.472 74.916 459.416 73.028 456.501 73 256 73 55.677 73 52.586 73.029 54.528 74.911M73 300.878V439H256 439V300.941C439 190.749 438.748 163.038 437.75 163.654 437.063 164.078 405.45 187.632 367.5 215.994c-37.95 28.363-78.528 58.655-90.173 67.316l-21.173 15.748-66.827-49.716C152.572 221.999 111.925 191.776 99 182.18s-24.062-17.892-24.75-18.436C73.251 162.954 73 190.519 73 300.878" stroke="none" fill="#e5e5e5" fill-rule="evenodd"/></svg></a></li><li><a href=https://nexus-security.github.io/index.xml target=_blank title=RSS rel=me><svg id="Layer_1" xmlns:xlink="http://www.w3.org/1999/xlink" viewBox="0 0 512 512" style="enable-background:new 0 0 512 512"><path style="fill:#f78c20" d="M78.333 355.334C35.14 355.334.0 390.474.0 433.667S35.14 512 78.333 512s78.333-35.14 78.333-78.333-35.14-78.333-78.333-78.333z"/><g><path style="fill:#ffa929" d="M78.333 381.445c-28.795.0-52.222 23.427-52.222 52.222s23.427 52.222 52.222 52.222 52.222-23.427 52.222-52.222-23.427-52.222-52.222-52.222z"/><path style="fill:#ffa929" d="M477.918 264.861c-21.843-51.641-53.111-98.019-92.936-137.842-39.824-39.824-86.201-71.093-137.843-92.935C193.669 11.468 136.874.0 78.333.0c-4.807.0-8.704 3.897-8.704 8.704v85.519c0 4.807 3.897 8.704 8.704 8.704 182.37.0 330.74 148.369 330.74 330.74.0 4.807 3.897 8.704 8.704 8.704h85.52c4.807.0 8.704-3.897 8.704-8.704C512 375.126 500.533 318.331 477.918 264.861z"/><path style="fill:#ffa929" d="M78.333 163.853c-4.807.0-8.704 3.897-8.704 8.704v95.74c0 4.807 3.897 8.704 8.704 8.704 86.386.0 156.666 70.281 156.666 156.666.0 4.807 3.897 8.704 8.704 8.704h95.74c4.807.0 8.704-3.897 8.704-8.704.0-72.07-28.065-139.826-79.027-190.787-50.961-50.961-118.717-79.027-190.787-79.027z"/></g><g><path style="fill:#f78c20" d="M78.333 242.186c-2.918.0-5.817.076-8.704.206v25.905c0 4.807 3.897 8.704 8.704 8.704 86.386.0 156.666 70.281 156.666 156.666.0 4.807 3.897 8.704 8.704 8.704h25.905c.129-2.886.206-5.786.206-8.704.0-105.752-85.729-191.481-191.481-191.481z"/><path style="fill:#f78c20" d="M78.333 68.113c-2.91.0-5.81.042-8.704.11v26.001c0 4.807 3.897 8.704 8.704 8.704 182.37.0 330.74 148.369 330.74 330.74.0 4.807 3.897 8.704 8.704 8.704h26.001c.067-2.894.11-5.793.11-8.704C443.887 231.777 280.223 68.113 78.333 68.113z"/></g><g/><g/><g/><g/><g/><g/><g/><g/><g/><g/><g/><g/><g/><g/><g/></svg></a></li><li><a href=https://x.com/0x000216 target=_blank title=Twitter rel=me><svg xmlns:xlink="http://www.w3.org/1999/xlink" width="150" height="150" viewBox="0 0 150 150"><g id="surface1"><path style="stroke:none;fill-rule:nonzero;fill:#03a9f4;fill-opacity:1" d="M150 33.179688c-5.640625 2.460937-11.609375 4.09375-17.71875 4.855468 6.4375-3.820312 11.25-9.867187 13.527344-16.996094-6.027344 3.574219-12.621094 6.089844-19.5 7.441407-8.628906-9.210938-22.007813-12.214844-33.746094-7.574219-11.738281 4.636719-19.449219 15.980469-19.445312 28.601562.0 2.4375.203124000000003 4.78125.710937000000001 7.015626C49.085938 55.308594 26.03125 43.609375 10.445312 24.355469 2.25 38.402344 6.386719 56.398438 19.894531 65.457031 15.027344 65.324219 10.261719 64.027344 6 61.667969V62.007812c.015625 14.636719 10.304688 27.25 24.636719 30.214844-2.628907.691406000000001-5.339844 1.03125-8.0625 1.011719C20.621094 93.269531 18.667969 93.09375 16.753906 92.710938c4.070313 12.507812 15.585938 21.089843 28.734375 21.421874-10.886719 8.511719-24.308593 13.128907-38.128906 13.113282C4.835938 127.246094 2.417969 127.132812.0 126.824219c14.0625 9.0625 30.445312 13.855469 47.175781 13.800781 56.585938.0 87.523438-46.875 87.523438-87.507812.0-1.359376-.046875-2.671876-.113281000000001-3.972657C140.652344 44.804688 145.875 39.394531 150 33.179688zm0 0"/></g></svg></a></li><li><a href=https://www.minds.com/0x000216/ target=_blank title=Minds rel=me><svg id="Capa_1" xmlns:xlink="http://www.w3.org/1999/xlink" viewBox="0 0 512 512" style="enable-background:new 0 0 512 512"><g><g><path style="fill:#ffe1b2" d="M256 33.085C245.078 13.38 224.079.0 2e2.0c-23.781.0-45.57 13.293-56.594 34.184C115.711 41.602 96 66.977 96 96c0 .059.0.113.0.172-9.977 7.512-16 19.301-16 31.828.0 1.316.078 2.637.234 3.992C60.211 145.266 48 167.758 48 192c0 14.07 4.039 27.543 11.719 39.262C57.273 236.512 56 242.207 56 248c0 2.738.281 5.445.828 8.098C36.672 267.308 24 288.539 24 312c0 27.973 18.305 52.34 44.109 60.785C65.398 378.828 64 385.324 64 392c0 21.098 13.805 39.508 33.539 45.727C103.891 466.746 129.828 488 160 488c4.617.0 9.227-.512 13.766-1.527C181.992 502 198.141 512 216 512c16.687.0 31.396-8.567 40-21.523V33.085z"/></g><g><g><path style="fill:#ffb980" d="M264 256c-4.422.0-8-3.582-8-8 0-22.055-17.945-40-40-40-8.008.0-15.734 2.355-22.336 6.812-3.023 2.043-7.055 1.781-9.797-.652-3.156-2.809-8.477-6.16-15.867-6.16-4.422.0-8-3.582-8-8s3.578-8 8-8c7.711.0 15.234 2.293 21.719 6.539C197.773 194.246 206.758 192 216 192c30.875.0 56 25.121 56 56C272 252.418 268.422 256 264 256z"/></g></g><g><g><path style="fill:#ffb980" d="M120 120c18.977.0 36.875 7.312 50.414 20.594 3.141 3.09 8.203 3.047 11.312-.109 3.094-3.152 3.047-8.219-.109-11.312C165.07 112.941 143.187 104 120 104c-13.046.0-25.395 2.93-36.542 8.046C81.253 117.019 80 122.423 80 128c0 1.316.078 2.637.234 3.992-.094.062-.173.139-.267.202C91.423 124.501 105.193 120 120 120z"/></g></g><g><g><path style="fill:#ffb980" d="M216 360c0-4.418-3.578-8-8-8s-8 3.582-8 8c0 17.645-14.352 32-32 32-14.211.0-26.82-9.648-30.664-23.465-.703-2.512-2.578-4.523-5.039-5.395-2.453-.871-5.188-.492-7.305 1.02C114.094 371.906 101.305 376 88 376c-6.948.0-13.625-1.149-19.894-3.207-2.214 4.939-3.501 10.19-3.916 15.586C71.714 390.73 79.711 392 88 392c13.297.0 26.187-3.266 37.773-9.52C133.969 397.894 150.141 408 168 408c26.469.0 48-21.531 48-48z"/></g></g><g><path style="fill:#fdc88e" d="M488 312c0-23.461-12.672-44.691-32.828-55.902.547-2.652.828-5.359.828-8.098.0-5.793-1.273-11.488-3.719-16.738C459.961 219.543 464 206.07 464 192c0-24.242-12.211-46.734-32.234-60.008.156-1.355.234-2.676.234-3.992.0-12.527-6.023-24.316-16-31.828.0-.059.0-.113.0-.172.0-29.023-19.711-54.398-47.406-61.816C357.57 13.293 335.781.0 312 0c-24.08.0-45.078 13.38-56 33.085v457.391C264.604 503.433 279.313 512 296 512c17.859.0 34.008-10 42.234-25.527C342.773 487.488 347.383 488 352 488c30.172.0 56.109-21.254 62.461-50.273C434.195 431.508 448 413.097 448 392c0-6.676-1.398-13.172-4.109-19.215C469.695 364.34 488 339.973 488 312z"/></g><g><path style="fill:#f8ab6b" d="M272.008 151.199C272 151.465 272 151.734 272 152c0 26.469 21.531 48 48 48s48-21.531 48-48c0-4.418-3.578-8-8-8s-8 3.582-8 8c0 17.645-14.352 32-32 32s-32-14.355-32-32c0-2.184.219-4.359.656-6.465.492-2.395-.133-4.883-1.703-6.754-1.57-1.871-4.016-3.066-6.352-2.859-.453.012-.891.059-.602.078-13.234.0-24-10.766-24-24v31.813C260.673 147.348 266.061 149.988 272.008 151.199z"/></g><g><path style="fill:#f8ab6b" d="M296 328c9.242.0 18.219-2.246 26.281-6.539C328.765 325.707 336.289 328 344 328c4.422.0 8-3.582 8-8s-3.578-8-8-8c-7.391.0-12.711-3.352-15.867-6.16-2.742-2.434-6.766-2.695-9.797-.656C311.726 309.644 304 312 296 312c-22.055.0-40-17.945-40-40v39.116C266.174 321.517 280.337 328 296 328z"/></g><g><g><path style="fill:#f8ab6b" d="M431.765 131.992c.156-1.355.234-2.676.234-3.992.0-5.577-1.253-10.981-3.458-15.954C417.395 106.93 405.046 104 392 104c-4.422.0-8 3.582-8 8s3.578 8 8 8c14.807.0 28.577 4.501 40.032 12.194C431.939 132.131 431.859 132.054 431.765 131.992z"/></g></g><g><g><path style="fill:#f8ab6b" d="M447.81 388.38c-.415-5.396-1.702-10.647-3.916-15.586C437.624 374.85 430.948 376 424 376c-13.578.0-26.594-4.266-37.641-12.332-2.07-1.5-4.719-1.93-7.133-1.168-2.43.77-4.344 2.648-5.164 5.059C369.101 382.176 355.414 392 340 392c-4.422.0-8 3.582-8 8s3.578 8 8 8c18.875.0 35.961-10.191 45.094-26.156C396.976 388.512 410.258 392 424 392 432.288 392 440.285 390.73 447.81 388.38z"/></g></g></g><g/><g/><g/><g/><g/><g/><g/><g/><g/><g/><g/><g/><g/><g/><g/></svg></a></li><li><a href=https://www.buymeacoffee.com/0x000216 target=_blank title=Coffee rel=me><svg id="Capa_1" xmlns:xlink="http://www.w3.org/1999/xlink" viewBox="0 0 340 340" style="enable-background:new 0 0 340 340"><g id="XMLID_18_"><polygon id="XMLID_138_" style="fill:#dedde0" points="76.429,290 80,340 170,340 170,290"/><polygon id="XMLID_169_" style="fill:#dedde0" points="170,80 61.429,80 65,130 170,130"/><polygon id="XMLID_197_" style="fill:#acabb1" points="170,290 170,340 260,340 263.571,290"/><polygon id="XMLID_221_" style="fill:#acabb1" points="170,80 170,130 275,130 278.571,80"/><path id="XMLID_222_" style="fill:#ffda44" d="M170 260c-22.091.0-40-22.386-40-50s17.909-50 40-50v-30H65 50l10 160h16.429H170V260z"/><path id="XMLID_33_" style="fill:#ff9811" d="M170 130v30c22.091.0 40 22.386 40 50s-17.909 50-40 50v30h93.571H280l10-160h-15H170z"/><path id="XMLID_223_" style="fill:#50412e" d="M210 210c0-27.614-17.909-50-40-50v1e2c22.091.0 40-22.386 40-50z"/><path id="XMLID_224_" style="fill:#786145" d="M130 210c0 27.614 17.909 50 40 50V160c-22.091.0-40 22.386-40 50z"/><polygon id="XMLID_225_" style="fill:#50412e" points="278.571,80 300,80 300,40 260,40 260,0 80,0 80,40 40,40 40,80 61.429,80 170,80"/></g><g/><g/><g/><g/><g/><g/><g/><g/><g/><g/><g/><g/><g/><g/><g/></svg></a></li></ol><ol class=menu id=main-menu><li><a href=/><svg class="icon icon-tabler icon-tabler-home" width="24" height="24" viewBox="0 0 24 24" stroke-width="2" stroke="currentcolor" fill="none" stroke-linecap="round" stroke-linejoin="round"><path stroke="none" d="M0 0h24v24H0z"/><polyline points="5 12 3 12 12 3 21 12 19 12"/><path d="M5 12v7a2 2 0 002 2h10a2 2 0 002-2v-7"/><path d="M9 21v-6a2 2 0 012-2h2a2 2 0 012 2v6"/></svg>
<span>Home</span></a></li><li><a href=/about/><svg class="icon icon-tabler icon-tabler-user" width="24" height="24" viewBox="0 0 24 24" stroke-width="2" stroke="currentcolor" fill="none" stroke-linecap="round" stroke-linejoin="round"><path stroke="none" d="M0 0h24v24H0z"/><circle cx="12" cy="7" r="4"/><path d="M6 21v-2a4 4 0 014-4h4a4 4 0 014 4v2"/></svg>
<span>About Us</span></a></li><li><a href=/termsofservice/><svg class="icon icon-tabler icon-tabler-pencil" width="24" height="24" viewBox="0 0 24 24" stroke-width="2" stroke="currentcolor" fill="none" stroke-linecap="round" stroke-linejoin="round"><path stroke="none" d="M0 0h24v24H0z" fill="none"/><path d="M4 20h4L18.5 9.5a1.5 1.5.0 00-4-4L4 16v4"/><line x1="13.5" y1="6.5" x2="17.5" y2="10.5"/></svg>
<span>Terms Of Service</span></a></li><li><a href=/privacypolicy/><svg class="icon icon-tabler icon-tabler-archive" width="24" height="24" viewBox="0 0 24 24" stroke-width="2" stroke="currentcolor" fill="none" stroke-linecap="round" stroke-linejoin="round"><path stroke="none" d="M0 0h24v24H0z"/><rect x="3" y="4" width="18" height="4" rx="2"/><path d="M5 8v10a2 2 0 002 2h10a2 2 0 002-2V8"/><line x1="10" y1="12" x2="14" y2="12"/></svg>
<span>Privacy Policy</span></a></li><li><a href=/disclaimer/><svg class="icon icon-tabler icon-tabler-link" width="24" height="24" viewBox="0 0 24 24" stroke-width="2" stroke="currentcolor" fill="none" stroke-linecap="round" stroke-linejoin="round"><path stroke="none" d="M0 0h24v24H0z"/><path d="M10 14a3.5 3.5.0 005 0l4-4a3.5 3.5.0 00-5-5l-.5.5"/><path d="M14 10a3.5 3.5.0 00-5 0l-4 4a3.5 3.5.0 005 5l.5-.5"/></svg>
<span>Disclaimer</span></a></li><li><a href=/search/><svg class="icon icon-tabler icon-tabler-search" width="24" height="24" viewBox="0 0 24 24" stroke-width="2" stroke="currentcolor" fill="none" stroke-linecap="round" stroke-linejoin="round"><path stroke="none" d="M0 0h24v24H0z"/><circle cx="10" cy="10" r="7"/><line x1="21" y1="21" x2="15" y2="15"/></svg>
<span>Search</span></a></li><li><a href=/contact/><svg class="icon icon-tabler icon-tabler-mail" width="44" height="44" viewBox="0 0 24 24" stroke-width="1.5" stroke="currentcolor" fill="none" stroke-linecap="round" stroke-linejoin="round"><path stroke="none" d="M0 0h24v24H0z" fill="none"/><rect x="3" y="5" width="18" height="14" rx="2"/><polyline points="3 7 12 13 21 7"/></svg>
<span>Contact Us</span></a></li><li class=menu-bottom-section><ol class=menu><li id=dark-mode-toggle><svg class="icon icon-tabler icon-tabler-toggle-left" width="24" height="24" viewBox="0 0 24 24" stroke-width="2" stroke="currentcolor" fill="none" stroke-linecap="round" stroke-linejoin="round"><path stroke="none" d="M0 0h24v24H0z"/><circle cx="8" cy="12" r="2"/><rect x="2" y="6" width="20" height="12" rx="6"/></svg>
<svg class="icon icon-tabler icon-tabler-toggle-right" width="24" height="24" viewBox="0 0 24 24" stroke-width="2" stroke="currentcolor" fill="none" stroke-linecap="round" stroke-linejoin="round"><path stroke="none" d="M0 0h24v24H0z"/><circle cx="16" cy="12" r="2"/><rect x="2" y="6" width="20" height="12" rx="6"/></svg>
<span>Dark Mode</span></li></ol></li></ol></aside><aside class="sidebar right-sidebar sticky"><section class="widget archives"><div class=widget-icon><svg class="icon icon-tabler icon-tabler-hash" width="24" height="24" viewBox="0 0 24 24" stroke-width="2" stroke="currentcolor" fill="none" stroke-linecap="round" stroke-linejoin="round"><path stroke="none" d="M0 0h24v24H0z"/><line x1="5" y1="9" x2="19" y2="9"/><line x1="5" y1="15" x2="19" y2="15"/><line x1="11" y1="4" x2="7" y2="20"/><line x1="17" y1="4" x2="13" y2="20"/></svg></div><h2 class="widget-title section-title">Table of contents</h2><div class=widget--toc><nav id=TableOfContents><ol><li><a href=#fundamentals-of-reinforcement-learning>Fundamentals of Reinforcement Learning</a></li><li><a href=#setting-up-the-environment>Setting Up the Environment</a></li><li><a href=#deep-q-learning>Deep $Q$-learning</a></li><li><a href=#running-the-code>Running the Code</a></li><li><a href=#wrapping-up>Wrapping Up</a></li></ol></nav></div></section></aside><main class="main full-width"><article class=main-article><header class=article-header><div class=article-details><div class=article-title-wrapper><h2 class=article-title><a href=/exploring-reinforcement-learning-in-depth/>Exploring Reinforcement Learning in Depth</a></h2></div><footer class=article-time><div><svg class="icon icon-tabler icon-tabler-calendar-time" width="56" height="56" viewBox="0 0 24 24" stroke-width="2" stroke="currentcolor" fill="none" stroke-linecap="round" stroke-linejoin="round"><path stroke="none" d="M0 0h24v24H0z"/><path d="M11.795 21H5a2 2 0 01-2-2V7a2 2 0 012-2h12a2 2 0 012 2v4"/><circle cx="18" cy="18" r="4"/><path d="M15 3v4"/><path d="M7 3v4"/><path d="M3 11h16"/><path d="M18 16.496V18l1 1"/></svg>
<time class=article-time--published>May 15, 2024</time></div><div><svg class="icon icon-tabler icon-tabler-clock" width="24" height="24" viewBox="0 0 24 24" stroke-width="2" stroke="currentcolor" fill="none" stroke-linecap="round" stroke-linejoin="round"><path stroke="none" d="M0 0h24v24H0z"/><circle cx="12" cy="12" r="9"/><polyline points="12 7 12 12 15 15"/></svg>
<time class=article-time--reading>17 minute read</time></div></footer></div></header><section class=article-content><p>Let&rsquo;s explore reinforcement learning by tackling a practical problem using popular libraries such as TensorFlow, TensorBoard, Keras, and OpenAI Gym. We&rsquo;ll implement deep $Q$-learning to understand its workings. The code will run on a standard PC, leveraging all CPU cores thanks to TensorFlow&rsquo;s capabilities.</p><p>Our problem is the Mountain Car: Imagine a car on a one-dimensional track between two mountains. The goal is to drive up the right mountain (reaching a flag). However, the car&rsquo;s engine is weak and cannot climb the mountain directly. Success requires building momentum by driving back and forth.</p><div data-testid=image-container class="X3Bg-O9c _1xIzGpI7"><figure class=_1sK7TguO><img alt="Visual representation of the Mountain Car problem" src=https://uploads.toptal.io/blog/image/126561/toptal-blog-image-1530882217193-fdd6cbc8bf66b7f811a24cf0a07e7a41.gif loading=lazy decoding=async class=Djd3YO_2></figure></div><p>This problem is ideal for our purpose because it&rsquo;s simple enough for reinforcement learning to solve quickly on a single CPU core yet complex enough to demonstrate the core concepts.</p><p>First, we&rsquo;ll briefly overview reinforcement learning. We&rsquo;ll then define basic terms and frame our problem using them. Next, we&rsquo;ll delve into the deep $Q$-learning algorithm and implement it to solve the Mountain Car problem.</p><h2 id=fundamentals-of-reinforcement-learning>Fundamentals of Reinforcement Learning</h2><p>Reinforcement learning, simply put, is learning through trial and error. The central entity is an &ldquo;agent,&rdquo; which is our car in this scenario. The agent interacts with an environment by taking actions. For each action, it receives feedback in the form of a new observation and a reward. Actions leading to higher rewards are encouraged, hence the term &ldquo;reinforcement.&rdquo; This concept draws inspiration from observing how living beings learn.</p><p>The agent&rsquo;s interactions within the environment are depicted in the following diagram:</p><div data-testid=image-container class="X3Bg-O9c _1xIzGpI7"><figure class=_1sK7TguO><picture class=!contents data-testid=picture><source srcset="https://assets.toptal.io/images?url=https%3A%2F%2Fuploads.toptal.io%2Fblog%2Fimage%2F126562%2Ftoptal-blog-image-1530882293397-eef53b127852f6a1f04d4fa8e90a89a1.png&amp;width=360, https://assets.toptal.io/images?url=https%3A%2F%2Fuploads.toptal.io%2Fblog%2Fimage%2F126562%2Ftoptal-blog-image-1530882293397-eef53b127852f6a1f04d4fa8e90a89a1.png&amp;width=360&amp;dpr=2 2x" media="(max-width: 360px)"><source srcset="https://assets.toptal.io/images?url=https%3A%2F%2Fuploads.toptal.io%2Fblog%2Fimage%2F126562%2Ftoptal-blog-image-1530882293397-eef53b127852f6a1f04d4fa8e90a89a1.png&amp;width=480, https://assets.toptal.io/images?url=https%3A%2F%2Fuploads.toptal.io%2Fblog%2Fimage%2F126562%2Ftoptal-blog-image-1530882293397-eef53b127852f6a1f04d4fa8e90a89a1.png&amp;width=480&amp;dpr=2 2x" media="(min-width: 360.1px) and (max-width: 480px)"><source srcset="https://assets.toptal.io/images?url=https%3A%2F%2Fuploads.toptal.io%2Fblog%2Fimage%2F126562%2Ftoptal-blog-image-1530882293397-eef53b127852f6a1f04d4fa8e90a89a1.png&amp;width=768, https://assets.toptal.io/images?url=https%3A%2F%2Fuploads.toptal.io%2Fblog%2Fimage%2F126562%2Ftoptal-blog-image-1530882293397-eef53b127852f6a1f04d4fa8e90a89a1.png&amp;width=768&amp;dpr=2 2x" media="(min-width: 480.1px) and (max-width: 768px)"><source srcset="https://assets.toptal.io/images?url=https%3A%2F%2Fuploads.toptal.io%2Fblog%2Fimage%2F126562%2Ftoptal-blog-image-1530882293397-eef53b127852f6a1f04d4fa8e90a89a1.png&amp;width=1024, https://assets.toptal.io/images?url=https%3A%2F%2Fuploads.toptal.io%2Fblog%2Fimage%2F126562%2Ftoptal-blog-image-1530882293397-eef53b127852f6a1f04d4fa8e90a89a1.png&amp;width=1024&amp;dpr=2 2x" media="(min-width: 768.1px) and (max-width: 1024px)"><source srcset="https://assets.toptal.io/images?url=https%3A%2F%2Fuploads.toptal.io%2Fblog%2Fimage%2F126562%2Ftoptal-blog-image-1530882293397-eef53b127852f6a1f04d4fa8e90a89a1.png&amp;width=1440, https://assets.toptal.io/images?url=https%3A%2F%2Fuploads.toptal.io%2Fblog%2Fimage%2F126562%2Ftoptal-blog-image-1530882293397-eef53b127852f6a1f04d4fa8e90a89a1.png&amp;width=1440&amp;dpr=2 2x" media="(min-width: 1024.1px) and (max-width: 1440px)"><source srcset="https://assets.toptal.io/images?url=https%3A%2F%2Fuploads.toptal.io%2Fblog%2Fimage%2F126562%2Ftoptal-blog-image-1530882293397-eef53b127852f6a1f04d4fa8e90a89a1.png&amp;width=1920, https://assets.toptal.io/images?url=https%3A%2F%2Fuploads.toptal.io%2Fblog%2Fimage%2F126562%2Ftoptal-blog-image-1530882293397-eef53b127852f6a1f04d4fa8e90a89a1.png&amp;width=1920&amp;dpr=2 2x" media="(min-width: 1440.1px) and (max-width: 1920px)"><source srcset="https://assets.toptal.io/images?url=https%3A%2F%2Fuploads.toptal.io%2Fblog%2Fimage%2F126562%2Ftoptal-blog-image-1530882293397-eef53b127852f6a1f04d4fa8e90a89a1.png" media="(min-width: 1920.1px)"><img alt="Diagram of the interactions between agent and environment" src="https://assets.toptal.io/images?url=https%3A%2F%2Fuploads.toptal.io%2Fblog%2Fimage%2F126562%2Ftoptal-blog-image-1530882293397-eef53b127852f6a1f04d4fa8e90a89a1.png" loading=lazy decoding=async class=Djd3YO_2></picture></figure></div><p>The agent perceives an observation and receives a reward based on its action. It then chooses another action, leading to step two. The environment responds with potentially altered observations and rewards. This cycle repeats until a terminal state is reached, signaled by a &ldquo;done&rdquo; notification sent to the agent. The complete sequence of <strong>observations > actions > next_observations > rewards</strong> is called an episode (or trajectory).</p><p>Returning to our Mountain Car: The car represents our agent, while the one-dimensional mountainous terrain acts as the environment. The car&rsquo;s action is represented by a single number: a positive value signifies pushing the car right, and a negative value signifies pushing it left. The agent perceives its surroundings through observations: the car&rsquo;s X position and velocity. To guide the car to the mountaintop, we define the reward accordingly: The agent receives a -1 reward for each step where it hasn&rsquo;t reached the goal. Upon reaching the goal, the episode concludes. This setup effectively penalizes the agent for not being in the desired position, encouraging it to reach the goal as swiftly as possible. The agent aims to maximize its total reward, which is the sum of rewards earned within an episode. For instance, reaching the goal after 110 steps yields a total return of -110, considered a favorable outcome for the Mountain Car problem. Failure to reach the goal within 200 steps results in a -200 return.</p><p>With the problem formulated, we can employ algorithms capable of solving it efficiently (when properly configured). It&rsquo;s crucial to note that we don&rsquo;t explicitly instruct the agent on how to achieve the goal; we provide no hints or heuristics. The agent is left to discover its own winning strategy (policy).</p><h2 id=setting-up-the-environment>Setting Up the Environment</h2><p>Begin by downloading the entire tutorial code:</p><div class=highlight><div class=chroma><table class=lntable><tr><td class=lntd><pre tabindex=0 class=chroma><code><span class=lnt>1
</span><span class=lnt>2
</span></code></pre></td><td class=lntd><pre tabindex=0 class=chroma><code class=language-fallback data-lang=fallback><span class=line><span class=cl>git clone https://github.com/AdamStelmaszczyk/rl-tutorial
</span></span><span class=line><span class=cl>cd rl-tutorial
</span></span></code></pre></td></tr></table></div></div><p>Next, we&rsquo;ll install the necessary Python packages. To prevent conflicts with existing installations, we&rsquo;ll create a clean conda environment. If you haven&rsquo;t installed conda, please refer to <a class=link href=https://conda.io/docs/user-guide/install/index.html target=_blank rel=noopener>https://conda.io/docs/user-guide/install/index.html</a>.</p><p>To create the conda environment:</p><div class=highlight><div class=chroma><table class=lntable><tr><td class=lntd><pre tabindex=0 class=chroma><code><span class=lnt>1
</span></code></pre></td><td class=lntd><pre tabindex=0 class=chroma><code class=language-fallback data-lang=fallback><span class=line><span class=cl>conda create -n tutorial python=3.6.5 -y
</span></span></code></pre></td></tr></table></div></div><p>Activate it using:</p><div class=highlight><div class=chroma><table class=lntable><tr><td class=lntd><pre tabindex=0 class=chroma><code><span class=lnt>1
</span></code></pre></td><td class=lntd><pre tabindex=0 class=chroma><code class=language-fallback data-lang=fallback><span class=line><span class=cl>source activate tutorial
</span></span></code></pre></td></tr></table></div></div><p>You should now see <code>(tutorial)</code> near your shell prompt, indicating the activation of the &ldquo;tutorial&rdquo; conda environment. Execute all subsequent commands within this environment.</p><p>Install the required dependencies within the conda environment:</p><div class=highlight><div class=chroma><table class=lntable><tr><td class=lntd><pre tabindex=0 class=chroma><code><span class=lnt>1
</span></code></pre></td><td class=lntd><pre tabindex=0 class=chroma><code class=language-fallback data-lang=fallback><span class=line><span class=cl>pip install -r requirements.txt
</span></span></code></pre></td></tr></table></div></div><p>With the installation complete, let&rsquo;s execute some code. We don&rsquo;t need to implement the Mountain Car environment from scratch, as the OpenAI Gym library provides a ready-made implementation. Let&rsquo;s observe a random agent (an agent selecting random actions) in our environment:</p><div class=highlight><div class=chroma><table class=lntable><tr><td class=lntd><pre tabindex=0 class=chroma><code><span class=lnt> 1
</span><span class=lnt> 2
</span><span class=lnt> 3
</span><span class=lnt> 4
</span><span class=lnt> 5
</span><span class=lnt> 6
</span><span class=lnt> 7
</span><span class=lnt> 8
</span><span class=lnt> 9
</span><span class=lnt>10
</span><span class=lnt>11
</span><span class=lnt>12
</span><span class=lnt>13
</span><span class=lnt>14
</span><span class=lnt>15
</span><span class=lnt>16
</span><span class=lnt>17
</span><span class=lnt>18
</span><span class=lnt>19
</span><span class=lnt>20
</span><span class=lnt>21
</span></code></pre></td><td class=lntd><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=kn>import</span> <span class=nn>gym</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=n>env</span> <span class=o>=</span> <span class=n>gym</span><span class=o>.</span><span class=n>make</span><span class=p>(</span><span class=s1>&#39;MountainCar-v0&#39;</span><span class=p>)</span>
</span></span><span class=line><span class=cl><span class=n>done</span> <span class=o>=</span> <span class=kc>True</span>
</span></span><span class=line><span class=cl><span class=n>episode</span> <span class=o>=</span> <span class=mi>0</span>
</span></span><span class=line><span class=cl><span class=n>episode_return</span> <span class=o>=</span> <span class=mf>0.0</span>
</span></span><span class=line><span class=cl><span class=k>for</span> <span class=n>episode</span> <span class=ow>in</span> <span class=nb>range</span><span class=p>(</span><span class=mi>5</span><span class=p>):</span>
</span></span><span class=line><span class=cl>   <span class=k>for</span> <span class=n>step</span> <span class=ow>in</span> <span class=nb>range</span><span class=p>(</span><span class=mi>200</span><span class=p>):</span>
</span></span><span class=line><span class=cl>       <span class=k>if</span> <span class=n>done</span><span class=p>:</span>
</span></span><span class=line><span class=cl>           <span class=k>if</span> <span class=n>episode</span> <span class=o>&gt;</span> <span class=mi>0</span><span class=p>:</span>
</span></span><span class=line><span class=cl>               <span class=nb>print</span><span class=p>(</span><span class=s2>&#34;Episode return: &#34;</span><span class=p>,</span> <span class=n>episode_return</span><span class=p>)</span>
</span></span><span class=line><span class=cl>           <span class=n>obs</span> <span class=o>=</span> <span class=n>env</span><span class=o>.</span><span class=n>reset</span><span class=p>()</span>
</span></span><span class=line><span class=cl>           <span class=n>episode</span> <span class=o>+=</span> <span class=mi>1</span>
</span></span><span class=line><span class=cl>           <span class=n>episode_return</span> <span class=o>=</span> <span class=mf>0.0</span>
</span></span><span class=line><span class=cl>           <span class=n>env</span><span class=o>.</span><span class=n>render</span><span class=p>()</span>
</span></span><span class=line><span class=cl>       <span class=k>else</span><span class=p>:</span>
</span></span><span class=line><span class=cl>           <span class=n>obs</span> <span class=o>=</span> <span class=n>next_obs</span>
</span></span><span class=line><span class=cl>       <span class=n>action</span> <span class=o>=</span> <span class=n>env</span><span class=o>.</span><span class=n>action_space</span><span class=o>.</span><span class=n>sample</span><span class=p>()</span>
</span></span><span class=line><span class=cl>       <span class=n>next_obs</span><span class=p>,</span> <span class=n>reward</span><span class=p>,</span> <span class=n>done</span><span class=p>,</span> <span class=n>_</span> <span class=o>=</span> <span class=n>env</span><span class=o>.</span><span class=n>step</span><span class=p>(</span><span class=n>action</span><span class=p>)</span>
</span></span><span class=line><span class=cl>       <span class=n>episode_return</span> <span class=o>+=</span> <span class=n>reward</span>
</span></span><span class=line><span class=cl>       <span class=n>env</span><span class=o>.</span><span class=n>render</span><span class=p>()</span>
</span></span></code></pre></td></tr></table></div></div><p>This is the <code>see.py</code> file. Execute it using:</p><div class=highlight><div class=chroma><table class=lntable><tr><td class=lntd><pre tabindex=0 class=chroma><code><span class=lnt>1
</span></code></pre></td><td class=lntd><pre tabindex=0 class=chroma><code class=language-fallback data-lang=fallback><span class=line><span class=cl>python see.py
</span></span></code></pre></td></tr></table></div></div><p>You should see the car moving randomly back and forth. Each episode will last 200 steps, resulting in a total return of -200.</p><div data-testid=image-container class="X3Bg-O9c _1xIzGpI7"><figure class=_1sK7TguO><img alt="Graphic representation of the random agent" src=https://uploads.toptal.io/blog/image/126565/toptal-blog-image-1530883192219-b816bdcbfa175dfa55d23205dd21e9ac.gif loading=lazy decoding=async class=Djd3YO_2></figure></div><p>Now, we need to replace these random actions with a more effective strategy. Numerous algorithms can be employed; deep $Q$-learning is well-suited for this introductory tutorial. Understanding this method provides a strong foundation for exploring other approaches.</p><h2 id=deep-q-learning>Deep $Q$-learning</h2><p>We will be using the algorithm first presented in 2013 by Mnih et al. in <a class=link href=https://www.cs.toronto.edu/~vmnih/docs/dqn.pdf target=_blank rel=noopener>Playing Atari with Deep Reinforcement Learning</a> and refined two years later in <a class=link href=https://web.stanford.edu/class/psych209/Readings/MnihEtAlHassibis15NatureControlDeepRL.pdf target=_blank rel=noopener>Human-level control through deep reinforcement learning</a>. These papers have spawned many advancements in the field, including the current state-of-the-art algorithm <a class=link href=https://arxiv.org/abs/1710.02298 target=_blank rel=noopener>Rainbow</a> (2017):</p><div data-testid=image-container class="X3Bg-O9c _1xIzGpI7"><figure class=_1sK7TguO><picture class=!contents data-testid=picture><source srcset="https://assets.toptal.io/images?url=https%3A%2F%2Fuploads.toptal.io%2Fblog%2Fimage%2F126564%2Ftoptal-blog-image-1530882962169-c9e14a82465f5e662a4445fe37227796.png&amp;width=360, https://assets.toptal.io/images?url=https%3A%2F%2Fuploads.toptal.io%2Fblog%2Fimage%2F126564%2Ftoptal-blog-image-1530882962169-c9e14a82465f5e662a4445fe37227796.png&amp;width=360&amp;dpr=2 2x" media="(max-width: 360px)"><source srcset="https://assets.toptal.io/images?url=https%3A%2F%2Fuploads.toptal.io%2Fblog%2Fimage%2F126564%2Ftoptal-blog-image-1530882962169-c9e14a82465f5e662a4445fe37227796.png&amp;width=480, https://assets.toptal.io/images?url=https%3A%2F%2Fuploads.toptal.io%2Fblog%2Fimage%2F126564%2Ftoptal-blog-image-1530882962169-c9e14a82465f5e662a4445fe37227796.png&amp;width=480&amp;dpr=2 2x" media="(min-width: 360.1px) and (max-width: 480px)"><source srcset="https://assets.toptal.io/images?url=https%3A%2F%2Fuploads.toptal.io%2Fblog%2Fimage%2F126564%2Ftoptal-blog-image-1530882962169-c9e14a82465f5e662a4445fe37227796.png&amp;width=768, https://assets.toptal.io/images?url=https%3A%2F%2Fuploads.toptal.io%2Fblog%2Fimage%2F126564%2Ftoptal-blog-image-1530882962169-c9e14a82465f5e662a4445fe37227796.png&amp;width=768&amp;dpr=2 2x" media="(min-width: 480.1px) and (max-width: 768px)"><source srcset="https://assets.toptal.io/images?url=https%3A%2F%2Fuploads.toptal.io%2Fblog%2Fimage%2F126564%2Ftoptal-blog-image-1530882962169-c9e14a82465f5e662a4445fe37227796.png&amp;width=1024, https://assets.toptal.io/images?url=https%3A%2F%2Fuploads.toptal.io%2Fblog%2Fimage%2F126564%2Ftoptal-blog-image-1530882962169-c9e14a82465f5e662a4445fe37227796.png&amp;width=1024&amp;dpr=2 2x" media="(min-width: 768.1px) and (max-width: 1024px)"><source srcset="https://assets.toptal.io/images?url=https%3A%2F%2Fuploads.toptal.io%2Fblog%2Fimage%2F126564%2Ftoptal-blog-image-1530882962169-c9e14a82465f5e662a4445fe37227796.png&amp;width=1440, https://assets.toptal.io/images?url=https%3A%2F%2Fuploads.toptal.io%2Fblog%2Fimage%2F126564%2Ftoptal-blog-image-1530882962169-c9e14a82465f5e662a4445fe37227796.png&amp;width=1440&amp;dpr=2 2x" media="(min-width: 1024.1px) and (max-width: 1440px)"><source srcset="https://assets.toptal.io/images?url=https%3A%2F%2Fuploads.toptal.io%2Fblog%2Fimage%2F126564%2Ftoptal-blog-image-1530882962169-c9e14a82465f5e662a4445fe37227796.png&amp;width=1920, https://assets.toptal.io/images?url=https%3A%2F%2Fuploads.toptal.io%2Fblog%2Fimage%2F126564%2Ftoptal-blog-image-1530882962169-c9e14a82465f5e662a4445fe37227796.png&amp;width=1920&amp;dpr=2 2x" media="(min-width: 1440.1px) and (max-width: 1920px)"><source srcset="https://assets.toptal.io/images?url=https%3A%2F%2Fuploads.toptal.io%2Fblog%2Fimage%2F126564%2Ftoptal-blog-image-1530882962169-c9e14a82465f5e662a4445fe37227796.png" media="(min-width: 1920.1px)"><img alt="A graph representing the algorithm at work" src="https://assets.toptal.io/images?url=https%3A%2F%2Fuploads.toptal.io%2Fblog%2Fimage%2F126564%2Ftoptal-blog-image-1530882962169-c9e14a82465f5e662a4445fe37227796.png" loading=lazy decoding=async class=Djd3YO_2></picture></figure></div><p>Rainbow achieves superhuman performance in many Atari 2600 games. We&rsquo;ll concentrate on the fundamental DQN version with minimal enhancements to keep this tutorial concise.</p><p>A policy, often denoted as $π(s)$, is a function that outputs the probabilities of taking specific actions in a given state $s$. For instance, a random Mountain Car policy would always return a 50% chance for both left and right actions in any state. During gameplay, we sample from this policy (distribution) to determine the actual actions taken.</p><p>$Q$-learning (Q stands for Quality) refers to the action-value function, denoted as $Q_π(s, a)$. This function returns the expected total return when starting in state $s$, taking action $a$, and then following the policy $π$. The total return represents the sum of all rewards obtained during an episode.</p><p>Knowing the optimal $Q$-function, represented as $Q^*$, would make solving the game trivial. We would simply select actions associated with the highest $Q^*$ values, guaranteeing the highest possible return.</p><p>However, $Q^*$ is usually unknown. In such cases, we can approximate or &ldquo;learn&rdquo; it through interactions with the environment. This is the essence of &ldquo;$Q$-learning.&rdquo; The term &ldquo;deep&rdquo; comes from utilizing deep neural networks, versatile function approximators, to approximate this function. Deep neural networks used for this purpose are known as Deep Q-Networks (DQNs). In simpler environments (where the number of states is manageable), a table could be used instead of a neural network to represent the $Q$-function, leading to &ldquo;tabular $Q$-learning.&rdquo;</p><p>Our goal is to approximate the $Q^*$ function. The Bellman equation aids us in this endeavor:</p><p>[Q(s, a) = r + γ \space \textrm{max}_{a&rsquo;} Q(s&rsquo;, a&rsquo;)]</p><p>$s’$ represents the state reached after taking action $a$ in state $s$. $γ$ (gamma), often set to 0.99, is a discount factor (a hyperparameter). It assigns less weight to future rewards, as they are less certain than immediate rewards given our imperfect $Q$. The Bellman equation is pivotal in deep $Q$-learning. It states that the $Q$-value for a specific state and action equals the immediate reward $r$ plus the discounted maximum $Q$-value achievable from the next state $s’$. We select the action $a’$ that leads to the highest total return from $s’$.</p><p>Leveraging the Bellman equation, we can employ supervised learning to approximate $Q^*$. The $Q$-function is represented (parametrized) by the neural network weights, denoted as $θ$ (theta). A naive implementation might take a state and an action as input and output the corresponding $Q$-value. However, this becomes inefficient when we need to know the $Q$-values for all actions in a given state, requiring as many calls to the $Q$ function as there are actions. A more efficient approach is to input only the state and obtain $Q$-values for all possible actions as output. This allows us to retrieve all $Q$-values in a single forward pass.</p><p>We begin by initializing the $Q$ network with random weights. As the agent interacts with the environment, we collect numerous transitions or &ldquo;experiences.&rdquo; These are tuples of (state, action, next state, reward), often abbreviated as ($s$, $a$, $s’$, $r$). We store thousands of these experiences in a ring buffer known as the &ldquo;experience replay.&rdquo; Experiences are then sampled from this buffer, aiming to fulfill the Bellman equation. Although we could process experiences one by one (online or on-policy learning), consecutive experiences tend to be highly correlated, hindering DQN training. The experience replay (offline or off-policy learning) mitigates this issue by disrupting data correlation. The <code>replay_buffer.py</code> file contains a straightforward ring buffer implementation that is worth examining.</p><p>Initially, with random neural network weights, the left-hand side of the Bellman equation will deviate significantly from the right-hand side. We use the squared difference as our loss function and minimize it by adjusting the neural network weights $θ$. The loss function can be expressed as:</p><p>[L(θ) = [Q(s, a) - r - γ \space \textrm{max}_{a&rsquo;}Q(s&rsquo;, a&rsquo;)]^2]</p><p>This is simply a rearrangement of the Bellman equation. For example, suppose we sample the experience ($s$, left, $s’$, -1) from the Mountain Car experience replay. We pass state $s$ through our $Q$ network, and let&rsquo;s say it outputs -120 for the left action, so $Q(s, \textrm{left}) = -120$. Next, we input $s’$ to the network, yielding -130 for left and -122 for right. Clearly, the better action for $s’$ is right, resulting in $\textrm{max}_{a’}Q(s’, a’) = -122$. We know $r$, the actual reward, which was -1. This means our $Q$-network&rsquo;s prediction was slightly off, as $L(θ) = [-120 - 1 + 0.99 ⋅ 122]^2 = (-0.22^2) = 0.0484$. We then backpropagate the error to adjust the weights $θ$. If we recalculate the loss for the same experience, it would now be lower.</p><p>One crucial observation before diving into the code: Updating our DQN involves two forward passes through the same network, which can lead to unstable learning. To address this, we use a slightly older version of the network, referred to as <code>target_model</code> (as opposed to the main DQN, <code>model</code>), to predict the next state $Q$ values. This provides a stable target. We synchronize <code>target_model</code> with the weights of <code>model</code> every 1000 steps, while <code>model</code> is updated after each step.</p><p>Let&rsquo;s examine the code responsible for creating the DQN model:</p><div class=highlight><div class=chroma><table class=lntable><tr><td class=lntd><pre tabindex=0 class=chroma><code><span class=lnt> 1
</span><span class=lnt> 2
</span><span class=lnt> 3
</span><span class=lnt> 4
</span><span class=lnt> 5
</span><span class=lnt> 6
</span><span class=lnt> 7
</span><span class=lnt> 8
</span><span class=lnt> 9
</span><span class=lnt>10
</span><span class=lnt>11
</span><span class=lnt>12
</span><span class=lnt>13
</span></code></pre></td><td class=lntd><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=k>def</span> <span class=nf>create_model</span><span class=p>(</span><span class=n>env</span><span class=p>):</span>
</span></span><span class=line><span class=cl>   <span class=n>n_actions</span> <span class=o>=</span> <span class=n>env</span><span class=o>.</span><span class=n>action_space</span><span class=o>.</span><span class=n>n</span>
</span></span><span class=line><span class=cl>   <span class=n>obs_shape</span> <span class=o>=</span> <span class=n>env</span><span class=o>.</span><span class=n>observation_space</span><span class=o>.</span><span class=n>shape</span>
</span></span><span class=line><span class=cl>   <span class=n>observations_input</span> <span class=o>=</span> <span class=n>keras</span><span class=o>.</span><span class=n>layers</span><span class=o>.</span><span class=n>Input</span><span class=p>(</span><span class=n>obs_shape</span><span class=p>,</span> <span class=n>name</span><span class=o>=</span><span class=s1>&#39;observations_input&#39;</span><span class=p>)</span>
</span></span><span class=line><span class=cl>   <span class=n>action_mask</span> <span class=o>=</span> <span class=n>keras</span><span class=o>.</span><span class=n>layers</span><span class=o>.</span><span class=n>Input</span><span class=p>((</span><span class=n>n_actions</span><span class=p>,),</span> <span class=n>name</span><span class=o>=</span><span class=s1>&#39;action_mask&#39;</span><span class=p>)</span>
</span></span><span class=line><span class=cl>   <span class=n>hidden</span> <span class=o>=</span> <span class=n>keras</span><span class=o>.</span><span class=n>layers</span><span class=o>.</span><span class=n>Dense</span><span class=p>(</span><span class=mi>32</span><span class=p>,</span> <span class=n>activation</span><span class=o>=</span><span class=s1>&#39;relu&#39;</span><span class=p>)(</span><span class=n>observations_input</span><span class=p>)</span>
</span></span><span class=line><span class=cl>   <span class=n>hidden_2</span> <span class=o>=</span> <span class=n>keras</span><span class=o>.</span><span class=n>layers</span><span class=o>.</span><span class=n>Dense</span><span class=p>(</span><span class=mi>32</span><span class=p>,</span> <span class=n>activation</span><span class=o>=</span><span class=s1>&#39;relu&#39;</span><span class=p>)(</span><span class=n>hidden</span><span class=p>)</span>
</span></span><span class=line><span class=cl>   <span class=n>output</span> <span class=o>=</span> <span class=n>keras</span><span class=o>.</span><span class=n>layers</span><span class=o>.</span><span class=n>Dense</span><span class=p>(</span><span class=n>n_actions</span><span class=p>)(</span><span class=n>hidden_2</span><span class=p>)</span>
</span></span><span class=line><span class=cl>   <span class=n>filtered_output</span> <span class=o>=</span> <span class=n>keras</span><span class=o>.</span><span class=n>layers</span><span class=o>.</span><span class=n>multiply</span><span class=p>([</span><span class=n>output</span><span class=p>,</span> <span class=n>action_mask</span><span class=p>])</span>
</span></span><span class=line><span class=cl>   <span class=n>model</span> <span class=o>=</span> <span class=n>keras</span><span class=o>.</span><span class=n>models</span><span class=o>.</span><span class=n>Model</span><span class=p>([</span><span class=n>observations_input</span><span class=p>,</span> <span class=n>action_mask</span><span class=p>],</span> <span class=n>filtered_output</span><span class=p>)</span>
</span></span><span class=line><span class=cl>   <span class=n>optimizer</span> <span class=o>=</span> <span class=n>keras</span><span class=o>.</span><span class=n>optimizers</span><span class=o>.</span><span class=n>Adam</span><span class=p>(</span><span class=n>lr</span><span class=o>=</span><span class=n>LEARNING_RATE</span><span class=p>,</span> <span class=n>clipnorm</span><span class=o>=</span><span class=mf>1.0</span><span class=p>)</span>
</span></span><span class=line><span class=cl>   <span class=n>model</span><span class=o>.</span><span class=n>compile</span><span class=p>(</span><span class=n>optimizer</span><span class=p>,</span> <span class=n>loss</span><span class=o>=</span><span class=s1>&#39;mean_squared_error&#39;</span><span class=p>)</span>
</span></span><span class=line><span class=cl>   <span class=k>return</span> <span class=n>model</span>
</span></span></code></pre></td></tr></table></div></div><p>This function begins by retrieving the action and observation space dimensions from the OpenAI Gym environment. This information is crucial, determining factors such as the number of outputs for our network (equal to the number of actions). Actions are one-hot encoded:</p><div class=highlight><div class=chroma><table class=lntable><tr><td class=lntd><pre tabindex=0 class=chroma><code><span class=lnt>1
</span><span class=lnt>2
</span><span class=lnt>3
</span><span class=lnt>4
</span></code></pre></td><td class=lntd><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=k>def</span> <span class=nf>one_hot_encode</span><span class=p>(</span><span class=n>n</span><span class=p>,</span> <span class=n>action</span><span class=p>):</span>
</span></span><span class=line><span class=cl>   <span class=n>one_hot</span> <span class=o>=</span> <span class=n>np</span><span class=o>.</span><span class=n>zeros</span><span class=p>(</span><span class=n>n</span><span class=p>)</span>
</span></span><span class=line><span class=cl>   <span class=n>one_hot</span><span class=p>[</span><span class=nb>int</span><span class=p>(</span><span class=n>action</span><span class=p>)]</span> <span class=o>=</span> <span class=mi>1</span>
</span></span><span class=line><span class=cl>   <span class=k>return</span> <span class=n>one_hot</span>
</span></span></code></pre></td></tr></table></div></div><p>For example, left becomes [1, 0] and right becomes [0, 1].</p><p>Observe that we input the observations. We also provide <code>action_mask</code> as a second input. This is because, when calculating $Q(s,a)$, we are only interested in the $Q$-value for a single specific action, not all of them. <code>action_mask</code> contains a 1 for the action whose $Q$-value we want to retrieve from the DQN output. If <code>action_mask</code> has a 0 for a particular action, its corresponding $Q$-value is zeroed out at the output. The <code>filtered_output</code> layer handles this. When we need all $Q$-values (for the max calculation), we can simply pass a vector of ones.</p><p>The code utilizes <code>keras.layers.Dense</code> to define fully connected layers. Keras is a Python library that provides a higher-level abstraction over TensorFlow. Behind the scenes, Keras constructs a TensorFlow graph with biases, weight initialization, and other low-level details. While we could have used raw TensorFlow to define the graph, Keras simplifies this process significantly.</p><p>Observations are passed through the first hidden layer, which uses ReLU (rectified linear unit) activations. <code>ReLU(x)</code> is simply the $\textrm{max}(0, x)$ function. This layer is fully connected to a second identical layer, <code>hidden_2</code>. Finally, the output layer reduces the number of neurons to match the number of actions. The <code>filtered_output</code> layer then multiplies the output by <code>action_mask</code>.</p><p>To determine the optimal $θ$ weights, we employ the Adam optimizer with a mean squared error loss function.</p><p>With our model in place, we can predict $Q$ values for given state observations:</p><div class=highlight><div class=chroma><table class=lntable><tr><td class=lntd><pre tabindex=0 class=chroma><code><span class=lnt>1
</span><span class=lnt>2
</span><span class=lnt>3
</span></code></pre></td><td class=lntd><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=k>def</span> <span class=nf>predict</span><span class=p>(</span><span class=n>env</span><span class=p>,</span> <span class=n>model</span><span class=p>,</span> <span class=n>observations</span><span class=p>):</span>
</span></span><span class=line><span class=cl>   <span class=n>action_mask</span> <span class=o>=</span> <span class=n>np</span><span class=o>.</span><span class=n>ones</span><span class=p>((</span><span class=nb>len</span><span class=p>(</span><span class=n>observations</span><span class=p>),</span> <span class=n>env</span><span class=o>.</span><span class=n>action_space</span><span class=o>.</span><span class=n>n</span><span class=p>))</span>
</span></span><span class=line><span class=cl>   <span class=k>return</span> <span class=n>model</span><span class=o>.</span><span class=n>predict</span><span class=p>(</span><span class=n>x</span><span class=o>=</span><span class=p>[</span><span class=n>observations</span><span class=p>,</span> <span class=n>action_mask</span><span class=p>])</span>
</span></span></code></pre></td></tr></table></div></div><p>Here, we require $Q$-values for all actions, hence <code>action_mask</code> is a vector of ones.</p><p>The actual training is performed using the <code>fit_batch()</code> function:</p><div class=highlight><div class=chroma><table class=lntable><tr><td class=lntd><pre tabindex=0 class=chroma><code><span class=lnt> 1
</span><span class=lnt> 2
</span><span class=lnt> 3
</span><span class=lnt> 4
</span><span class=lnt> 5
</span><span class=lnt> 6
</span><span class=lnt> 7
</span><span class=lnt> 8
</span><span class=lnt> 9
</span><span class=lnt>10
</span><span class=lnt>11
</span><span class=lnt>12
</span><span class=lnt>13
</span><span class=lnt>14
</span><span class=lnt>15
</span><span class=lnt>16
</span></code></pre></td><td class=lntd><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=k>def</span> <span class=nf>fit_batch</span><span class=p>(</span><span class=n>env</span><span class=p>,</span> <span class=n>model</span><span class=p>,</span> <span class=n>target_model</span><span class=p>,</span> <span class=n>batch</span><span class=p>):</span>
</span></span><span class=line><span class=cl>   <span class=n>observations</span><span class=p>,</span> <span class=n>actions</span><span class=p>,</span> <span class=n>rewards</span><span class=p>,</span> <span class=n>next_observations</span><span class=p>,</span> <span class=n>dones</span> <span class=o>=</span> <span class=n>batch</span>
</span></span><span class=line><span class=cl>   <span class=c1># Predict the Q values of the next states. Passing ones as the action mask.</span>
</span></span><span class=line><span class=cl>   <span class=n>next_q_values</span> <span class=o>=</span> <span class=n>predict</span><span class=p>(</span><span class=n>env</span><span class=p>,</span> <span class=n>target_model</span><span class=p>,</span> <span class=n>next_observations</span><span class=p>)</span>
</span></span><span class=line><span class=cl>   <span class=c1># The Q values of terminal states is 0 by definition.</span>
</span></span><span class=line><span class=cl>   <span class=n>next_q_values</span><span class=p>[</span><span class=n>dones</span><span class=p>]</span> <span class=o>=</span> <span class=mf>0.0</span>
</span></span><span class=line><span class=cl>   <span class=c1># The Q values of each start state is the reward + gamma * the max next state Q value</span>
</span></span><span class=line><span class=cl>   <span class=n>q_values</span> <span class=o>=</span> <span class=n>rewards</span> <span class=o>+</span> <span class=n>DISCOUNT_FACTOR_GAMMA</span> <span class=o>*</span> <span class=n>np</span><span class=o>.</span><span class=n>max</span><span class=p>(</span><span class=n>next_q_values</span><span class=p>,</span> <span class=n>axis</span><span class=o>=</span><span class=mi>1</span><span class=p>)</span>
</span></span><span class=line><span class=cl>   <span class=n>one_hot_actions</span> <span class=o>=</span> <span class=n>np</span><span class=o>.</span><span class=n>array</span><span class=p>([</span><span class=n>one_hot_encode</span><span class=p>(</span><span class=n>env</span><span class=o>.</span><span class=n>action_space</span><span class=o>.</span><span class=n>n</span><span class=p>,</span> <span class=n>action</span><span class=p>)</span> <span class=k>for</span> <span class=n>action</span> <span class=ow>in</span> <span class=n>actions</span><span class=p>])</span>
</span></span><span class=line><span class=cl>   <span class=n>history</span> <span class=o>=</span> <span class=n>model</span><span class=o>.</span><span class=n>fit</span><span class=p>(</span>
</span></span><span class=line><span class=cl>       <span class=n>x</span><span class=o>=</span><span class=p>[</span><span class=n>observations</span><span class=p>,</span> <span class=n>one_hot_actions</span><span class=p>],</span>
</span></span><span class=line><span class=cl>       <span class=n>y</span><span class=o>=</span><span class=n>one_hot_actions</span> <span class=o>*</span> <span class=n>q_values</span><span class=p>[:,</span> <span class=kc>None</span><span class=p>],</span>
</span></span><span class=line><span class=cl>       <span class=n>batch_size</span><span class=o>=</span><span class=n>BATCH_SIZE</span><span class=p>,</span>
</span></span><span class=line><span class=cl>       <span class=n>verbose</span><span class=o>=</span><span class=mi>0</span><span class=p>,</span>
</span></span><span class=line><span class=cl>   <span class=p>)</span>
</span></span><span class=line><span class=cl>   <span class=k>return</span> <span class=n>history</span><span class=o>.</span><span class=n>history</span><span class=p>[</span><span class=s1>&#39;loss&#39;</span><span class=p>][</span><span class=mi>0</span><span class=p>]</span>
</span></span></code></pre></td></tr></table></div></div><p>Each batch contains <code>BATCH_SIZE</code> experiences. <code>next_q_values</code> represents $Q(s, a)$, while <code>q_values</code> represents $r + γ \space \textrm{max}_{a’}Q(s’, a’)$ from the Bellman equation. The actions taken are one-hot encoded and passed as <code>action_mask</code> during the <code>model.fit()</code> call. The variable $y$ typically represents the &ldquo;target&rdquo; in supervised learning, and we are setting it to <code>q_values</code>. The expression <code>q_values[:. None]</code> increases the array dimension to match the dimension of <code>one_hot_actions</code>. This is known as slice notation.</p><p>We return the loss value for logging in the TensorBoard log file, allowing us to visualize the training progress. Besides the loss, we also monitor other metrics such as steps per second, total RAM usage, and average episode return.</p><h2 id=running-the-code>Running the Code</h2><p>Before visualizing the TensorBoard log file, we need to generate it. Let&rsquo;s begin training:</p><div class=highlight><div class=chroma><table class=lntable><tr><td class=lntd><pre tabindex=0 class=chroma><code><span class=lnt>1
</span></code></pre></td><td class=lntd><pre tabindex=0 class=chroma><code class=language-fallback data-lang=fallback><span class=line><span class=cl>python run.py
</span></span></code></pre></td></tr></table></div></div><p>This command first displays a summary of our model. It then creates a log directory named after the current date and initiates the training process. Every 2000 steps, a logline similar to this will be printed:</p><div class=highlight><div class=chroma><table class=lntable><tr><td class=lntd><pre tabindex=0 class=chroma><code><span class=lnt>1
</span></code></pre></td><td class=lntd><pre tabindex=0 class=chroma><code class=language-fallback data-lang=fallback><span class=line><span class=cl>episode 10 steps 200/2001 loss 0.3346639 return -200.0 in 1.02s 195.7 steps/s 9.0/15.6 GB RAM
</span></span></code></pre></td></tr></table></div></div><p>Every 20,000 steps, we evaluate the model&rsquo;s performance over 10,000 steps:</p><div class=highlight><div class=chroma><table class=lntable><tr><td class=lntd><pre tabindex=0 class=chroma><code><span class=lnt>1
</span><span class=lnt>2
</span><span class=lnt>3
</span></code></pre></td><td class=lntd><pre tabindex=0 class=chroma><code class=language-fallback data-lang=fallback><span class=line><span class=cl>Evaluation
</span></span><span class=line><span class=cl>100%|█████████████████████████████████████████████████████████████████████████████████| 10000/10000 [00:07&lt;00:00, 1254.40it/s]
</span></span><span class=line><span class=cl>episode 677 step 120000 episode_return_avg -136.750 avg_max_q_value -56.004
</span></span></code></pre></td></tr></table></div></div><p>After 677 episodes and 120,000 steps, the average episode return improved from -200 to -136.75, indicating that our agent is learning. Understanding the meaning of <code>avg_max_q_value</code> is left as a beneficial exercise. It&rsquo;s a valuable statistic to observe during training.</p><p>The training concludes after 200,000 steps, taking approximately 20 minutes on a four-core CPU. Within the <code>date-log</code> directory (e.g., <code>06-07-18-39-log</code>), you&rsquo;ll find four model files with the <code>.h5</code> extension. These files store snapshots of the TensorFlow graph weights, saved every 50,000 steps. They allow us to examine the learned policy later. To view it:</p><div class=highlight><div class=chroma><table class=lntable><tr><td class=lntd><pre tabindex=0 class=chroma><code><span class=lnt>1
</span></code></pre></td><td class=lntd><pre tabindex=0 class=chroma><code class=language-fallback data-lang=fallback><span class=line><span class=cl>python run.py --model 06-08-18-42-log/06-08-18-42-200000.h5 --view
</span></span></code></pre></td></tr></table></div></div><p>For additional options, use: <code>python run.py --help</code>.</p><p>Now, the car should navigate the track much more effectively, reaching the goal more consistently. Inside the <code>date-log</code> directory, there&rsquo;s also an <code>events.out.*</code> file, where TensorBoard stores its data. We write to this file using the basic <code>TensorBoardLogger</code> defined in <code>loggers.py.</code> To visualize the events file, we need to launch a local TensorBoard server:</p><div class=highlight><div class=chroma><table class=lntable><tr><td class=lntd><pre tabindex=0 class=chroma><code><span class=lnt>1
</span></code></pre></td><td class=lntd><pre tabindex=0 class=chroma><code class=language-fallback data-lang=fallback><span class=line><span class=cl>tensorboard --logdir=.
</span></span></code></pre></td></tr></table></div></div><p>The <code>--logdir</code> flag points to the directory containing the date-log directories. In our case, it&rsquo;s the current directory, so we use <code>.</code>. TensorBoard will print the URL it&rsquo;s listening on. Open <a class=link href=http://127.0.0.1:6006 target=_blank rel=noopener>http://127.0.0.1:6006</a> in your web browser, and you should see eight plots resembling these:</p><div data-testid=image-container class="X3Bg-O9c _1xIzGpI7"><figure class=_1sK7TguO><picture class=!contents data-testid=picture><source srcset="https://assets.toptal.io/images?url=https%3A%2F%2Fuploads.toptal.io%2Fblog%2Fimage%2F126566%2Ftoptal-blog-image-1530883271409-36aae0a145b8160b092cff0c3bb34e39.png&amp;width=360, https://assets.toptal.io/images?url=https%3A%2F%2Fuploads.toptal.io%2Fblog%2Fimage%2F126566%2Ftoptal-blog-image-1530883271409-36aae0a145b8160b092cff0c3bb34e39.png&amp;width=360&amp;dpr=2 2x" media="(max-width: 360px)"><source srcset="https://assets.toptal.io/images?url=https%3A%2F%2Fuploads.toptal.io%2Fblog%2Fimage%2F126566%2Ftoptal-blog-image-1530883271409-36aae0a145b8160b092cff0c3bb34e39.png&amp;width=480, https://assets.toptal.io/images?url=https%3A%2F%2Fuploads.toptal.io%2Fblog%2Fimage%2F126566%2Ftoptal-blog-image-1530883271409-36aae0a145b8160b092cff0c3bb34e39.png&amp;width=480&amp;dpr=2 2x" media="(min-width: 360.1px) and (max-width: 480px)"><source srcset="https://assets.toptal.io/images?url=https%3A%2F%2Fuploads.toptal.io%2Fblog%2Fimage%2F126566%2Ftoptal-blog-image-1530883271409-36aae0a145b8160b092cff0c3bb34e39.png&amp;width=768, https://assets.toptal.io/images?url=https%3A%2F%2Fuploads.toptal.io%2Fblog%2Fimage%2F126566%2Ftoptal-blog-image-1530883271409-36aae0a145b8160b092cff0c3bb34e39.png&amp;width=768&amp;dpr=2 2x" media="(min-width: 480.1px) and (max-width: 768px)"><source srcset="https://assets.toptal.io/images?url=https%3A%2F%2Fuploads.toptal.io%2Fblog%2Fimage%2F126566%2Ftoptal-blog-image-1530883271409-36aae0a145b8160b092cff0c3bb34e39.png&amp;width=1024, https://assets.toptal.io/images?url=https%3A%2F%2Fuploads.toptal.io%2Fblog%2Fimage%2F126566%2Ftoptal-blog-image-1530883271409-36aae0a145b8160b092cff0c3bb34e39.png&amp;width=1024&amp;dpr=2 2x" media="(min-width: 768.1px) and (max-width: 1024px)"><source srcset="https://assets.toptal.io/images?url=https%3A%2F%2Fuploads.toptal.io%2Fblog%2Fimage%2F126566%2Ftoptal-blog-image-1530883271409-36aae0a145b8160b092cff0c3bb34e39.png&amp;width=1440, https://assets.toptal.io/images?url=https%3A%2F%2Fuploads.toptal.io%2Fblog%2Fimage%2F126566%2Ftoptal-blog-image-1530883271409-36aae0a145b8160b092cff0c3bb34e39.png&amp;width=1440&amp;dpr=2 2x" media="(min-width: 1024.1px) and (max-width: 1440px)"><source srcset="https://assets.toptal.io/images?url=https%3A%2F%2Fuploads.toptal.io%2Fblog%2Fimage%2F126566%2Ftoptal-blog-image-1530883271409-36aae0a145b8160b092cff0c3bb34e39.png&amp;width=1920, https://assets.toptal.io/images?url=https%3A%2F%2Fuploads.toptal.io%2Fblog%2Fimage%2F126566%2Ftoptal-blog-image-1530883271409-36aae0a145b8160b092cff0c3bb34e39.png&amp;width=1920&amp;dpr=2 2x" media="(min-width: 1440.1px) and (max-width: 1920px)"><source srcset="https://assets.toptal.io/images?url=https%3A%2F%2Fuploads.toptal.io%2Fblog%2Fimage%2F126566%2Ftoptal-blog-image-1530883271409-36aae0a145b8160b092cff0c3bb34e39.png" media="(min-width: 1920.1px)"><img alt="Sample plots" src="https://assets.toptal.io/images?url=https%3A%2F%2Fuploads.toptal.io%2Fblog%2Fimage%2F126566%2Ftoptal-blog-image-1530883271409-36aae0a145b8160b092cff0c3bb34e39.png" loading=lazy decoding=async class=Djd3YO_2></picture></figure></div><h2 id=wrapping-up>Wrapping Up</h2><p>The <code>train()</code> function handles the entire training process. First, it creates the model and initializes the replay buffer. Then, in a loop similar to the one in <code>see.py</code>, it interacts with the environment and stores experiences in the buffer. One key difference is that we follow an epsilon-greedy policy. While we could always select the best action according to the $Q$-function, this hinders exploration, which is crucial for overall performance. To enforce exploration, we choose random actions with epsilon probability:</p><div class=highlight><div class=chroma><table class=lntable><tr><td class=lntd><pre tabindex=0 class=chroma><code><span class=lnt> 1
</span><span class=lnt> 2
</span><span class=lnt> 3
</span><span class=lnt> 4
</span><span class=lnt> 5
</span><span class=lnt> 6
</span><span class=lnt> 7
</span><span class=lnt> 8
</span><span class=lnt> 9
</span><span class=lnt>10
</span><span class=lnt>11
</span></code></pre></td><td class=lntd><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=k>def</span> <span class=nf>greedy_action</span><span class=p>(</span><span class=n>env</span><span class=p>,</span> <span class=n>model</span><span class=p>,</span> <span class=n>observation</span><span class=p>):</span>
</span></span><span class=line><span class=cl>   <span class=n>next_q_values</span> <span class=o>=</span> <span class=n>predict</span><span class=p>(</span><span class=n>env</span><span class=p>,</span> <span class=n>model</span><span class=p>,</span> <span class=n>observations</span><span class=o>=</span><span class=p>[</span><span class=n>observation</span><span class=p>])</span>
</span></span><span class=line><span class=cl>   <span class=k>return</span> <span class=n>np</span><span class=o>.</span><span class=n>argmax</span><span class=p>(</span><span class=n>next_q_values</span><span class=p>)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=k>def</span> <span class=nf>epsilon_greedy_action</span><span class=p>(</span><span class=n>env</span><span class=p>,</span> <span class=n>model</span><span class=p>,</span> <span class=n>observation</span><span class=p>,</span> <span class=n>epsilon</span><span class=p>):</span>
</span></span><span class=line><span class=cl>   <span class=k>if</span> <span class=n>random</span><span class=o>.</span><span class=n>random</span><span class=p>()</span> <span class=o>&lt;</span> <span class=n>epsilon</span><span class=p>:</span>
</span></span><span class=line><span class=cl>       <span class=n>action</span> <span class=o>=</span> <span class=n>env</span><span class=o>.</span><span class=n>action_space</span><span class=o>.</span><span class=n>sample</span><span class=p>()</span>
</span></span><span class=line><span class=cl>   <span class=k>else</span><span class=p>:</span>
</span></span><span class=line><span class=cl>       <span class=n>action</span> <span class=o>=</span> <span class=n>greedy_action</span><span class=p>(</span><span class=n>env</span><span class=p>,</span> <span class=n>model</span><span class=p>,</span> <span class=n>observation</span><span class=p>)</span>
</span></span><span class=line><span class=cl>   <span class=k>return</span> <span class=n>action</span>
</span></span></code></pre></td></tr></table></div></div><p>In this case, epsilon is set to 1%. Once 2000 experiences are collected, the replay buffer is sufficiently filled to commence training. We accomplish this by calling <code>fit_batch()</code> with a random batch of experiences sampled from the buffer:</p><div class=highlight><div class=chroma><table class=lntable><tr><td class=lntd><pre tabindex=0 class=chroma><code><span class=lnt>1
</span><span class=lnt>2
</span></code></pre></td><td class=lntd><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=n>batch</span> <span class=o>=</span> <span class=n>replay</span><span class=o>.</span><span class=n>sample</span><span class=p>(</span><span class=n>BATCH_SIZE</span><span class=p>)</span>
</span></span><span class=line><span class=cl><span class=n>loss</span> <span class=o>=</span> <span class=n>fit_batch</span><span class=p>(</span><span class=n>env</span><span class=p>,</span> <span class=n>model</span><span class=p>,</span> <span class=n>target_model</span><span class=p>,</span> <span class=n>batch</span><span class=p>)</span>
</span></span></code></pre></td></tr></table></div></div><p>We evaluate the model&rsquo;s performance and log the results every 20,000 steps (evaluation uses <code>epsilon = 0</code>, a purely greedy policy):</p><div class=highlight><div class=chroma><table class=lntable><tr><td class=lntd><pre tabindex=0 class=chroma><code><span class=lnt> 1
</span><span class=lnt> 2
</span><span class=lnt> 3
</span><span class=lnt> 4
</span><span class=lnt> 5
</span><span class=lnt> 6
</span><span class=lnt> 7
</span><span class=lnt> 8
</span><span class=lnt> 9
</span><span class=lnt>10
</span><span class=lnt>11
</span><span class=lnt>12
</span><span class=lnt>13
</span><span class=lnt>14
</span><span class=lnt>15
</span><span class=lnt>16
</span><span class=lnt>17
</span></code></pre></td><td class=lntd><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=k>if</span> <span class=n>step</span> <span class=o>&gt;=</span> <span class=n>TRAIN_START</span> <span class=ow>and</span> <span class=n>step</span> <span class=o>%</span> <span class=n>EVAL_EVERY</span> <span class=o>==</span> <span class=mi>0</span><span class=p>:</span>
</span></span><span class=line><span class=cl>   <span class=n>episode_return_avg</span> <span class=o>=</span> <span class=n>evaluate</span><span class=p>(</span><span class=n>env</span><span class=p>,</span> <span class=n>model</span><span class=p>)</span>
</span></span><span class=line><span class=cl>   <span class=n>q_values</span> <span class=o>=</span> <span class=n>predict</span><span class=p>(</span><span class=n>env</span><span class=p>,</span> <span class=n>model</span><span class=p>,</span> <span class=n>q_validation_observations</span><span class=p>)</span>
</span></span><span class=line><span class=cl>   <span class=n>max_q_values</span> <span class=o>=</span> <span class=n>np</span><span class=o>.</span><span class=n>max</span><span class=p>(</span><span class=n>q_values</span><span class=p>,</span> <span class=n>axis</span><span class=o>=</span><span class=mi>1</span><span class=p>)</span>
</span></span><span class=line><span class=cl>   <span class=n>avg_max_q_value</span> <span class=o>=</span> <span class=n>np</span><span class=o>.</span><span class=n>mean</span><span class=p>(</span><span class=n>max_q_values</span><span class=p>)</span>
</span></span><span class=line><span class=cl>   <span class=nb>print</span><span class=p>(</span>
</span></span><span class=line><span class=cl>       <span class=s2>&#34;episode </span><span class=si>{}</span><span class=s2> &#34;</span>
</span></span><span class=line><span class=cl>       <span class=s2>&#34;step </span><span class=si>{}</span><span class=s2> &#34;</span>
</span></span><span class=line><span class=cl>       <span class=s2>&#34;episode_return_avg </span><span class=si>{:.3f}</span><span class=s2> &#34;</span>
</span></span><span class=line><span class=cl>       <span class=s2>&#34;avg_max_q_value </span><span class=si>{:.3f}</span><span class=s2>&#34;</span><span class=o>.</span><span class=n>format</span><span class=p>(</span>
</span></span><span class=line><span class=cl>           <span class=n>episode</span><span class=p>,</span>
</span></span><span class=line><span class=cl>           <span class=n>step</span><span class=p>,</span>
</span></span><span class=line><span class=cl>           <span class=n>episode_return_avg</span><span class=p>,</span>
</span></span><span class=line><span class=cl>           <span class=n>avg_max_q_value</span><span class=p>,</span>
</span></span><span class=line><span class=cl>       <span class=p>))</span>
</span></span><span class=line><span class=cl>   <span class=n>logger</span><span class=o>.</span><span class=n>log_scalar</span><span class=p>(</span><span class=s1>&#39;episode_return_avg&#39;</span><span class=p>,</span> <span class=n>episode_return_avg</span><span class=p>,</span> <span class=n>step</span><span class=p>)</span>
</span></span><span class=line><span class=cl>   <span class=n>logger</span><span class=o>.</span><span class=n>log_scalar</span><span class=p>(</span><span class=s1>&#39;avg_max_q_value&#39;</span><span class=p>,</span> <span class=n>avg_max_q_value</span><span class=p>,</span> <span class=n>step</span><span class=p>)</span>
</span></span></code></pre></td></tr></table></div></div><p>The entire codebase consists of approximately 300 lines, with <code>run.py</code> containing roughly 250 of the most important ones.</p><p>You might have noticed the numerous hyperparameters involved:</p><div class=highlight><div class=chroma><table class=lntable><tr><td class=lntd><pre tabindex=0 class=chroma><code><span class=lnt> 1
</span><span class=lnt> 2
</span><span class=lnt> 3
</span><span class=lnt> 4
</span><span class=lnt> 5
</span><span class=lnt> 6
</span><span class=lnt> 7
</span><span class=lnt> 8
</span><span class=lnt> 9
</span><span class=lnt>10
</span><span class=lnt>11
</span><span class=lnt>12
</span><span class=lnt>13
</span><span class=lnt>14
</span></code></pre></td><td class=lntd><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=n>DISCOUNT_FACTOR_GAMMA</span> <span class=o>=</span> <span class=mf>0.99</span>
</span></span><span class=line><span class=cl><span class=n>LEARNING_RATE</span> <span class=o>=</span> <span class=mf>0.001</span>
</span></span><span class=line><span class=cl><span class=n>BATCH_SIZE</span> <span class=o>=</span> <span class=mi>64</span>
</span></span><span class=line><span class=cl><span class=n>TARGET_UPDATE_EVERY</span> <span class=o>=</span> <span class=mi>1000</span>
</span></span><span class=line><span class=cl><span class=n>TRAIN_START</span> <span class=o>=</span> <span class=mi>2000</span>
</span></span><span class=line><span class=cl><span class=n>REPLAY_BUFFER_SIZE</span> <span class=o>=</span> <span class=mi>50000</span>
</span></span><span class=line><span class=cl><span class=n>MAX_STEPS</span> <span class=o>=</span> <span class=mi>200000</span>
</span></span><span class=line><span class=cl><span class=n>LOG_EVERY</span> <span class=o>=</span> <span class=mi>2000</span>
</span></span><span class=line><span class=cl><span class=n>SNAPSHOT_EVERY</span> <span class=o>=</span> <span class=mi>50000</span>
</span></span><span class=line><span class=cl><span class=n>EVAL_EVERY</span> <span class=o>=</span> <span class=mi>20000</span>
</span></span><span class=line><span class=cl><span class=n>EVAL_STEPS</span> <span class=o>=</span> <span class=mi>10000</span>
</span></span><span class=line><span class=cl><span class=n>EVAL_EPSILON</span> <span class=o>=</span> <span class=mi>0</span>
</span></span><span class=line><span class=cl><span class=n>TRAIN_EPSILON</span> <span class=o>=</span> <span class=mf>0.01</span>
</span></span><span class=line><span class=cl><span class=n>Q_VALIDATION_SIZE</span> <span class=o>=</span> <span class=mi>10000</span>
</span></span></code></pre></td></tr></table></div></div><p>This isn&rsquo;t even an exhaustive list. Other factors, like network architecture, play a significant role as well. We used two hidden layers with 32 neurons, ReLU activations, and the Adam optimizer, but countless other configurations are possible. Even seemingly small adjustments can profoundly impact training. Fine-tuning hyperparameters often demands significant effort. In a recent OpenAI competition, the runner-up <a class=link href=http://www.noob-programmer.com/openai-retro-contest/how-to-score-6k-in-leaderboard/ target=_blank rel=noopener>found out</a> that it&rsquo;s possible to almost <strong>double</strong> Rainbow&rsquo;s score through meticulous hyperparameter optimization. However, it&rsquo;s crucial to be mindful of overfitting. Currently, reinforcement learning algorithms struggle with generalizing knowledge to similar environments. Our Mountain Car agent, for instance, might not perform well on all types of mountains. You can experiment with modifying the OpenAI Gym environment to assess the agent&rsquo;s generalization capabilities.</p><p>A worthwhile exercise is to try and discover a better set of hyperparameters than the ones used in this tutorial. However, judging the effectiveness of your changes based on a single training run is insufficient. Training runs often exhibit high variance, so multiple runs are necessary for meaningful comparisons. If you&rsquo;re interested in learning more about the critical topic of reproducibility in machine learning, refer to <a class=link href=https://arxiv.org/abs/1709.06560 target=_blank rel=noopener>Deep Reinforcement Learning that Matters</a>. Instead of manual tuning, we can partially automate the hyperparameter optimization process, albeit at the cost of increased computational resources. A simple approach is to define a promising range for each hyperparameter and then perform a grid search (testing various combinations) with parallel training runs. Parallelization itself is a vast topic crucial for achieving high performance.</p><p>Deep $Q$-learning belongs to a broader family of reinforcement learning algorithms that rely on value iteration. We focused on approximating the $Q$-function and primarily used it greedily. Another family of algorithms utilizes policy iteration, aiming to directly find the optimal policy $π^*$ without explicitly approximating the $Q$-function. To see where value iteration fits within the broader landscape of reinforcement learning algorithms, refer to:</p><div data-testid=image-container class="X3Bg-O9c _1xIzGpI7"><figure class=_1sK7TguO><picture class=!contents data-testid=picture><source srcset="https://assets.toptal.io/images?url=https%3A%2F%2Fuploads.toptal.io%2Fblog%2Fimage%2F126567%2Ftoptal-blog-image-1530883325885-1c16e03adae8c225170f2a5740fc8620.png&amp;width=360, https://assets.toptal.io/images?url=https%3A%2F%2Fuploads.toptal.io%2Fblog%2Fimage%2F126567%2Ftoptal-blog-image-1530883325885-1c16e03adae8c225170f2a5740fc8620.png&amp;width=360&amp;dpr=2 2x" media="(max-width: 360px)"><source srcset="https://assets.toptal.io/images?url=https%3A%2F%2Fuploads.toptal.io%2Fblog%2Fimage%2F126567%2Ftoptal-blog-image-1530883325885-1c16e03adae8c225170f2a5740fc8620.png&amp;width=480, https://assets.toptal.io/images?url=https%3A%2F%2Fuploads.toptal.io%2Fblog%2Fimage%2F126567%2Ftoptal-blog-image-1530883325885-1c16e03adae8c225170f2a5740fc8620.png&amp;width=480&amp;dpr=2 2x" media="(min-width: 360.1px) and (max-width: 480px)"><source srcset="https://assets.toptal.io/images?url=https%3A%2F%2Fuploads.toptal.io%2Fblog%2Fimage%2F126567%2Ftoptal-blog-image-1530883325885-1c16e03adae8c225170f2a5740fc8620.png&amp;width=768, https://assets.toptal.io/images?url=https%3A%2F%2Fuploads.toptal.io%2Fblog%2Fimage%2F126567%2Ftoptal-blog-image-1530883325885-1c16e03adae8c225170f2a5740fc8620.png&amp;width=768&amp;dpr=2 2x" media="(min-width: 480.1px) and (max-width: 768px)"><source srcset="https://assets.toptal.io/images?url=https%3A%2F%2Fuploads.toptal.io%2Fblog%2Fimage%2F126567%2Ftoptal-blog-image-1530883325885-1c16e03adae8c225170f2a5740fc8620.png&amp;width=1024, https://assets.toptal.io/images?url=https%3A%2F%2Fuploads.toptal.io%2Fblog%2Fimage%2F126567%2Ftoptal-blog-image-1530883325885-1c16e03adae8c225170f2a5740fc8620.png&amp;width=1024&amp;dpr=2 2x" media="(min-width: 768.1px) and (max-width: 1024px)"><source srcset="https://assets.toptal.io/images?url=https%3A%2F%2Fuploads.toptal.io%2Fblog%2Fimage%2F126567%2Ftoptal-blog-image-1530883325885-1c16e03adae8c225170f2a5740fc8620.png&amp;width=1440, https://assets.toptal.io/images?url=https%3A%2F%2Fuploads.toptal.io%2Fblog%2Fimage%2F126567%2Ftoptal-blog-image-1530883325885-1c16e03adae8c225170f2a5740fc8620.png&amp;width=1440&amp;dpr=2 2x" media="(min-width: 1024.1px) and (max-width: 1440px)"><source srcset="https://assets.toptal.io/images?url=https%3A%2F%2Fuploads.toptal.io%2Fblog%2Fimage%2F126567%2Ftoptal-blog-image-1530883325885-1c16e03adae8c225170f2a5740fc8620.png&amp;width=1920, https://assets.toptal.io/images?url=https%3A%2F%2Fuploads.toptal.io%2Fblog%2Fimage%2F126567%2Ftoptal-blog-image-1530883325885-1c16e03adae8c225170f2a5740fc8620.png&amp;width=1920&amp;dpr=2 2x" media="(min-width: 1440.1px) and (max-width: 1920px)"><source srcset="https://assets.toptal.io/images?url=https%3A%2F%2Fuploads.toptal.io%2Fblog%2Fimage%2F126567%2Ftoptal-blog-image-1530883325885-1c16e03adae8c225170f2a5740fc8620.png" media="(min-width: 1920.1px)"><img alt="Visual demonstration of the landscape of reinforcement learning algorithms" src="https://assets.toptal.io/images?url=https%3A%2F%2Fuploads.toptal.io%2Fblog%2Fimage%2F126567%2Ftoptal-blog-image-1530883325885-1c16e03adae8c225170f2a5740fc8620.png" loading=lazy decoding=async class=Djd3YO_2></picture></figure></div><p>You might be thinking that deep reinforcement learning appears brittle, and you wouldn&rsquo;t be wrong; it faces numerous challenges. For further reading on these challenges, see <a class=link href=https://www.alexirpan.com/2018/02/14/rl-hard.html target=_blank rel=noopener>Deep Reinforcement Learning Doesn’t Work Yet</a> and &ldquo;Reinforcement Learning never worked, and &lsquo;deep&rsquo; only helped a bit.&rdquo;</p><p>This concludes our tutorial. We implemented a basic DQN for educational purposes. <a class=link href=https://github.com/AdamStelmaszczyk/dqn target=_blank rel=noopener>Very similar code</a> can be used to achieve good performance in some Atari games. In practical applications, it&rsquo;s common to leverage well-tested, high-performance implementations, such as those found in <a class=link href=https://github.com/openai/baselines target=_blank rel=noopener>OpenAI baselines</a>. If you&rsquo;re interested in the hurdles of applying deep reinforcement learning to more complex environments, read <a class=link href=https://medium.com/@stelmaszczykadam/our-nips-2017-learning-to-run-approach-b80a295d3bb5 target=_blank rel=noopener>Our NIPS 2017: Learning to Run approach</a>. For a fun and competitive learning environment, explore platforms like <a class=link href=https://nips.cc/Conferences/2018/CompetitionTrack target=_blank rel=noopener>NIPS 2018 Competitions</a> or <a class=link href=https://www.crowdai.org/ target=_blank rel=noopener>crowdai.org</a>.</p><p>If your journey involves becoming a machine learning expert and you wish to expand your supervised learning expertise, consider exploring &ldquo;<em>Machine Learning Video Analysis: Identifying Fish</em>&rdquo; for an engaging fish identification experiment.</p></section><footer class=article-footer><section class=article-copyright><svg class="icon icon-tabler icon-tabler-copyright" width="24" height="24" viewBox="0 0 24 24" stroke-width="2" stroke="currentcolor" fill="none" stroke-linecap="round" stroke-linejoin="round"><path stroke="none" d="M0 0h24v24H0z"/><circle cx="12" cy="12" r="9"/><path d="M14.5 9a3.5 4 0 100 6"/></svg>
<span>Licensed under CC BY-NC-SA 4.0</span></section></footer></article><footer class=site-footer><script async src="https://pagead2.googlesyndication.com/pagead/js/adsbygoogle.js?client=ca-pub-3987358164777632" crossorigin=anonymous></script><script>(function(e,t,n,s,o){e[s]=e[s]||[],e[s].push({"gtm.start":(new Date).getTime(),event:"gtm.js"});var a=t.getElementsByTagName(n)[0],i=t.createElement(n),r=s!="dataLayer"?"&l="+s:"";i.async=!0,i.src="https://www.googletagmanager.com/gtm.js?id="+o+r,a.parentNode.insertBefore(i,a)})(window,document,"script","dataLayer","GTM-5X67X4Q4")</script><noscript><iframe src="https://www.googletagmanager.com/ns.html?id=GTM-5X67X4Q4" height=0 width=0 style=display:none;visibility:hidden></iframe></noscript></footer><div class=pswp tabindex=-1 role=dialog aria-hidden=true><div class=pswp__bg></div><div class=pswp__scroll-wrap><div class=pswp__container><div class=pswp__item></div><div class=pswp__item></div><div class=pswp__item></div></div><div class="pswp__ui pswp__ui--hidden"><div class=pswp__top-bar><div class=pswp__counter></div><button class="pswp__button pswp__button--close" title="Close (Esc)"></button>
<button class="pswp__button pswp__button--share" title=Share></button>
<button class="pswp__button pswp__button--fs" title="Toggle fullscreen"></button>
<button class="pswp__button pswp__button--zoom" title="Zoom in/out"></button><div class=pswp__preloader><div class=pswp__preloader__icn><div class=pswp__preloader__cut><div class=pswp__preloader__donut></div></div></div></div></div><div class="pswp__share-modal pswp__share-modal--hidden pswp__single-tap"><div class=pswp__share-tooltip></div></div><button class="pswp__button pswp__button--arrow--left" title="Previous (arrow left)">
</button>
<button class="pswp__button pswp__button--arrow--right" title="Next (arrow right)"></button><div class=pswp__caption><div class=pswp__caption__center></div></div></div></div></div><script src=https://cdn.jsdelivr.net/npm/photoswipe@4.1.3/dist/photoswipe.min.js integrity="sha256-ePwmChbbvXbsO02lbM3HoHbSHTHFAeChekF1xKJdleo=" crossorigin=anonymous defer></script><script src=https://cdn.jsdelivr.net/npm/photoswipe@4.1.3/dist/photoswipe-ui-default.min.js integrity="sha256-UKkzOn/w1mBxRmLLGrSeyB4e1xbrp4xylgAWb3M42pU=" crossorigin=anonymous defer></script><link rel=stylesheet href=https://cdn.jsdelivr.net/npm/photoswipe@4.1.3/dist/default-skin/default-skin.min.css crossorigin=anonymous><link rel=stylesheet href=https://cdn.jsdelivr.net/npm/photoswipe@4.1.3/dist/photoswipe.min.css crossorigin=anonymous></main></div><script src=https://cdn.jsdelivr.net/npm/node-vibrant@3.1.6/dist/vibrant.min.js integrity="sha256-awcR2jno4kI5X0zL8ex0vi2z+KMkF24hUW8WePSA9HM=" crossorigin=anonymous></script><script type=text/javascript src=/ts/main.js defer></script><script>(function(){const e=document.createElement("link");e.href="https://fonts.googleapis.com/css2?family=Lato:wght@300;400;700&display=swap",e.type="text/css",e.rel="stylesheet",document.head.appendChild(e)})()</script></body></html>