<!doctype html><html lang=en-us dir=ltr><head><meta charset=utf-8><meta http-equiv=x-ua-compatible content="IE=edge,chrome=1"><meta name=robots content="all"><meta name=robots content="index,follow"><meta name=googlebot content="index,follow"><meta name=bingbot content="index,follow"><link rel=sitemap type=application/xml title=Sitemap href=https://nexus-security.github.io/sitemap.xml><meta property="og:type" content="article"><meta name=author content="0x000216"><meta property="og:locale" content="en"><meta name=language content="English"><base href=https://Nexus-Security.github.io/posts/2016-09-09-data-structures-part-3-arrays-of-arrays/><link rel=icon href=/fav.ico type=image/x-icon><link rel=alternate type=application/rss+xml title=RSS href=https://nexus-security.github.io/index.xml><meta name=viewport content="width=device-width,initial-scale=1"><meta name=description content="Welcome to part 3 in this series covering all the data structures you really need (kind of).
In part 1, we saw that the best way to store most data is just to use a big old array and that with some tricks we can allocate this array so that the objects in it have fixed indices and permanent pointers.
In part 2, we saw how to fix the most glaring problem with these arrays — that it is kind of expensive to find things in them — by adding search indices that let us quickly find any subset of objects that we are interested in."><title>Data Structures Part 3: Arrays of Arrays</title><link rel=canonical href=https://Nexus-Security.github.io/posts/2016-09-09-data-structures-part-3-arrays-of-arrays/><link rel=stylesheet href=/scss/style.min.1f3200acb41251d94439982f0ccc8f6599efd31eaa9b4d6412b905bc3d5fa0b8.css><meta property="og:title" content="Data Structures Part 3: Arrays of Arrays"><meta property="og:description" content="Welcome to part 3 in this series covering all the data structures you really need (kind of).
In part 1, we saw that the best way to store most data is just to use a big old array and that with some tricks we can allocate this array so that the objects in it have fixed indices and permanent pointers.
In part 2, we saw how to fix the most glaring problem with these arrays — that it is kind of expensive to find things in them — by adding search indices that let us quickly find any subset of objects that we are interested in."><meta property="og:url" content="https://Nexus-Security.github.io/posts/2016-09-09-data-structures-part-3-arrays-of-arrays/"><meta property="og:site_name" content="0x000216"><meta property="og:type" content="article"><meta property="article:section" content="Posts"><meta property="article:published_time" content="2019-10-01T03:46:00+01:00"><meta property="article:modified_time" content="2019-10-01T03:46:00+01:00"><meta name=twitter:title content="Data Structures Part 3: Arrays of Arrays"><meta name=twitter:description content="Welcome to part 3 in this series covering all the data structures you really need (kind of).
In part 1, we saw that the best way to store most data is just to use a big old array and that with some tricks we can allocate this array so that the objects in it have fixed indices and permanent pointers.
In part 2, we saw how to fix the most glaring problem with these arrays — that it is kind of expensive to find things in them — by adding search indices that let us quickly find any subset of objects that we are interested in."><link rel="shortcut icon" href=/fav.ico></head><body class=article-page><script>(function(){const e="StackColorScheme";localStorage.getItem(e)||localStorage.setItem(e,"light")})()</script><script>(function(){const t="StackColorScheme",e=localStorage.getItem(t),n=window.matchMedia("(prefers-color-scheme: dark)").matches===!0;e=="dark"||e==="auto"&&n?document.documentElement.dataset.scheme="dark":document.documentElement.dataset.scheme="light"})()</script><div class="container main-container flex on-phone--column extended"><aside class="sidebar left-sidebar sticky"><button class="hamburger hamburger--spin" type=button id=toggle-menu aria-label="Toggle Menu">
<span class=hamburger-box><span class=hamburger-inner></span></span></button><header><center><figure class=site-avatar><a href=/><img src=/img/avatar_hue825486955cd7c56d95e38b4bd2a8e3c_229979_300x0_resize_box_3.png width=300 height=300 class=site-logo loading=lazy alt=Avatar></a></figure><div class=site-meta><h2 class=site-name><a href=/>0x000216</a></h1></center><h2 class=site-description>Where Do Russian Hackers Store Their Exploits? 🤓 /ussr/bin/ 😋</h2></div></header><ol class=social-menu><li><a href=https://github.com/Nexus-Security target=_blank title=GitHub><svg width="72" height="72" viewBox="0 0 72 72" xmlns="http://www.w3.org/2000/svg"><g fill="none" fill-rule="evenodd"><path d="M36 72c19.882251.0 36-16.117749 36-36 0-19.882251-16.117749-36-36-36-19.882251 365231026e-23-36 16.117749-36 36C24348735e-22 55.882251 16.117749 72 36 72z" fill="#3e75c3"/><path d="M35.9985 12C22.746 12 12 22.7870921 12 36.096644c0 10.6440272 6.876 19.6751861 16.4145 22.8617681C29.6145 59.1797862 30.0525 58.4358488 30.0525 57.7973276 30.0525 57.2250681 30.0315 55.7100863 30.0195 53.6996482c-6.6765 1.4562499-8.085-3.2302544-8.085-3.2302544-1.0905-2.7829884-2.664-3.5239139-2.664-3.5239139C17.091 45.4500754 19.4355 45.4801943 19.4355 45.4801943c2.4075.1701719 3.675 2.4833051 3.675 2.4833051 2.142 3.6820383 5.6175 2.6188404 6.9855 2.0014024C30.3135 48.4077535 30.9345 47.3460615 31.62 46.7436831 26.2905 46.1352808 20.688 44.0691228 20.688 34.8361671c0-2.6308879.9345-4.781379 2.4705-6.4665327C22.911 27.7597262 22.0875 25.3110578 23.3925 21.9934585c0 0 2.016-.6475568 6.6 2.4697516C31.908 23.9285993 33.96 23.6620468 36.0015 23.6515052 38.04 23.6620468 40.0935 23.9285993 42.0105 24.4632101c4.581-3.1173084 6.5925-2.4697516 6.5925-2.4697516C49.9125 25.3110578 49.089 27.7597262 48.8415 28.3696344 50.3805 30.0547881 51.309 32.2052792 51.309 34.8361671c0 9.2555448-5.6115 11.29309-10.9575 11.8894446.860999999999997.7439374 1.629 2.2137408 1.629 4.4621184C41.9805 54.4089489 41.9505 57.0067059 41.9505 57.7973276 41.9505 58.4418726 42.3825 59.1918338 43.6005 58.9554002 53.13 55.7627944 60 46.7376593 60 36.096644 60 22.7870921 49.254 12 35.9985 12" fill="#fff"/></g></svg></a></li><li><a href=mailto:0x000216@gmail.com target=_blank title=Email><svg id="Layer_1" xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink" viewBox="0 0 512 512" style="enable-background:new 0 0 512 512"><path style="fill:#e6f3ff" d="M512 105.739v300.522c0 27.715-22.372 50.087-50.087 50.087H50.087C22.372 456.348.0 433.976.0 406.261V105.739c0-.89.0-1.781.111-2.671 1.336-25.6 21.704-45.969 47.304-47.304.89-.111 1.781-.111 2.671-.111h411.826c.89.0 1.892.0 2.783.111 25.489 1.336 45.857 21.704 47.193 47.193C512 103.847 512 104.849 512 105.739z"/><path style="fill:#cfdbe6" d="M464.696 55.763c-.892-.111-1.891-.111-2.783-.111H256v400.696h205.913c27.715.0 50.087-22.372 50.087-50.087V105.739c0-.89.0-1.892-.111-2.783C510.553 77.468 490.184 57.099 464.696 55.763z"/><path style="fill:#ff4b26" d="M511.889 102.957c-1.336-25.489-21.704-45.857-47.193-47.193C382.89 137.569 336.951 183.509 256 264.459 225.291 233.732 77.61 85.958 47.416 55.763c-25.6 1.336-45.969 21.704-47.304 47.304C0 103.958.0 104.849.0 105.739v300.522c0 27.715 22.372 50.087 50.087 50.087h16.696V169.739l165.621 165.51c6.456 6.567 15.026 9.795 23.597 9.795 8.57.0 17.141-3.228 23.597-9.795l165.621-165.621v286.72h16.696c27.715.0 50.087-22.372 50.087-50.087V105.739C512 104.849 512 103.847 511.889 102.957z"/><path style="fill:#d93f21" d="M279.596 335.249l165.621-165.621v286.72h16.696c27.715.0 50.087-22.372 50.087-50.087V105.739c0-.89.0-1.892-.111-2.783-1.336-25.489-21.704-45.857-47.193-47.193C382.891 137.569 336.951 183.509 256 264.459v80.584C264.57 345.043 273.141 341.816 279.596 335.249z"/><g/><g/><g/><g/><g/><g/><g/><g/><g/><g/><g/><g/><g/><g/><g/></svg></a></li><li><a href=https://nexus-security.github.io/index.xml target=_blank title=RSS><svg id="Layer_1" xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink" viewBox="0 0 512 512" style="enable-background:new 0 0 512 512"><path style="fill:#f78c20" d="M78.333 355.334C35.14 355.334.0 390.474.0 433.667S35.14 512 78.333 512s78.333-35.14 78.333-78.333-35.14-78.333-78.333-78.333z"/><g><path style="fill:#ffa929" d="M78.333 381.445c-28.795.0-52.222 23.427-52.222 52.222s23.427 52.222 52.222 52.222 52.222-23.427 52.222-52.222-23.427-52.222-52.222-52.222z"/><path style="fill:#ffa929" d="M477.918 264.861c-21.843-51.641-53.111-98.019-92.936-137.842-39.824-39.824-86.201-71.093-137.843-92.935C193.669 11.468 136.874.0 78.333.0c-4.807.0-8.704 3.897-8.704 8.704v85.519c0 4.807 3.897 8.704 8.704 8.704 182.37.0 330.74 148.369 330.74 330.74.0 4.807 3.897 8.704 8.704 8.704h85.52c4.807.0 8.704-3.897 8.704-8.704C512 375.126 500.533 318.331 477.918 264.861z"/><path style="fill:#ffa929" d="M78.333 163.853c-4.807.0-8.704 3.897-8.704 8.704v95.74c0 4.807 3.897 8.704 8.704 8.704 86.386.0 156.666 70.281 156.666 156.666.0 4.807 3.897 8.704 8.704 8.704h95.74c4.807.0 8.704-3.897 8.704-8.704.0-72.07-28.065-139.826-79.027-190.787-50.961-50.961-118.717-79.027-190.787-79.027z"/></g><g><path style="fill:#f78c20" d="M78.333 242.186c-2.918.0-5.817.076-8.704.206v25.905c0 4.807 3.897 8.704 8.704 8.704 86.386.0 156.666 70.281 156.666 156.666.0 4.807 3.897 8.704 8.704 8.704h25.905c.129-2.886.206-5.786.206-8.704.0-105.752-85.729-191.481-191.481-191.481z"/><path style="fill:#f78c20" d="M78.333 68.113c-2.91.0-5.81.042-8.704.11v26.001c0 4.807 3.897 8.704 8.704 8.704 182.37.0 330.74 148.369 330.74 330.74.0 4.807 3.897 8.704 8.704 8.704h26.001c.067-2.894.11-5.793.11-8.704C443.887 231.777 280.223 68.113 78.333 68.113z"/></g><g/><g/><g/><g/><g/><g/><g/><g/><g/><g/><g/><g/><g/><g/><g/></svg></a></li><li><a href target=_blank title=Slack><svg width="256" height="256" viewBox="0 0 256 256" xmlns="http://www.w3.org/2000/svg" preserveAspectRatio="xMidYMid"><path d="M165.964 15.838c-3.89-11.975-16.752-18.528-28.725-14.636-11.975 3.89-18.528 16.752-14.636 28.725l58.947 181.365c4.048 11.187 16.132 17.473 27.732 14.135 12.1-3.483 19.475-16.334 15.614-28.217L165.964 15.838" fill="#dfa22f"/><path d="M74.626 45.516C70.734 33.542 57.873 26.989 45.9 30.879 33.924 34.77 27.37 47.631 31.263 59.606l58.948 181.366c4.047 11.186 16.132 17.473 27.732 14.132 12.099-3.481 19.474-16.332 15.613-28.217L74.626 45.516" fill="#3cb187"/><path d="M240.162 166.045c11.975-3.89 18.526-16.75 14.636-28.726-3.89-11.973-16.752-18.527-28.725-14.636L44.708 181.632c-11.187 4.046-17.473 16.13-14.135 27.73 3.483 12.099 16.334 19.475 28.217 15.614l181.372-58.93" fill="#ce1e5b"/><path d="M82.508 217.27l43.347-14.084-14.086-43.352-43.35 14.09 14.089 43.347" fill="#392538"/><path d="M173.847 187.591c16.388-5.323 31.62-10.273 43.348-14.084l-14.088-43.36-43.35 14.09 14.09 43.354" fill="#bb242a"/><path d="M210.484 74.706c11.974-3.89 18.527-16.751 14.637-28.727-3.89-11.973-16.752-18.526-28.727-14.636L15.028 90.293C3.842 94.337-2.445 106.422.896 118.022c3.481 12.098 16.332 19.474 28.217 15.613l181.371-58.93" fill="#72c5cd"/><path d="M52.822 125.933c11.805-3.836 27.025-8.782 43.354-14.086-5.323-16.39-10.273-31.622-14.084-43.352l-43.36 14.092 14.09 43.346" fill="#248c73"/><path d="M144.16 96.256l43.356-14.088a546179.21 546179.21.0 00-14.089-43.36L130.07 52.9l14.09 43.356" fill="#62803a"/></svg></a></li><li><a href=https://www.minds.com/0x000216/ target=_blank title=Minds><svg id="Capa_1" xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink" viewBox="0 0 512 512" style="enable-background:new 0 0 512 512"><g><g><path style="fill:#ffe1b2" d="M256 33.085C245.078 13.38 224.079.0 2e2.0c-23.781.0-45.57 13.293-56.594 34.184C115.711 41.602 96 66.977 96 96c0 .059.0.113.0.172-9.977 7.512-16 19.301-16 31.828.0 1.316.078 2.637.234 3.992C60.211 145.266 48 167.758 48 192c0 14.07 4.039 27.543 11.719 39.262C57.273 236.512 56 242.207 56 248c0 2.738.281 5.445.828 8.098C36.672 267.308 24 288.539 24 312c0 27.973 18.305 52.34 44.109 60.785C65.398 378.828 64 385.324 64 392c0 21.098 13.805 39.508 33.539 45.727C103.891 466.746 129.828 488 160 488c4.617.0 9.227-.512 13.766-1.527C181.992 502 198.141 512 216 512c16.687.0 31.396-8.567 40-21.523V33.085z"/></g><g><g><path style="fill:#ffb980" d="M264 256c-4.422.0-8-3.582-8-8 0-22.055-17.945-40-40-40-8.008.0-15.734 2.355-22.336 6.812-3.023 2.043-7.055 1.781-9.797-.652-3.156-2.809-8.477-6.16-15.867-6.16-4.422.0-8-3.582-8-8s3.578-8 8-8c7.711.0 15.234 2.293 21.719 6.539C197.773 194.246 206.758 192 216 192c30.875.0 56 25.121 56 56C272 252.418 268.422 256 264 256z"/></g></g><g><g><path style="fill:#ffb980" d="M120 120c18.977.0 36.875 7.312 50.414 20.594 3.141 3.09 8.203 3.047 11.312-.109 3.094-3.152 3.047-8.219-.109-11.312C165.07 112.941 143.187 104 120 104c-13.046.0-25.395 2.93-36.542 8.046C81.253 117.019 80 122.423 80 128c0 1.316.078 2.637.234 3.992-.094.062-.173.139-.267.202C91.423 124.501 105.193 120 120 120z"/></g></g><g><g><path style="fill:#ffb980" d="M216 360c0-4.418-3.578-8-8-8s-8 3.582-8 8c0 17.645-14.352 32-32 32-14.211.0-26.82-9.648-30.664-23.465-.703-2.512-2.578-4.523-5.039-5.395-2.453-.871-5.188-.492-7.305 1.02C114.094 371.906 101.305 376 88 376c-6.948.0-13.625-1.149-19.894-3.207-2.214 4.939-3.501 10.19-3.916 15.586C71.714 390.73 79.711 392 88 392c13.297.0 26.187-3.266 37.773-9.52C133.969 397.894 150.141 408 168 408c26.469.0 48-21.531 48-48z"/></g></g><g><path style="fill:#fdc88e" d="M488 312c0-23.461-12.672-44.691-32.828-55.902.547-2.652.828-5.359.828-8.098.0-5.793-1.273-11.488-3.719-16.738C459.961 219.543 464 206.07 464 192c0-24.242-12.211-46.734-32.234-60.008.156-1.355.234-2.676.234-3.992.0-12.527-6.023-24.316-16-31.828.0-.059.0-.113.0-.172.0-29.023-19.711-54.398-47.406-61.816C357.57 13.293 335.781.0 312 0c-24.08.0-45.078 13.38-56 33.085v457.391C264.604 503.433 279.313 512 296 512c17.859.0 34.008-10 42.234-25.527C342.773 487.488 347.383 488 352 488c30.172.0 56.109-21.254 62.461-50.273C434.195 431.508 448 413.097 448 392c0-6.676-1.398-13.172-4.109-19.215C469.695 364.34 488 339.973 488 312z"/></g><g><path style="fill:#f8ab6b" d="M272.008 151.199C272 151.465 272 151.734 272 152c0 26.469 21.531 48 48 48s48-21.531 48-48c0-4.418-3.578-8-8-8s-8 3.582-8 8c0 17.645-14.352 32-32 32s-32-14.355-32-32c0-2.184.219-4.359.656-6.465.492-2.395-.133-4.883-1.703-6.754-1.57-1.871-4.016-3.066-6.352-2.859-.453.012-.891.059-.602.078-13.234.0-24-10.766-24-24v31.813C260.673 147.348 266.061 149.988 272.008 151.199z"/></g><g><path style="fill:#f8ab6b" d="M296 328c9.242.0 18.219-2.246 26.281-6.539C328.765 325.707 336.289 328 344 328c4.422.0 8-3.582 8-8s-3.578-8-8-8c-7.391.0-12.711-3.352-15.867-6.16-2.742-2.434-6.766-2.695-9.797-.656C311.726 309.644 304 312 296 312c-22.055.0-40-17.945-40-40v39.116C266.174 321.517 280.337 328 296 328z"/></g><g><g><path style="fill:#f8ab6b" d="M431.765 131.992c.156-1.355.234-2.676.234-3.992.0-5.577-1.253-10.981-3.458-15.954C417.395 106.93 405.046 104 392 104c-4.422.0-8 3.582-8 8s3.578 8 8 8c14.807.0 28.577 4.501 40.032 12.194C431.939 132.131 431.859 132.054 431.765 131.992z"/></g></g><g><g><path style="fill:#f8ab6b" d="M447.81 388.38c-.415-5.396-1.702-10.647-3.916-15.586C437.624 374.85 430.948 376 424 376c-13.578.0-26.594-4.266-37.641-12.332-2.07-1.5-4.719-1.93-7.133-1.168-2.43.77-4.344 2.648-5.164 5.059C369.101 382.176 355.414 392 340 392c-4.422.0-8 3.582-8 8s3.578 8 8 8c18.875.0 35.961-10.191 45.094-26.156C396.976 388.512 410.258 392 424 392 432.288 392 440.285 390.73 447.81 388.38z"/></g></g></g><g/><g/><g/><g/><g/><g/><g/><g/><g/><g/><g/><g/><g/><g/><g/></svg></a></li><li><a href=https://www.buymeacoffee.com/0x000216 target=_blank title=Coffee><svg id="Capa_1" xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink" viewBox="0 0 340 340" style="enable-background:new 0 0 340 340"><g id="XMLID_18_"><polygon id="XMLID_138_" style="fill:#dedde0" points="76.429,290 80,340 170,340 170,290"/><polygon id="XMLID_169_" style="fill:#dedde0" points="170,80 61.429,80 65,130 170,130"/><polygon id="XMLID_197_" style="fill:#acabb1" points="170,290 170,340 260,340 263.571,290"/><polygon id="XMLID_221_" style="fill:#acabb1" points="170,80 170,130 275,130 278.571,80"/><path id="XMLID_222_" style="fill:#ffda44" d="M170 260c-22.091.0-40-22.386-40-50s17.909-50 40-50v-30H65 50l10 160h16.429H170V260z"/><path id="XMLID_33_" style="fill:#ff9811" d="M170 130v30c22.091.0 40 22.386 40 50s-17.909 50-40 50v30h93.571H280l10-160h-15H170z"/><path id="XMLID_223_" style="fill:#50412e" d="M210 210c0-27.614-17.909-50-40-50v1e2c22.091.0 40-22.386 40-50z"/><path id="XMLID_224_" style="fill:#786145" d="M130 210c0 27.614 17.909 50 40 50V160c-22.091.0-40 22.386-40 50z"/><polygon id="XMLID_225_" style="fill:#50412e" points="278.571,80 300,80 300,40 260,40 260,0 80,0 80,40 40,40 40,80 61.429,80 170,80"/></g><g/><g/><g/><g/><g/><g/><g/><g/><g/><g/><g/><g/><g/><g/><g/></svg></a></li></ol><ol class=menu id=main-menu><li><a href=/><svg xmlns="http://www.w3.org/2000/svg" class="icon icon-tabler icon-tabler-home" width="24" height="24" viewBox="0 0 24 24" stroke-width="2" stroke="currentcolor" fill="none" stroke-linecap="round" stroke-linejoin="round"><path stroke="none" d="M0 0h24v24H0z"/><polyline points="5 12 3 12 12 3 21 12 19 12"/><path d="M5 12v7a2 2 0 002 2h10a2 2 0 002-2v-7"/><path d="M9 21v-6a2 2 0 012-2h2a2 2 0 012 2v6"/></svg><span>Home</span></a></li><li><a href=/about/><svg xmlns="http://www.w3.org/2000/svg" class="icon icon-tabler icon-tabler-user" width="24" height="24" viewBox="0 0 24 24" stroke-width="2" stroke="currentcolor" fill="none" stroke-linecap="round" stroke-linejoin="round"><path stroke="none" d="M0 0h24v24H0z"/><circle cx="12" cy="7" r="4"/><path d="M6 21v-2a4 4 0 014-4h4a4 4 0 014 4v2"/></svg><span>About Us</span></a></li><li><a href=/termsofservice/><svg xmlns="http://www.w3.org/2000/svg" class="icon icon-tabler icon-tabler-pencil" width="24" height="24" viewBox="0 0 24 24" stroke-width="2" stroke="currentcolor" fill="none" stroke-linecap="round" stroke-linejoin="round"><path stroke="none" d="M0 0h24v24H0z" fill="none"/><path d="M4 20h4L18.5 9.5a1.5 1.5.0 00-4-4L4 16v4"/><line x1="13.5" y1="6.5" x2="17.5" y2="10.5"/></svg><span>Terms Of Service</span></a></li><li><a href=/privacypolicy/><svg xmlns="http://www.w3.org/2000/svg" class="icon icon-tabler icon-tabler-archive" width="24" height="24" viewBox="0 0 24 24" stroke-width="2" stroke="currentcolor" fill="none" stroke-linecap="round" stroke-linejoin="round"><path stroke="none" d="M0 0h24v24H0z"/><rect x="3" y="4" width="18" height="4" rx="2"/><path d="M5 8v10a2 2 0 002 2h10a2 2 0 002-2V8"/><line x1="10" y1="12" x2="14" y2="12"/></svg><span>Privacy Policy</span></a></li><li><a href=/disclaimer/><svg xmlns="http://www.w3.org/2000/svg" class="icon icon-tabler icon-tabler-link" width="24" height="24" viewBox="0 0 24 24" stroke-width="2" stroke="currentcolor" fill="none" stroke-linecap="round" stroke-linejoin="round"><path stroke="none" d="M0 0h24v24H0z"/><path d="M10 14a3.5 3.5.0 005 0l4-4a3.5 3.5.0 00-5-5l-.5.5"/><path d="M14 10a3.5 3.5.0 00-5 0l-4 4a3.5 3.5.0 005 5l.5-.5"/></svg><span>Disclaimer</span></a></li><li><a href=/contact/><svg xmlns="http://www.w3.org/2000/svg" class="icon icon-tabler icon-tabler-mail" width="44" height="44" viewBox="0 0 24 24" stroke-width="1.5" stroke="currentcolor" fill="none" stroke-linecap="round" stroke-linejoin="round"><path stroke="none" d="M0 0h24v24H0z" fill="none"/><rect x="3" y="5" width="18" height="14" rx="2"/><polyline points="3 7 12 13 21 7"/></svg><span>Contact Us</span></a></li><div class=menu-bottom-section><li id=dark-mode-toggle><svg xmlns="http://www.w3.org/2000/svg" class="icon icon-tabler icon-tabler-toggle-left" width="24" height="24" viewBox="0 0 24 24" stroke-width="2" stroke="currentcolor" fill="none" stroke-linecap="round" stroke-linejoin="round"><path stroke="none" d="M0 0h24v24H0z"/><circle cx="8" cy="12" r="2"/><rect x="2" y="6" width="20" height="12" rx="6"/></svg><svg xmlns="http://www.w3.org/2000/svg" class="icon icon-tabler icon-tabler-toggle-right" width="24" height="24" viewBox="0 0 24 24" stroke-width="2" stroke="currentcolor" fill="none" stroke-linecap="round" stroke-linejoin="round"><path stroke="none" d="M0 0h24v24H0z"/><circle cx="16" cy="12" r="2"/><rect x="2" y="6" width="20" height="12" rx="6"/></svg><span>Dark Mode</span></li></div></ol></aside><main class="main full-width"><article class=main-article><header class=article-header><div class=article-details><div class=article-title-wrapper><h2 class=article-title><a href=/posts/2016-09-09-data-structures-part-3-arrays-of-arrays/>Data Structures Part 3: Arrays of Arrays</a></h2></div><footer class=article-time><div><svg xmlns="http://www.w3.org/2000/svg" class="icon icon-tabler icon-tabler-calendar-time" width="56" height="56" viewBox="0 0 24 24" stroke-width="2" stroke="currentcolor" fill="none" stroke-linecap="round" stroke-linejoin="round"><path stroke="none" d="M0 0h24v24H0z"/><path d="M11.795 21H5a2 2 0 01-2-2V7a2 2 0 012-2h12a2 2 0 012 2v4"/><circle cx="18" cy="18" r="4"/><path d="M15 3v4"/><path d="M7 3v4"/><path d="M3 11h16"/><path d="M18 16.496V18l1 1"/></svg><time class=article-time--published>Oct 01, 2019</time></div><div><svg xmlns="http://www.w3.org/2000/svg" class="icon icon-tabler icon-tabler-clock" width="24" height="24" viewBox="0 0 24 24" stroke-width="2" stroke="currentcolor" fill="none" stroke-linecap="round" stroke-linejoin="round"><path stroke="none" d="M0 0h24v24H0z"/><circle cx="12" cy="12" r="9"/><polyline points="12 7 12 12 15 15"/></svg><time class=article-time--reading>28 minute read</time></div></footer></div></header><section class=article-content><p>Welcome to part 3 in this series covering all the data structures you <em>really</em> need (kind of).</p><ul><li><p>In <a class=link href=https://ourmachinery.com/post/data-structures-part-1-bulk-data/ target=_blank rel=noopener>part 1</a>, we saw that the best way to store most data is just to use a big old array and that with some tricks we can allocate this array so that the objects in it have fixed indices and permanent pointers.</p></li><li><p>In <a class=link href=https://ourmachinery.com/post/data-structures-part-2-indices/ target=_blank rel=noopener>part 2</a>, we saw how to fix the most glaring problem with these arrays — that it is kind of expensive to find things in them — by adding search indices that let us quickly find any subset of objects that we are interested in.</p></li></ul><p>In this part, we will tackle a final problem. When we created our bulk data arrays, we assumed that all objects were fixed size so that we could fit their data into a single struct and store the bulk data as just an array of such structs:</p><div class=highlight><div class=chroma><table class=lntable><tr><td class=lntd><pre tabindex=0 class=chroma><code><span class=lnt>1
</span></code></pre></td><td class=lntd><pre tabindex=0 class=chroma><code class=language-fallback data-lang=fallback><span class=line><span class=cl>struct object_t { ... }; uint32_t num_objects; struct object_t *objects; 
</span></span></code></pre></td></tr></table></div></div><p>But what if we need some fields in the object that are <em>dynamically</em> sized?</p><div class=highlight><div class=chroma><table class=lntable><tr><td class=lntd><pre tabindex=0 class=chroma><code><span class=lnt>1
</span></code></pre></td><td class=lntd><pre tabindex=0 class=chroma><code class=language-fallback data-lang=fallback><span class=line><span class=cl>struct object_t { const char *name; uint32_t num_children; object_t **children; }; 
</span></span></code></pre></td></tr></table></div></div><p>How can we store the object names and children in a good way?</p><h2 id=capped-size>Capped Size</h2><p>The simplest way of handling dynamically sized objects is to just get rid of the problem. If we set a maximum size for our strings and arrays, we’re back to having a fixed size struct, with the string and array data stored in the struct itself:</p><div class=highlight><div class=chroma><table class=lntable><tr><td class=lntd><pre tabindex=0 class=chroma><code><span class=lnt>1
</span></code></pre></td><td class=lntd><pre tabindex=0 class=chroma><code class=language-fallback data-lang=fallback><span class=line><span class=cl>enum {MAX_NAME_LENGTH = 63}; enum {MAX_CHILDREN = 8}; struct object_t { const char name[MAX_NAME_LENGTH + 1]; uint32_t num_children; object_t *children[MAX_CHILDREN]; }; 
</span></span></code></pre></td></tr></table></div></div><p>Now we can just make an <code>object_t[]</code> as before and have all the data in there.</p><p>Capping sizes is always a bit scary, because what if we set the cap too low and hit the limit? Personally, I feel a little bit dirty every time I do this. I think it’s PTSD from run-ins with some really ridiculous size limits, such as <code>MAX_PATH = 260</code> in Windows.</p><p>But there are a lot of situations where having a maximum size is fine. There is a big difference between locking a maximum size down in a file format or an OS API, where it might live for 50 years, causing headaches to generations of programmers, versus just having it in your runtime, where you can change it whenever you want without worrying about compatibility with old software. You can start with a fixed size and change to something more advanced when you need to. Code doesn’t have to be “future-proof” if it is easy for future you to change it.</p><p>Also, if you are writing code for just one particular game and not a generic engine, you don’t need to support anything beyond what your game needs. If you only support four players, your player arrays can all be <code>players[4]</code>. And anything that’s just for internal use, you can limit to whatever you find reasonable. For example, object debug names can probably be <code>s[64]</code> without any problems. Longer names are too hard to read anyway.</p><p>Also, keep in mind, that even if you don’t specify an explicit maximum size, there is probably a practical limit anyway. If you have too many things you will eventually overflow an <code>uint32_t</code>, run out of memory or drive the FPS down to the point where your app is unusable.</p><p>Judicious use of maximum sizes can simplify the code a lot. Of course, it doesn’t always work:</p><ul><li><p>Some things can get arbitrarily large.</p></li><li><p>Some things have a limit, but the limit is so high that you will waste a lot of memory by using a fixed-size array.</p></li><li><p>Some things have a small limit, but enforcing the limit is more trouble than its worth.</p></li></ul><p>As an example of the last case, consider your application’s window system. You could probably limit the number of windows to 128 because no sane user will want to deal with more than 128 open windows. (Note that this <em>does not</em> apply to tabs — many users are perfectly happy having a couple of thousand Chrome tabs open.) Using a fixed size of 128 won’t waste much memory either since you only need one global window array.</p><p><em>But…</em> if you decide to set a limit of 128 windows, you have to decide what to do if the user tries to open more than that. You probably don’t want to <em>crash</em>. Do you beep? Show an error message? What if the window you can’t open is the window that was supposed to show the error message? Now you have this extra code path in your code, that you have to write, maintain and test. An extra place where things can go wrong. It might be easier to just support an “unlimited” number of windows and let the users dig their own graves.</p><h2 id=heap-allocation>Heap allocation</h2><p>If we can’t use a fixed-size array, the next simplest thing is to just use an off-the-shelf STL object, such as <code>std::string</code> or <code>std::vector</code>.</p><div class=highlight><div class=chroma><table class=lntable><tr><td class=lntd><pre tabindex=0 class=chroma><code><span class=lnt>1
</span></code></pre></td><td class=lntd><pre tabindex=0 class=chroma><code class=language-fallback data-lang=fallback><span class=line><span class=cl>struct object_t { std::string name; std::vector children; }; 
</span></span></code></pre></td></tr></table></div></div><p>What happens when we use one of these types? STL will allocate data for the <code>string</code> and the <code>vector</code> on the system heap. Each <code>string</code> and <code>vector</code> gets its own individual little memory allocation, so we have a situation that looks something like this:</p><p><img src=https://ourmachinery.com/images/ds3-stl-memory.jpg loading=lazy></p><h4 id=stl-memory-layout>STL memory layout.</h4><p>If we have an <code>std::vector</code> with 10 000 items in it, we will use <em>one</em> memory allocation for the <code>object_t *</code> array, but an <em>additional</em> 20 000 allocations for the names and children of those objects.</p><p>This seems a bit wasteful, but computers are really fast, so this approach can work well in many cases. We have a fair number of arrays like this in <em>The Machinery</em> – not for strings (more about that later), but certainly for vectors. The only difference is that instead of using an <code>std::vector</code> we use a <a class=link href=https://ourmachinery.com/post/minimalist-container-library-in-c-part-1/ target=_blank rel=noopener>stretchy buffer</a>. Why? Well, for one, we program in C rather than C++ so we can’t use STL. But even if we used C++ we might still prefer this approach, because C++ classes can be tricky to use with the bulk data allocation approaches I described in <a class=link href=https://ourmachinery.com/post/data-structures-part-1-bulk-data/ target=_blank rel=noopener>part 1</a>.</p><p>First, remember how we stored the objects in an “array with holes” by making the stored data type a union of the original data we wanted to store and a free list pointer for keeping track of the free items:</p><div class=highlight><div class=chroma><table class=lntable><tr><td class=lntd><pre tabindex=0 class=chroma><code><span class=lnt>1
</span></code></pre></td><td class=lntd><pre tabindex=0 class=chroma><code class=language-fallback data-lang=fallback><span class=line><span class=cl>typedef union { object_t; freelist_item_t; } item_t; 
</span></span></code></pre></td></tr></table></div></div><p>This doesn’t work with <code>std::vector</code>, because objects with non-trivial copy constructors can’t be stored in unions. To make this work with STL objects we either have to use extra memory to store the freelist pointers with the object itself, or use something like <code>std::variant</code> (shudder).</p><p>Second, remember our nice trick of just reserving a large chunk of VM memory to hold our bulk data array. This works well with our C data structures, because they are zero-initialized and the memory we get from the VM is always cleared to zero (since otherwise, the VM would leak information between processes). So, with C data structures, we can just use the memory directly, whereas with C++ we would have to placement-new our strings and vectors into the memory.</p><p>In other words, in this use case, using the STL data types adds a lot of extra bookkeeping for limited benefits.</p><p>As I said above, heap allocation can certainly be “good enough” in a lot of circumstances, but if you want to store a really large number of objects, it has some drawbacks that I’ve already talked about earlier in this series:</p><ul><li><p>Making thousands of small allocations is time-consuming and can waste a substantial amount of memory on fragmentation, allocation headers, etc.</p></li><li><p>Since the arrays are allocated individually on the heap, they might be far apart in memory, resulting in cache misses when we process the objects.</p></li><li><p>With lots and lots of small allocations, it is hard to get a good picture of how much memory the system is using and what the access patterns are.</p></li></ul><p>So let’s look at alternatives!</p><h2 id=private-heap>Private heap</h2><p>One way of avoiding to make lots of <em>small</em> allocations from system memory is to make one <em>big</em> allocation and then chop it up ourselves into smaller pieces. Since we’re allocating a big chunk, we can request it directly from the VM and bypass the system heap allocator completely:</p><p><img src=https://ourmachinery.com/images/ds3-private-heap.jpg loading=lazy></p><h4 id=private-heap-1>Private heap.</h4><p>Essentially, what we’re doing here is implementing our own memory allocator. Getting chunks of memory from the VM and chopping them up into chunks to fulfill <code>malloc</code> and <code>new</code> requests is just what a memory allocator does.</p><p>Why would we ever want to do this, since we already have a perfectly good system memory allocator?</p><p>Well, to begin with, I think it is a really useful exercise. Writing your own memory allocator makes you think about how memory really works — it’s just bytes all the way down, and any “objects” you see are just your own mental projections. It may seem daunting and scary at first, but it’s not as hard as you might think. Try it!</p><p>But OK, personal development aside, why would we want to write our own memory allocator? Certainly, the system allocator has been written by “experts” and there is nothing we mere mortals could ever do to improve on it.</p><p>Actually, you’d be surprised. It used to be, not too long ago, a lot of systems shipped with really crappy system allocators. By switching out the standard allocator for something like <a class=link href=http://gee.cs.oswego.edu/dl/html/malloc.html target=_blank rel=noopener>dlmalloc</a>, you could give your game a nice 5-10 % performance boost. (Provided your game did tons of small memory allocations in the update loop, which of course you should <em>not</em> be doing. That’s what this whole series is about, remember?) I think that’s all fixed these days, though, and everything ships with something that is at least comparable to dlmalloc in terms of performance.</p><p>But it’s still possible to do better! How? Well, the standard memory allocator has to deal with a lot of different situations. Different applications have different memory patterns and since the standard allocator can never know exactly what’s going to be thrown at it, it has to do something that works kind-of-okay in a lot of different situations. And it’s really hard to write something that works good for everything, so you can usually come up with some adversarial worst-case input that makes the allocator completely crap out and go O(n²).</p><p>In contrast, if you are writing an allocator just for your specific application — or even better, for <em>one</em> <em>specific system</em> in your application — you have a lot more information. Your allocator doesn’t have to work well for any weird data that might be thrown at it, it just has to work well for the allocation patterns of <em>that</em> system. That is a lot simpler, and a lot more well-defined problem to solve. It is also a lot easier to test — you can just feed the allocator the data you expect and see how well it performs. So you can easily check if you manage to do better than the system allocator or not.</p><p>Just to convince you further, here are some examples of how you can do better than the system allocator:</p><ul><li><p>A lot of command-line programs just perform a single task and then exit. They tend to allocate a bunch of memory as they are performing the task and then free it all when they exit. These programs may actually run significantly faster with an allocator that just <em>never frees</em> the memory. Memory leaks aren’t really an issue with these programs, since once they exit, all the memory is returned to the system anyway. So doing a lot of bookkeeping in order to reclaim memory is not worth it. If memory is never freed we can use a simple <a class=link href=https://en.wikipedia.org/wiki/Region-based_memory_management target=_blank rel=noopener>pointer bump allocator</a> to allocate it.</p></li><li><p>If you’re not writing a generic allocator, you can change the allocator API. For example, you can make your <code>free()</code> function take two arguments — the pointer and the <em>size</em> of the memory block that is being freed. This size should always be known by the owner of the memory block, because if the owner didn’t know the size, how could they use the memory without causing access violations? Forcing the owner to pass the size to <code>free()</code> means that the allocator doesn’t have to remember it, which is nice. For example, suppose someone calls <code>malloc()</code> to allocate 4K of memory. With a 4K page size, <code>malloc()</code> could service this directly from the VM, but where would it store the size of the memory block? Either it has to allocate more memory for a preamble that holds the size, which means that the allocation no longer fits nicely into a system page, or it has to store the size some other way — such as in a hash table. If <code>free()</code> passes the size, <code>malloc()</code> doesn’t have to worry about storing it. We actually use this approach for all memory allocations in <em>The Machinery</em>.</p></li><li><p>A generic allocator needs to implement synchronization to make sure multiple threads are not allocating memory simultaneously. If you are writing your own allocator for a specific system, synchronization might already be dealt with by that system, at a higher level, which means you don’t have to repeat the effort in the allocator.</p></li></ul><p>So writing your own memory allocator can make things faster. But depending on how simple you are able to make it, there can still be a significant overhead from all those allocations. Also, we haven’t done anything to solve the problem of fragmentation. Allocating and freeing blocks of different sizes will leave holes of different sizes in our memory block, that are hard to reuse.</p><p>Can we do better?</p><h2 id=chunked-allocator>Chunked allocator</h2><p>One way of getting rid of fragmentation is to implement <a class=link href=https://en.wikipedia.org/wiki/Mark-compact_algorithm target=_blank rel=noopener>heap compaction</a>. I.e., instead of holding pointers to objects we hold <em>handles</em> that can be resolved into pointers. This allows us to <a class=link href=https://en.wikipedia.org/wiki/Defragmentation target=_blank rel=noopener>defragment</a> the heap by moving the memory blocks and updating the pointers that the handles resolve to. The main problem with this is that it can be really error prone, since you have to make sure to lock and unlock pointers so they don’t get moved while you are using them.</p><p>But there is another way. Fragmentation is caused by allocating and freeing memory blocks of <em>different sizes</em>. This leaves differently sized holes that we have to try to fill with future allocations. If all allocations were <em>the same size</em> we wouldn’t have this problem at all. In fact, in this case, we would just have the same situation as we had in the first part of this series — the bulk data array. We can just keep all the freed blocks in a free list and when the user wants to allocate more memory, we just give her the first block from the free list. Since all the blocks are the same size, the block will be the right size.</p><p>Is there a way to make all allocations the same size when all the objects can have a different number of children? Yes. What if we make each allocation a fixed-size “chunk” or “block” of children — say 16 at a time. If an object needs more than 16 children we can just allocate multiple chunks for that object and link them together with pointers.</p><p>Let’s see what it might look like:</p><div class=highlight><div class=chroma><table class=lntable><tr><td class=lntd><pre tabindex=0 class=chroma><code><span class=lnt>1
</span></code></pre></td><td class=lntd><pre tabindex=0 class=chroma><code class=language-fallback data-lang=fallback><span class=line><span class=cl>enum {CHILD_CHUNK_SIZE = 16}; struct child_chunk_t { object_t *children[CHILD_CHUNK_SIZE]; child_chunk_t *prev_chunk; child_chunk_t *next_chunk; }; struct object_t { const char *name; uint32_t num_children; child_chunk_t *child_chunks; }; uint32_t num_child_chunks; struct child_chunk_t *child_chunks; uint32_t num_objects; struct object_t *objects; 
</span></span></code></pre></td></tr></table></div></div><p>With this approach, both <code>objects</code> and <code>child_chunks</code> are just bulk data arrays, like the ones we encountered in the first part of this series. To find all the children of an object, we follow its <code>child_chunks</code> pointer to get to the first chunk of children and then follow the <code>next_chunk</code> pointers to get any additional chunks.</p><p>How does this solution perform compared to using an <code>std::vector</code>? The main drawback is that we can no longer randomly access objects in constant time with <code>[i]</code>, so if you need that, this is not a good solution. Note though, that we can still iterate over all objects in <em>O(n)</em> — same as for the <code>std::vector</code>.</p><p>Allocating and freeing a <code>child_chunk_t</code> is very cheap — just a few operations — a lot cheaper than if we were using a generic memory allocator. We can also make use of the tricks we used for the bulk data — such as allocating memory directly from the VM. It is very easy to see how much memory the system is using and what the access patterns are since we only have two allocations that we need to worry about — <code>objects</code> and <code>child_chunks</code>.</p><p>We get less memory coherency than we would with an <code>std::vector</code>. Instead of having all items continuous in memory, they are only continuous in chunks of <code>CHILD_CHUNK_SIZE</code>. Whenever we follow the <code>next_chunk</code> pointer we might get a cache miss. Note though that we have some control of this, by using different values for <code>CHILD_CHUNK_SIZE</code>. The higher we set it, the more objects we can process at one time before pointer chasing.</p><p>Also note that <em>most of the time</em>, chunks from the same child array will tend to end up next to each other since typically the children will be added at the same time, which means they will be allocated next to each other in memory (unless they’re allocated from the free list). So this might be less of an issue than you think.</p><p>In terms of memory, this solution has no external fragmentation — all the “holes” created when we free a <code>child_chunk_t</code> are perfectly sized for the allocation of another <code>child_chunk_t</code>. However, we do have internal fragmentation, since we’re always allocating at least <code>CHILD_CHUNK_SIZE</code> items in every chunk, even if we need less than that. The larger <code>CHILD_CHUNK_SIZE</code> is, the more memory we lose to internal fragmentation. We also have some overhead from the <code>prev_chunk</code> and <code>next_chunk</code> pointers. (We could store them as indices to reduce the overhead.)</p><p>How does the memory use compare to <code>std::vector</code>? It’s a bit tricky to say since there are a lot of factors involved. First, will children be added dynamically, so that we need to use <code>push_back()</code> and rely on geometric growth to get amortized constant time for adding elements, or, are all children known in advance so that we can just <code>resize()</code> the vector to the right size? If it’s the former, then the vector will be about 50 % empty on average, which for big vectors will be a lot more than the <code>CHILD_CHUNK_SIZE</code>. Second, the <code>vector</code> will have some amount of external fragmentation as well as overhead from preambles and postambles. But how much this is depends on the implementation details of the allocator and the size of the vector. For example, if the vector is small enough to fit in one of the fixed-size allocation pools of the system allocator, it doesn’t need a preamble and postamble and will not cause external fragmentation.</p><p>Personally, the main reason I like this approach over using an std::vector is that it’s <em>transparent</em>. It is easy to see exactly how much memory is being used and where memory is being wasted. We can easily follow the access pattern and see how much pointer-chasing we are doing. This transparency makes it easy to discover and fix any performance issues we run into. For example, we can play with the <code>CHILD_CHUNK_SIZE</code> or even create two different <code>child_chunk</code> arrays with different chunk sizes, to better accommodate both large and small arrays.</p><p>In constrast, the system allocator is essentially a black box. Unless you want to do some really deep digging, it’s hard to tell how it works, hard to tell how much memory you are spending on headers and fragmentation. It’s hard to even see if you have a problem, and even harder to fix it, since that means rewriting the system allocator.</p><p>I always prefer simple transparent system that can be easily hacked and tweaked over complicated black boxes.</p><p>What is a good chunk size for the chunked allocator? If we set it too low then we’re jumping around in memory a lot and also spending a lot of memory overhead on the <code>prev_chunk</code> and <code>next_chunk</code> pointers. If we set it too high, then we’re losing a lot of memory to internal fragmentation.</p><p>I think you shouldn’t go smaller than fitting a <code>child_chunk_t</code> in a cache line. If we use 32-bit indices instead of pointers for everything to save space, that means we’re using 4 bytes each for the <code>prev</code> and <code>next</code> references and 4 bytes for every object index. So if we use <code>CHILD_CHUNK_SIZE = 14</code> each <code>child_chunk_t</code> will be exactly 64 bytes. I don’t think there are many cases where you would want to go >14 elements either. Usually, small arrays are more common than large ones, so a higher number would mean a lot of internal fragmentation:</p><div class=highlight><div class=chroma><table class=lntable><tr><td class=lntd><pre tabindex=0 class=chroma><code><span class=lnt>1
</span></code></pre></td><td class=lntd><pre tabindex=0 class=chroma><code class=language-fallback data-lang=fallback><span class=line><span class=cl>enum {CHILD_CHUNK_SIZE = 14}; struct child_chunk_t { uint32_t child_indices[CHILD_CHUNK_SIZE]; uint32_t prev_chunk_index; uint32_t next_chunk_index; }; 
</span></span></code></pre></td></tr></table></div></div><p>It is interesting to note what happens if we let <code>CHILD_CHUNK_SIZE = 1</code>. In this case, we get:</p><div class=highlight><div class=chroma><table class=lntable><tr><td class=lntd><pre tabindex=0 class=chroma><code><span class=lnt>1
</span></code></pre></td><td class=lntd><pre tabindex=0 class=chroma><code class=language-fallback data-lang=fallback><span class=line><span class=cl>struct child_chunk_t { object_t *child; child_chunk_t *prev_sibling; child_chunk_t *next_sibling; }; 
</span></span></code></pre></td></tr></table></div></div><p>I.e., the child chunks just turn into a linked list of siblings. In fact, in this case, we don’t really need the <code>child_chunk_t</code> structure at all, we could just let the sibling pointers point directly into the <code>object_t *</code> array and get:</p><div class=highlight><div class=chroma><table class=lntable><tr><td class=lntd><pre tabindex=0 class=chroma><code><span class=lnt>1
</span></code></pre></td><td class=lntd><pre tabindex=0 class=chroma><code class=language-fallback data-lang=fallback><span class=line><span class=cl>struct object_t { const char *name; object_t *first_child; object_t *prev_sibling; object_t *next_sibling; }; 
</span></span></code></pre></td></tr></table></div></div><p>To enumerate all the children of an object stored in this way, we would first follow the <code>first_child</code> pointer to get to its first child and then keep following the <code>next_sibling</code> pointers to enumerate all the siblings of that child.</p><p>This solution is almost exactly the same as the one we used in part 2 of this series to find all the observations with a particular observer.</p><p>So this gives us another way of storing arrays of arrays. Instead of explicitly representing the children as an array:</p><p><strong>Parent</strong></p><p><strong>Children</strong></p><p>Ewing</p><p>[Edith, Phelan, Bouvier]</p><p>Bouvier</p><p>[Jr, Nicholas, Christopher]</p><p>We can represent that data as a relationship, as we would in a relational database:</p><p><strong>Parent</strong></p><p><strong>Child</strong></p><p>Ewing</p><p>Edith</p><p>Ewing</p><p>Phelan</p><p>Ewing</p><p>Bouvier</p><p>Bouvier</p><p>Jr</p><p>Bouvier</p><p>Nicholas</p><p>Bouvier</p><p>Christopher</p><p>and then use the techniques outlined in the previous post to enumerate all the children of a particular parent.</p><p>Which approach is best? Using explicit arrays is faster and uses less memory, but it only allows us to search the data one way, from parent to children. If we want to remove a particular child from the child array of its parent, we have to iterate through all the children to find the child we are looking for, which can be expensive if the arrays get large.</p><p>In contrast, the relational representation allows us to search using multiple criteria. For example, as we saw in the last post, we can add a child index and then we can search for a particular child and remove it from its parent as an <em>O(1)</em> operation.</p><p>I would use the array representation if I didn’t need these more advanced search options, and the relational representation otherwise.</p><h2 id=arrays-of-strings>Arrays of strings</h2><p>So far, I’ve talked a lot about the <code>children</code> array, but I haven’t really said anything about the <code>name</code> string. Well, a string is just an array of characters, right? So we could just use any of the techniques described above for storing <em>arrays of things</em> to store an <em>array of characters</em>, i.e. a <em>string,</em> right?</p><p>Wrong! Well, not totally wrong, of course, we <em>could</em> do that, theoretically. But I think that thinking of a string as an “array of characters” is fundamentally misguided. In fact, it’s one of my pet peeves. What matters in data-oriented design is how data is used, and strings are used <em>very differently</em> than other “arrays of things”.</p><p>When you think of an <code>std::vector</code>, what are the typical operations that you want to do with it?</p><ul><li><p>Iterate through all the items and call some method on each one:</p><div class=highlight><div class=chroma><table class=lntable><tr><td class=lntd><pre tabindex=0 class=chroma><code><span class=lnt>1
</span></code></pre></td><td class=lntd><pre tabindex=0 class=chroma><code class=language-fallback data-lang=fallback><span class=line><span class=cl>for (const auto &amp;it = v.begin(); it != v.end(); ++it) f(it); 
</span></span></code></pre></td></tr></table></div></div></li><li><p>Add a new item to the vector:</p><div class=highlight><div class=chroma><table class=lntable><tr><td class=lntd><pre tabindex=0 class=chroma><code><span class=lnt>1
</span></code></pre></td><td class=lntd><pre tabindex=0 class=chroma><code class=language-fallback data-lang=fallback><span class=line><span class=cl>v.push_back(x); 
</span></span></code></pre></td></tr></table></div></div></li><li><p>Remove an item from the vector:</p><div class=highlight><div class=chroma><table class=lntable><tr><td class=lntd><pre tabindex=0 class=chroma><code><span class=lnt>1
</span></code></pre></td><td class=lntd><pre tabindex=0 class=chroma><code class=language-fallback data-lang=fallback><span class=line><span class=cl>std::iter_swap(it, v.end() - 1); v.pop_back(); 
</span></span></code></pre></td></tr></table></div></div></li></ul><p>None of these operations are things you would typically do with a string!</p><p>If I have a string <code>"Niklas"</code> in my program, I never want to randomly add or remove characters. <code>"Nklas"</code> and <code>"Niklasz"</code> make no sense. I might iterate over the characters to draw them in the UI, but that’s only at one place in the code. Instead I’m probably more interested in comparing strings for equality (<code>strcmp()</code>) or composing substrings into larger strings (<code>sprintf()</code>). None of these are typical operations for other “arrays of things”.</p><p>The primary differences between strings and other arrays are:</p><p>By strings being <em>immutable</em>, I simply mean what I said earlier, that we don’t tend to add and remove characters in strings the same way we do with other arrays. Instead, when we change strings, we typically change <em>the whole thing.</em></p><p>For example, imagine we want to change the string:</p><div class=highlight><div class=chroma><table class=lntable><tr><td class=lntd><pre tabindex=0 class=chroma><code><span class=lnt>1
</span></code></pre></td><td class=lntd><pre tabindex=0 class=chroma><code class=language-fallback data-lang=fallback><span class=line><span class=cl>- last_name = &#34;Frykholm&#34; + last_name = &#34;Gray&#34; 
</span></span></code></pre></td></tr></table></div></div><p>We don’t think of this as removing the letters <code>F, k, h, o, l, m</code> and adding <code>G, a</code>. Rather, we think of this as a single <em>rename</em> operation — changing the whole last name. And that’s how most strings work (we’ll look at some exceptions later).</p><p>So strings aren’t really “arrays of characters”, they are <em>names</em> or <em>identifiers.</em> This is why in a lot of cases, you don’t even need to store the string itself, you can just store an <code>uint64_t</code> hash of the string and compare <em>that</em> to check if a name matches. The only time you need to store the string is if you need to show it to a human (debug printing, logging, UI) or pass it to another system (<code>fopen()</code>).</p><p>Unless you’re writing a word processor or something like that, I can only think of two cases where you have mutating strings:</p><ul><li><p>When the user is editing a string in a text box.</p></li><li><p>When you are building a string up from parts — i.e., concatenating substrings into a path, or an error message.</p></li></ul><p>For editing — presumably the user can’t be editing more than one string at a time (in whatever text box has focus), so this is really a special case. You can have a single <code>char</code> array in your program to hold this “currently edited string”. At the start of editing, you copy whatever string the user is editing into this array so that the text box can use it. When editing finishes, you create a new immutable string from the content of the array and then <em>rename</em> the original object to this new string.</p><p>Building up a string from parts is a good use case for temporary memory. In The Machinery, we have an <code>sprintf()</code> implementation that uses a temporary memory allocator, rather than a fixed-size buffer, so it can be used to build arbitrarily large strings. Once you are done constructing the string, you either use it (print the debug message, etc) or if you need to save it for later, convert it into an immutable string.</p><p>Reused strings can pop up in things like JSON parsing, where the same object key (e.g. <code>"properties"</code>) can show up hundreds or thousands of times. Storing a separate copy of the string each time it appears can be really wasteful.</p><p>If we treat strings as immutable, we don’t need separate copies, we can just make all the string pointers point to the same data:</p><p><img src=https://ourmachinery.com/images/ds3-immutable-strings.png loading=lazy></p><h4 id=immutable-strings>Immutable strings.</h4><p>It doesn’t matter how many <code>"properties"</code> strings we need. Using this technique, we can create as many strings as we like and still have only one copy in memory. Since the string data will never change (strings are immutable), sharing it is fine.</p><p>This technique of letting all the strings with the same data share the same pointer is called <a class=link href=https://en.wikipedia.org/wiki/String_interning target=_blank rel=noopener>string interning</a> and it has some interesting consequences. For one, we don’t have to compare strings with <code>strcmp()</code> or hashing anymore. Since we know that identical strings will use the same pointer, we can just compare the <code>const char *</code> pointers. s1 == s2 if and only if the pointers are equal.</p><p>Note that some programming languages, such as <a class=link href=https://www.lua.org target=_blank rel=noopener>Lua</a>, use interning for all their strings. Others, such as <a class=link href=https://en.wikipedia.org/wiki/Ruby_%28programming_language%29 target=_blank rel=noopener>Ruby</a> and <a class=link href=https://en.wikipedia.org/wiki/Common_Lisp target=_blank rel=noopener>Lisp</a>, have a separate <a class=link href=https://en.wikipedia.org/wiki/Symbol_%28programming%29 target=_blank rel=noopener>symbol</a> data type which represents an interned string.</p><p>To implement string interning in C, I use a big buffer to hold the string data and a hash table to look up strings in the buffer. The buffer can be reserved from virtual memory or allocated using one of the other techniques described in part 1 of this series. The buffer just stores all the strings consecutively and the hash table holds indices or pointers to these strings:</p><p><img src=https://ourmachinery.com/images/ds3-string-interning.jpg loading=lazy></p><h4 id=string-interning>String interning.</h4><p>The hash table is needed when strings enter the string interning system. For example, suppose you read a string from a file. After you’ve read it, it will be in some temporary memory array <code>char *</code>. To <em>intern</em> this string you need to check if you have a copy of it in your buffer already. If you do, you can just return a pointer to that string. If not, you should allocate a new string at the end of the buffer and return a pointer to <em>that</em>.</p><p>Here’s what the code might look like in practice:</p><div class=highlight><div class=chroma><table class=lntable><tr><td class=lntd><pre tabindex=0 class=chroma><code><span class=lnt>1
</span></code></pre></td><td class=lntd><pre tabindex=0 class=chroma><code class=language-fallback data-lang=fallback><span class=line><span class=cl>// Commit VM memory in 4K chunks. enum {CHUNK = 4 * 1024}; struct string_repository { uint32_t buffer_size; char *buffer; hash32_t lookup; }; const char *intern(struct string_repository *sr, const char *s) { const uint64_t h = murmurhash_string(s); const uint32_t idx = hash32_get(&amp;sr-&gt;lookup, h, UINT32_MAX); if (idx &lt; UINT32_MAX) return sr-&gt;buffer + idx; // Commit VM memory. (Assumes we&#39;re using the VM method for storing // the buffer.) Note: This is only needed on Windows which makes a // distinction between reserving and committing virtual memory. const uint32_t n = (uint32_t)strlen(s); const uint32_t size = sr-&gt;buffer_size; const uint32_t new_size = size + n + 1; const uint32_t chunks = div_round_up(sr-&gt;buffer_size, CHUNK); const uint32_t new_chunks = div_round_up(new_size, CHUNK); if (new_chunks != chunks) vm-&gt;commit(sr-&gt;buffer + chunks * CHUNK, (new_chunks - chunks) * CHUNK); // Copy string data into buffer and update hash table memcpy(sr-&gt;buffer + sr-&gt;buffer_size, s, n + 1); hash32_add(&amp;sr-&gt;lookup, h, sr-&gt;buffer_size); } 
</span></span></code></pre></td></tr></table></div></div><p>Note that if you want to do tricks like comparing strings by pointers, you have to make sure that both the strings are interned. If one of the strings is a string literal or has just been read from a file into a dynamic buffer, it won’t work. It’s only when strings have entered the interning system that strings with the same value have the same pointer.</p><p>When programming languages use string interning, they typically have one single string repository where all the strings in the program go. However, when you’re managing your own string repository I find it more useful to have separate per-system or per-object string repositories. For example, when parsing a JSON file, I would assign it it’s own string repository. This way, when you’re done with the parsing, you can just throw away the whole repository and free the memory.</p><p>Note that this again means that you need to be careful. If you try to compare strings by pointers and the strings are not from the <em>same</em> string repository, it won’t work.</p><p>So far, we haven’t described any mechanism for <em>removing</em> strings from the string repository. If we want to be able to do that, we first need to add reference counts to the string buffer. Since the same buffer data can be shared by multiple strings, we need to count how many times that happens, so that we can know when the data is no longer used and is safe to be deleted or reused.</p><p>Second, when strings are deleted, it will leave holes in our big string buffer:</p><p><img src=https://ourmachinery.com/images/ds3-string-buffer-holes.jpg loading=lazy></p><h4 id=string-buffer-holes>String buffer holes.</h4><p>To reuse this memory for other strings we must keep track of where all the holes are. We also probably want a mechanism for merging small neighboring holes into bigger holes, because otherwise, we’ll end up with smaller and smaller holes, that won’t be useful for anything but the shortest strings.</p><p>Note that this is exactly the same problem that a heap memory allocator has to solve, and we can address it using the same techniques. For example, we can link holes together in linked lists based on their sizes to make it possible to find them. We can also add preambles and postambles to all string allocations so that we can find neighboring allocations as targets for merging. Note that these preambles and postambles will add some overhead to every allocation. Also, we must round up the allocations to some minimum size, so that we are sure that the linked list pointers will fit in the “hole” that the allocation leaves when we free it.</p><p>Of course, we could also make use of all the other little tricks that memory allocators do. For example, we could have “pools” of allocators for strings of fixed sizes, etc, etc. It starts getting complicated.</p><p>Another thing you could do is to represent the strings as handles instead of pointers. I.e. you let the <code>intern()</code> function return an <code>uint32_t</code> string ID instead of an actual <code>char *</code> and then you have another function that looks up from the ID to the actual string data. Now, since nobody is pointing directly to the string, you can move the strings in memory and get rid of holes that way. This is the “compacting heap” approach.</p><p>In practice, I do it like this: I allocate the strings in blocks of 4 K. For each block, I keep track of how much “string” data and how much “hole” data it contains. When the block is > 50 % empty, I “defragment” the block by packing the strings tightly. The memory that’s left at the end of the block can then be used for new strings.</p><p><img src=https://ourmachinery.com/images/ds3-defragment.jpg loading=lazy></p><h4 id=string-buffer-defragmentation>String buffer defragmentation.</h4><p>The third option is to not bother with removing strings and reclaiming memory at all. This the absolutely simplest option and in many cases a perfectly valid one. Remember that our typical use case for this is to store object names and it is unlikely that objects will be renamed so much that the memory leaks become a problem. Another use case was for JSON parsing. Here, by using a designated string repository that we throw away at the end of the parse, there is no memory wasted at all. Any system that produced a large number of unique strings, such as logging, wouldn’t use this system anyway, but instead, just use temporary memory for the strings.</p><h2 id=conclusion>Conclusion</h2><p>Together, the techniques outlined in this series: bulk data arrays, indices and arrays of arrays, cover almost all the data structure needs we have in <em>The Machinery</em>. Is there something that seems to be missing? Tweet me at <a class=link href=https://twitter.com/niklasfrykholm target=_blank rel=noopener>@niklasfrykholm</a>.</p><p>from Hacker News <a class=link href=https://ift.tt/2oEOK1X target=_blank rel=noopener>https://ift.tt/2oEOK1X</a></p></section><footer class=article-footer><section class=article-copyright><svg xmlns="http://www.w3.org/2000/svg" class="icon icon-tabler icon-tabler-copyright" width="24" height="24" viewBox="0 0 24 24" stroke-width="2" stroke="currentcolor" fill="none" stroke-linecap="round" stroke-linejoin="round"><path stroke="none" d="M0 0h24v24H0z"/><circle cx="12" cy="12" r="9"/><path d="M14.5 9a3.5 4 0 100 6"/></svg><span>Licensed under CC BY-NC-SA 4.0</span></section></footer></article><footer class=site-footer><script async src="https://pagead2.googlesyndication.com/pagead/js/adsbygoogle.js?client=ca-pub-3987358164777632" crossorigin=anonymous></script>
<script data-name=BMC-Widget src=https://cdn.buymeacoffee.com/widget/1.0.0/prod/widget.prod.min.js data-id=0x000216 data-description=coffee! data-message=coffee! data-color=#FF813F data-position=right data-x_margin=28 data-y_margin=18></script></footer><div class=pswp tabindex=-1 role=dialog aria-hidden=true><div class=pswp__bg></div><div class=pswp__scroll-wrap><div class=pswp__container><div class=pswp__item></div><div class=pswp__item></div><div class=pswp__item></div></div><div class="pswp__ui pswp__ui--hidden"><div class=pswp__top-bar><div class=pswp__counter></div><button class="pswp__button pswp__button--close" title="Close (Esc)"></button>
<button class="pswp__button pswp__button--share" title=Share></button>
<button class="pswp__button pswp__button--fs" title="Toggle fullscreen"></button>
<button class="pswp__button pswp__button--zoom" title="Zoom in/out"></button><div class=pswp__preloader><div class=pswp__preloader__icn><div class=pswp__preloader__cut><div class=pswp__preloader__donut></div></div></div></div></div><div class="pswp__share-modal pswp__share-modal--hidden pswp__single-tap"><div class=pswp__share-tooltip></div></div><button class="pswp__button pswp__button--arrow--left" title="Previous (arrow left)"></button>
<button class="pswp__button pswp__button--arrow--right" title="Next (arrow right)"></button><div class=pswp__caption><div class=pswp__caption__center></div></div></div></div></div><script src=https://cdn.jsdelivr.net/npm/photoswipe@4.1.3/dist/photoswipe.min.js integrity="sha256-ePwmChbbvXbsO02lbM3HoHbSHTHFAeChekF1xKJdleo=" crossorigin=anonymous defer></script><script src=https://cdn.jsdelivr.net/npm/photoswipe@4.1.3/dist/photoswipe-ui-default.min.js integrity="sha256-UKkzOn/w1mBxRmLLGrSeyB4e1xbrp4xylgAWb3M42pU=" crossorigin=anonymous defer></script><link rel=stylesheet href=https://cdn.jsdelivr.net/npm/photoswipe@4.1.3/dist/default-skin/default-skin.css integrity="sha256-c0uckgykQ9v5k+IqViZOZKc47Jn7KQil4/MP3ySA3F8=" crossorigin=anonymous><link rel=stylesheet href=https://cdn.jsdelivr.net/npm/photoswipe@4.1.3/dist/photoswipe.css integrity="sha256-SBLU4vv6CA6lHsZ1XyTdhyjJxCjPif/TRkjnsyGAGnE=" crossorigin=anonymous></main><aside class="sidebar right-sidebar sticky"><section class="widget archives"><div class=widget-icon><svg xmlns="http://www.w3.org/2000/svg" class="icon icon-tabler icon-tabler-hash" width="24" height="24" viewBox="0 0 24 24" stroke-width="2" stroke="currentcolor" fill="none" stroke-linecap="round" stroke-linejoin="round"><path stroke="none" d="M0 0h24v24H0z"/><line x1="5" y1="9" x2="19" y2="9"/><line x1="5" y1="15" x2="19" y2="15"/><line x1="11" y1="4" x2="7" y2="20"/><line x1="17" y1="4" x2="13" y2="20"/></svg></div><h2 class="widget-title section-title">Table of contents</h2><div class=widget--toc><nav id=TableOfContents><ol><li><a href=#capped-size>Capped Size</a></li><li><a href=#heap-allocation>Heap allocation</a><ol><li><ol><li><a href=#stl-memory-layout>STL memory layout.</a></li></ol></li></ol></li><li><a href=#private-heap>Private heap</a><ol><li><ol><li><a href=#private-heap-1>Private heap.</a></li></ol></li></ol></li><li><a href=#chunked-allocator>Chunked allocator</a></li><li><a href=#arrays-of-strings>Arrays of strings</a><ol><li><ol><li><a href=#immutable-strings>Immutable strings.</a></li><li><a href=#string-interning>String interning.</a></li><li><a href=#string-buffer-holes>String buffer holes.</a></li><li><a href=#string-buffer-defragmentation>String buffer defragmentation.</a></li></ol></li></ol></li><li><a href=#conclusion>Conclusion</a></li></ol></nav></div></section></aside></div><script src=https://cdn.jsdelivr.net/npm/node-vibrant@3.1.5/dist/vibrant.min.js integrity="sha256-5NovOZc4iwiAWTYIFiIM7DxKUXKWvpVEuMEPLzcm5/g=" crossorigin=anonymous></script><script type=text/javascript src=/ts/main.js defer></script>
<script>(function(){const e=document.createElement("link");e.href="https://fonts.googleapis.com/css2?family=Lato:wght@300;400;700&display=swap",e.type="text/css",e.rel="stylesheet",document.head.appendChild(e)})()</script></body></html>