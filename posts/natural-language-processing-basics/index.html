<!doctype html><html lang=en-us dir=ltr><head><meta charset=utf-8><meta name=viewport content="width=device-width,initial-scale=1"><meta name=description content="Notes Taken on Stanford CS224N"><title>Natural Language Processing Basics</title><link rel=canonical href=https://Nexus-Security.github.io/posts/natural-language-processing-basics/><link rel=stylesheet href=/scss/style.min.7dbfdd4b0c439bdacf631096fda79b869b5850a9d35a3f67b9a557f3010f3972.css><meta property="og:title" content="Natural Language Processing Basics"><meta property="og:description" content="Notes Taken on Stanford CS224N"><meta property="og:url" content="https://Nexus-Security.github.io/posts/natural-language-processing-basics/"><meta property="og:site_name" content="0x000216"><meta property="og:type" content="article"><meta property="article:section" content="Posts"><meta property="article:tag" content="artificial intelligence"><meta property="article:tag" content="learning notes"><meta property="article:published_time" content="2020-09-01T00:00:00+00:00"><meta property="article:modified_time" content="2020-09-01T00:00:00+00:00"><meta name=twitter:title content="Natural Language Processing Basics"><meta name=twitter:description content="Notes Taken on Stanford CS224N"></head><body class=article-page><script>(function(){const e="StackColorScheme";localStorage.getItem(e)||localStorage.setItem(e,"auto")})()</script><script>(function(){const t="StackColorScheme",e=localStorage.getItem(t),n=window.matchMedia("(prefers-color-scheme: dark)").matches===!0;e=="dark"||e==="auto"&&n?document.documentElement.dataset.scheme="dark":document.documentElement.dataset.scheme="light"})()</script><div class="container main-container flex on-phone--column extended"><aside class="sidebar left-sidebar sticky"><button class="hamburger hamburger--spin" type=button id=toggle-menu aria-label="Toggle Menu">
<span class=hamburger-box><span class=hamburger-inner></span></span></button><header><figure class=site-avatar><a href=/><img src=/img/avatar_hue825486955cd7c56d95e38b4bd2a8e3c_229979_300x0_resize_box_3.png width=300 height=300 class=site-logo loading=lazy alt=Avatar></a></figure><div class=site-meta><h1 class=site-name><a href=/>0x000216</a></h1><h2 class=site-description>Where Do Russian Hackers Store Their Exploits? ðŸ¤“ /ussr/bin/ ðŸ˜‹</h2></div></header><ol class=social-menu><li><a href=https://github.com/Nexus-Security target=_blank title=GitHub><svg width="72" height="72" viewBox="0 0 72 72" xmlns="http://www.w3.org/2000/svg"><g fill="none" fill-rule="evenodd"><path d="M36 72c19.882251.0 36-16.117749 36-36 0-19.882251-16.117749-36-36-36-19.882251 365231026e-23-36 16.117749-36 36C24348735e-22 55.882251 16.117749 72 36 72z" fill="#3e75c3"/><path d="M35.9985 12C22.746 12 12 22.7870921 12 36.096644c0 10.6440272 6.876 19.6751861 16.4145 22.8617681C29.6145 59.1797862 30.0525 58.4358488 30.0525 57.7973276 30.0525 57.2250681 30.0315 55.7100863 30.0195 53.6996482c-6.6765 1.4562499-8.085-3.2302544-8.085-3.2302544-1.0905-2.7829884-2.664-3.5239139-2.664-3.5239139C17.091 45.4500754 19.4355 45.4801943 19.4355 45.4801943c2.4075.1701719 3.675 2.4833051 3.675 2.4833051 2.142 3.6820383 5.6175 2.6188404 6.9855 2.0014024C30.3135 48.4077535 30.9345 47.3460615 31.62 46.7436831 26.2905 46.1352808 20.688 44.0691228 20.688 34.8361671c0-2.6308879.9345-4.781379 2.4705-6.4665327C22.911 27.7597262 22.0875 25.3110578 23.3925 21.9934585c0 0 2.016-.6475568 6.6 2.4697516C31.908 23.9285993 33.96 23.6620468 36.0015 23.6515052 38.04 23.6620468 40.0935 23.9285993 42.0105 24.4632101c4.581-3.1173084 6.5925-2.4697516 6.5925-2.4697516C49.9125 25.3110578 49.089 27.7597262 48.8415 28.3696344 50.3805 30.0547881 51.309 32.2052792 51.309 34.8361671c0 9.2555448-5.6115 11.29309-10.9575 11.8894446.860999999999997.7439374 1.629 2.2137408 1.629 4.4621184C41.9805 54.4089489 41.9505 57.0067059 41.9505 57.7973276 41.9505 58.4418726 42.3825 59.1918338 43.6005 58.9554002 53.13 55.7627944 60 46.7376593 60 36.096644 60 22.7870921 49.254 12 35.9985 12" fill="#fff"/></g></svg></a></li><li><a href=mailto:0x000216@gmail.com target=_blank title=Email><svg id="Layer_1" xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink" viewBox="0 0 512 512" style="enable-background:new 0 0 512 512"><path style="fill:#e6f3ff" d="M512 105.739v300.522c0 27.715-22.372 50.087-50.087 50.087H50.087C22.372 456.348.0 433.976.0 406.261V105.739c0-.89.0-1.781.111-2.671 1.336-25.6 21.704-45.969 47.304-47.304.89-.111 1.781-.111 2.671-.111h411.826c.89.0 1.892.0 2.783.111 25.489 1.336 45.857 21.704 47.193 47.193C512 103.847 512 104.849 512 105.739z"/><path style="fill:#cfdbe6" d="M464.696 55.763c-.892-.111-1.891-.111-2.783-.111H256v400.696h205.913c27.715.0 50.087-22.372 50.087-50.087V105.739c0-.89.0-1.892-.111-2.783C510.553 77.468 490.184 57.099 464.696 55.763z"/><path style="fill:#ff4b26" d="M511.889 102.957c-1.336-25.489-21.704-45.857-47.193-47.193C382.89 137.569 336.951 183.509 256 264.459 225.291 233.732 77.61 85.958 47.416 55.763c-25.6 1.336-45.969 21.704-47.304 47.304C0 103.958.0 104.849.0 105.739v300.522c0 27.715 22.372 50.087 50.087 50.087h16.696V169.739l165.621 165.51c6.456 6.567 15.026 9.795 23.597 9.795 8.57.0 17.141-3.228 23.597-9.795l165.621-165.621v286.72h16.696c27.715.0 50.087-22.372 50.087-50.087V105.739C512 104.849 512 103.847 511.889 102.957z"/><path style="fill:#d93f21" d="M279.596 335.249l165.621-165.621v286.72h16.696c27.715.0 50.087-22.372 50.087-50.087V105.739c0-.89.0-1.892-.111-2.783-1.336-25.489-21.704-45.857-47.193-47.193C382.891 137.569 336.951 183.509 256 264.459v80.584C264.57 345.043 273.141 341.816 279.596 335.249z"/><g/><g/><g/><g/><g/><g/><g/><g/><g/><g/><g/><g/><g/><g/><g/></svg></a></li><li><a href=https://www.buymeacoffee.com/0x000216 target=_blank title=RSS><svg id="Layer_1" xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink" viewBox="0 0 512 512" style="enable-background:new 0 0 512 512"><path style="fill:#f78c20" d="M78.333 355.334C35.14 355.334.0 390.474.0 433.667S35.14 512 78.333 512s78.333-35.14 78.333-78.333-35.14-78.333-78.333-78.333z"/><g><path style="fill:#ffa929" d="M78.333 381.445c-28.795.0-52.222 23.427-52.222 52.222s23.427 52.222 52.222 52.222 52.222-23.427 52.222-52.222-23.427-52.222-52.222-52.222z"/><path style="fill:#ffa929" d="M477.918 264.861c-21.843-51.641-53.111-98.019-92.936-137.842-39.824-39.824-86.201-71.093-137.843-92.935C193.669 11.468 136.874.0 78.333.0c-4.807.0-8.704 3.897-8.704 8.704v85.519c0 4.807 3.897 8.704 8.704 8.704 182.37.0 330.74 148.369 330.74 330.74.0 4.807 3.897 8.704 8.704 8.704h85.52c4.807.0 8.704-3.897 8.704-8.704C512 375.126 500.533 318.331 477.918 264.861z"/><path style="fill:#ffa929" d="M78.333 163.853c-4.807.0-8.704 3.897-8.704 8.704v95.74c0 4.807 3.897 8.704 8.704 8.704 86.386.0 156.666 70.281 156.666 156.666.0 4.807 3.897 8.704 8.704 8.704h95.74c4.807.0 8.704-3.897 8.704-8.704.0-72.07-28.065-139.826-79.027-190.787-50.961-50.961-118.717-79.027-190.787-79.027z"/></g><g><path style="fill:#f78c20" d="M78.333 242.186c-2.918.0-5.817.076-8.704.206v25.905c0 4.807 3.897 8.704 8.704 8.704 86.386.0 156.666 70.281 156.666 156.666.0 4.807 3.897 8.704 8.704 8.704h25.905c.129-2.886.206-5.786.206-8.704.0-105.752-85.729-191.481-191.481-191.481z"/><path style="fill:#f78c20" d="M78.333 68.113c-2.91.0-5.81.042-8.704.11v26.001c0 4.807 3.897 8.704 8.704 8.704 182.37.0 330.74 148.369 330.74 330.74.0 4.807 3.897 8.704 8.704 8.704h26.001c.067-2.894.11-5.793.11-8.704C443.887 231.777 280.223 68.113 78.333 68.113z"/></g><g/><g/><g/><g/><g/><g/><g/><g/><g/><g/><g/><g/><g/><g/><g/></svg></a></li><li><a href target=_blank title=Slack><svg width="256" height="256" viewBox="0 0 256 256" xmlns="http://www.w3.org/2000/svg" preserveAspectRatio="xMidYMid"><path d="M165.964 15.838c-3.89-11.975-16.752-18.528-28.725-14.636-11.975 3.89-18.528 16.752-14.636 28.725l58.947 181.365c4.048 11.187 16.132 17.473 27.732 14.135 12.1-3.483 19.475-16.334 15.614-28.217L165.964 15.838" fill="#dfa22f"/><path d="M74.626 45.516C70.734 33.542 57.873 26.989 45.9 30.879 33.924 34.77 27.37 47.631 31.263 59.606l58.948 181.366c4.047 11.186 16.132 17.473 27.732 14.132 12.099-3.481 19.474-16.332 15.613-28.217L74.626 45.516" fill="#3cb187"/><path d="M240.162 166.045c11.975-3.89 18.526-16.75 14.636-28.726-3.89-11.973-16.752-18.527-28.725-14.636L44.708 181.632c-11.187 4.046-17.473 16.13-14.135 27.73 3.483 12.099 16.334 19.475 28.217 15.614l181.372-58.93" fill="#ce1e5b"/><path d="M82.508 217.27l43.347-14.084-14.086-43.352-43.35 14.09 14.089 43.347" fill="#392538"/><path d="M173.847 187.591c16.388-5.323 31.62-10.273 43.348-14.084l-14.088-43.36-43.35 14.09 14.09 43.354" fill="#bb242a"/><path d="M210.484 74.706c11.974-3.89 18.527-16.751 14.637-28.727-3.89-11.973-16.752-18.526-28.727-14.636L15.028 90.293C3.842 94.337-2.445 106.422.896 118.022c3.481 12.098 16.332 19.474 28.217 15.613l181.371-58.93" fill="#72c5cd"/><path d="M52.822 125.933c11.805-3.836 27.025-8.782 43.354-14.086-5.323-16.39-10.273-31.622-14.084-43.352l-43.36 14.092 14.09 43.346" fill="#248c73"/><path d="M144.16 96.256l43.356-14.088a546179.21 546179.21.0 00-14.089-43.36L130.07 52.9l14.09 43.356" fill="#62803a"/></svg></a></li><li><a href=https://www.buymeacoffee.com/0x000216 target=_blank title=Minds><svg id="Capa_1" xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink" viewBox="0 0 512 512" style="enable-background:new 0 0 512 512"><g><g><path style="fill:#ffe1b2" d="M256 33.085C245.078 13.38 224.079.0 2e2.0c-23.781.0-45.57 13.293-56.594 34.184C115.711 41.602 96 66.977 96 96c0 .059.0.113.0.172-9.977 7.512-16 19.301-16 31.828.0 1.316.078 2.637.234 3.992C60.211 145.266 48 167.758 48 192c0 14.07 4.039 27.543 11.719 39.262C57.273 236.512 56 242.207 56 248c0 2.738.281 5.445.828 8.098C36.672 267.308 24 288.539 24 312c0 27.973 18.305 52.34 44.109 60.785C65.398 378.828 64 385.324 64 392c0 21.098 13.805 39.508 33.539 45.727C103.891 466.746 129.828 488 160 488c4.617.0 9.227-.512 13.766-1.527C181.992 502 198.141 512 216 512c16.687.0 31.396-8.567 40-21.523V33.085z"/></g><g><g><path style="fill:#ffb980" d="M264 256c-4.422.0-8-3.582-8-8 0-22.055-17.945-40-40-40-8.008.0-15.734 2.355-22.336 6.812-3.023 2.043-7.055 1.781-9.797-.652-3.156-2.809-8.477-6.16-15.867-6.16-4.422.0-8-3.582-8-8s3.578-8 8-8c7.711.0 15.234 2.293 21.719 6.539C197.773 194.246 206.758 192 216 192c30.875.0 56 25.121 56 56C272 252.418 268.422 256 264 256z"/></g></g><g><g><path style="fill:#ffb980" d="M120 120c18.977.0 36.875 7.312 50.414 20.594 3.141 3.09 8.203 3.047 11.312-.109 3.094-3.152 3.047-8.219-.109-11.312C165.07 112.941 143.187 104 120 104c-13.046.0-25.395 2.93-36.542 8.046C81.253 117.019 80 122.423 80 128c0 1.316.078 2.637.234 3.992-.094.062-.173.139-.267.202C91.423 124.501 105.193 120 120 120z"/></g></g><g><g><path style="fill:#ffb980" d="M216 360c0-4.418-3.578-8-8-8s-8 3.582-8 8c0 17.645-14.352 32-32 32-14.211.0-26.82-9.648-30.664-23.465-.703-2.512-2.578-4.523-5.039-5.395-2.453-.871-5.188-.492-7.305 1.02C114.094 371.906 101.305 376 88 376c-6.948.0-13.625-1.149-19.894-3.207-2.214 4.939-3.501 10.19-3.916 15.586C71.714 390.73 79.711 392 88 392c13.297.0 26.187-3.266 37.773-9.52C133.969 397.894 150.141 408 168 408c26.469.0 48-21.531 48-48z"/></g></g><g><path style="fill:#fdc88e" d="M488 312c0-23.461-12.672-44.691-32.828-55.902.547-2.652.828-5.359.828-8.098.0-5.793-1.273-11.488-3.719-16.738C459.961 219.543 464 206.07 464 192c0-24.242-12.211-46.734-32.234-60.008.156-1.355.234-2.676.234-3.992.0-12.527-6.023-24.316-16-31.828.0-.059.0-.113.0-.172.0-29.023-19.711-54.398-47.406-61.816C357.57 13.293 335.781.0 312 0c-24.08.0-45.078 13.38-56 33.085v457.391C264.604 503.433 279.313 512 296 512c17.859.0 34.008-10 42.234-25.527C342.773 487.488 347.383 488 352 488c30.172.0 56.109-21.254 62.461-50.273C434.195 431.508 448 413.097 448 392c0-6.676-1.398-13.172-4.109-19.215C469.695 364.34 488 339.973 488 312z"/></g><g><path style="fill:#f8ab6b" d="M272.008 151.199C272 151.465 272 151.734 272 152c0 26.469 21.531 48 48 48s48-21.531 48-48c0-4.418-3.578-8-8-8s-8 3.582-8 8c0 17.645-14.352 32-32 32s-32-14.355-32-32c0-2.184.219-4.359.656-6.465.492-2.395-.133-4.883-1.703-6.754-1.57-1.871-4.016-3.066-6.352-2.859-.453.012-.891.059-.602.078-13.234.0-24-10.766-24-24v31.813C260.673 147.348 266.061 149.988 272.008 151.199z"/></g><g><path style="fill:#f8ab6b" d="M296 328c9.242.0 18.219-2.246 26.281-6.539C328.765 325.707 336.289 328 344 328c4.422.0 8-3.582 8-8s-3.578-8-8-8c-7.391.0-12.711-3.352-15.867-6.16-2.742-2.434-6.766-2.695-9.797-.656C311.726 309.644 304 312 296 312c-22.055.0-40-17.945-40-40v39.116C266.174 321.517 280.337 328 296 328z"/></g><g><g><path style="fill:#f8ab6b" d="M431.765 131.992c.156-1.355.234-2.676.234-3.992.0-5.577-1.253-10.981-3.458-15.954C417.395 106.93 405.046 104 392 104c-4.422.0-8 3.582-8 8s3.578 8 8 8c14.807.0 28.577 4.501 40.032 12.194C431.939 132.131 431.859 132.054 431.765 131.992z"/></g></g><g><g><path style="fill:#f8ab6b" d="M447.81 388.38c-.415-5.396-1.702-10.647-3.916-15.586C437.624 374.85 430.948 376 424 376c-13.578.0-26.594-4.266-37.641-12.332-2.07-1.5-4.719-1.93-7.133-1.168-2.43.77-4.344 2.648-5.164 5.059C369.101 382.176 355.414 392 340 392c-4.422.0-8 3.582-8 8s3.578 8 8 8c18.875.0 35.961-10.191 45.094-26.156C396.976 388.512 410.258 392 424 392 432.288 392 440.285 390.73 447.81 388.38z"/></g></g></g><g/><g/><g/><g/><g/><g/><g/><g/><g/><g/><g/><g/><g/><g/><g/></svg></a></li><li><a href=https://www.buymeacoffee.com/0x000216 target=_blank title=Coffee><svg id="Capa_1" xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink" viewBox="0 0 340 340" style="enable-background:new 0 0 340 340"><g id="XMLID_18_"><polygon id="XMLID_138_" style="fill:#dedde0" points="76.429,290 80,340 170,340 170,290"/><polygon id="XMLID_169_" style="fill:#dedde0" points="170,80 61.429,80 65,130 170,130"/><polygon id="XMLID_197_" style="fill:#acabb1" points="170,290 170,340 260,340 263.571,290"/><polygon id="XMLID_221_" style="fill:#acabb1" points="170,80 170,130 275,130 278.571,80"/><path id="XMLID_222_" style="fill:#ffda44" d="M170 260c-22.091.0-40-22.386-40-50s17.909-50 40-50v-30H65 50l10 160h16.429H170V260z"/><path id="XMLID_33_" style="fill:#ff9811" d="M170 130v30c22.091.0 40 22.386 40 50s-17.909 50-40 50v30h93.571H280l10-160h-15H170z"/><path id="XMLID_223_" style="fill:#50412e" d="M210 210c0-27.614-17.909-50-40-50v1e2c22.091.0 40-22.386 40-50z"/><path id="XMLID_224_" style="fill:#786145" d="M130 210c0 27.614 17.909 50 40 50V160c-22.091.0-40 22.386-40 50z"/><polygon id="XMLID_225_" style="fill:#50412e" points="278.571,80 300,80 300,40 260,40 260,0 80,0 80,40 40,40 40,80 61.429,80 170,80"/></g><g/><g/><g/><g/><g/><g/><g/><g/><g/><g/><g/><g/><g/><g/><g/></svg></a></li></ol><ol class=menu id=main-menu><li><a href=/><svg xmlns="http://www.w3.org/2000/svg" class="icon icon-tabler icon-tabler-home" width="24" height="24" viewBox="0 0 24 24" stroke-width="2" stroke="currentcolor" fill="none" stroke-linecap="round" stroke-linejoin="round"><path stroke="none" d="M0 0h24v24H0z"/><polyline points="5 12 3 12 12 3 21 12 19 12"/><path d="M5 12v7a2 2 0 002 2h10a2 2 0 002-2v-7"/><path d="M9 21v-6a2 2 0 012-2h2a2 2 0 012 2v6"/></svg><span>Home</span></a></li><li><a href=/about/><svg xmlns="http://www.w3.org/2000/svg" class="icon icon-tabler icon-tabler-user" width="24" height="24" viewBox="0 0 24 24" stroke-width="2" stroke="currentcolor" fill="none" stroke-linecap="round" stroke-linejoin="round"><path stroke="none" d="M0 0h24v24H0z"/><circle cx="12" cy="7" r="4"/><path d="M6 21v-2a4 4 0 014-4h4a4 4 0 014 4v2"/></svg><span>About Us</span></a></li><li><a href=/termsofservice/><svg xmlns="http://www.w3.org/2000/svg" class="icon icon-tabler icon-tabler-pencil" width="24" height="24" viewBox="0 0 24 24" stroke-width="2" stroke="currentcolor" fill="none" stroke-linecap="round" stroke-linejoin="round"><path stroke="none" d="M0 0h24v24H0z" fill="none"/><path d="M4 20h4L18.5 9.5a1.5 1.5.0 00-4-4L4 16v4"/><line x1="13.5" y1="6.5" x2="17.5" y2="10.5"/></svg><span>Terms Of Service</span></a></li><li><a href=/privacypolicy/><svg xmlns="http://www.w3.org/2000/svg" class="icon icon-tabler icon-tabler-archive" width="24" height="24" viewBox="0 0 24 24" stroke-width="2" stroke="currentcolor" fill="none" stroke-linecap="round" stroke-linejoin="round"><path stroke="none" d="M0 0h24v24H0z"/><rect x="3" y="4" width="18" height="4" rx="2"/><path d="M5 8v10a2 2 0 002 2h10a2 2 0 002-2V8"/><line x1="10" y1="12" x2="14" y2="12"/></svg><span>Privacy Policy</span></a></li><li><a href=/disclaimer/><svg xmlns="http://www.w3.org/2000/svg" class="icon icon-tabler icon-tabler-link" width="24" height="24" viewBox="0 0 24 24" stroke-width="2" stroke="currentcolor" fill="none" stroke-linecap="round" stroke-linejoin="round"><path stroke="none" d="M0 0h24v24H0z"/><path d="M10 14a3.5 3.5.0 005 0l4-4a3.5 3.5.0 00-5-5l-.5.5"/><path d="M14 10a3.5 3.5.0 00-5 0l-4 4a3.5 3.5.0 005 5l.5-.5"/></svg><span>Disclaimer</span></a></li><li><a href=/contact/><svg xmlns="http://www.w3.org/2000/svg" class="icon icon-tabler icon-tabler-mail" width="44" height="44" viewBox="0 0 24 24" stroke-width="1.5" stroke="#2c3e50" fill="none" stroke-linecap="round" stroke-linejoin="round"><path stroke="none" d="M0 0h24v24H0z" fill="none"/><rect x="3" y="5" width="18" height="14" rx="2"/><polyline points="3 7 12 13 21 7"/></svg><span>Contact Us</span></a></li><div class=menu-bottom-section><li id=dark-mode-toggle><svg xmlns="http://www.w3.org/2000/svg" class="icon icon-tabler icon-tabler-toggle-left" width="24" height="24" viewBox="0 0 24 24" stroke-width="2" stroke="currentcolor" fill="none" stroke-linecap="round" stroke-linejoin="round"><path stroke="none" d="M0 0h24v24H0z"/><circle cx="8" cy="12" r="2"/><rect x="2" y="6" width="20" height="12" rx="6"/></svg><svg xmlns="http://www.w3.org/2000/svg" class="icon icon-tabler icon-tabler-toggle-right" width="24" height="24" viewBox="0 0 24 24" stroke-width="2" stroke="currentcolor" fill="none" stroke-linecap="round" stroke-linejoin="round"><path stroke="none" d="M0 0h24v24H0z"/><circle cx="16" cy="12" r="2"/><rect x="2" y="6" width="20" height="12" rx="6"/></svg><span>Dark Mode</span></li></div></ol></aside><main class="main full-width"><article class=main-article><header class=article-header><div class=article-details><header class=article-category></header><div class=article-title-wrapper><h2 class=article-title><a href=/posts/natural-language-processing-basics/>Natural Language Processing Basics</a></h2><h3 class=article-subtitle>Notes Taken on Stanford CS224N</h3></div><footer class=article-time><div><svg xmlns="http://www.w3.org/2000/svg" class="icon icon-tabler icon-tabler-calendar-time" width="56" height="56" viewBox="0 0 24 24" stroke-width="2" stroke="currentcolor" fill="none" stroke-linecap="round" stroke-linejoin="round"><path stroke="none" d="M0 0h24v24H0z"/><path d="M11.795 21H5a2 2 0 01-2-2V7a2 2 0 012-2h12a2 2 0 012 2v4"/><circle cx="18" cy="18" r="4"/><path d="M15 3v4"/><path d="M7 3v4"/><path d="M3 11h16"/><path d="M18 16.496V18l1 1"/></svg><time class=article-time--published>Sep 01, 2020</time></div><div><svg xmlns="http://www.w3.org/2000/svg" class="icon icon-tabler icon-tabler-clock" width="24" height="24" viewBox="0 0 24 24" stroke-width="2" stroke="currentcolor" fill="none" stroke-linecap="round" stroke-linejoin="round"><path stroke="none" d="M0 0h24v24H0z"/><circle cx="12" cy="12" r="9"/><polyline points="12 7 12 12 15 15"/></svg><time class=article-time--reading>11 minute read</time></div></footer></div></header><section class=article-content><h2 id=how-to-define-the-meaning-of-a-word>How to define the meaning of a word</h2><h3 id=one-hot-vector>One-Hot Vector</h3><p>Say we have 3 word, &ldquo;natural&rdquo;, &ldquo;language&rdquo;, &ldquo;processing&rdquo;, we can define:</p><ul><li>&ldquo;nature&rdquo;=[1, 0, 0],</li><li>&ldquo;language&rdquo;=[0, 1, 0],</li><li>&ldquo;processing&rdquo;=[0, 0, 1]</li></ul><h4 id=cons>Cons:</h4><ul><li>With the increase of the words, the dimension of the vectors grows rapidly.</li><li>Any vector timing another vector equals 0, we cannot use cosine to measure their similarity.</li></ul><h3 id=word2vec>Word2Vec</h3><p>The basic theory is that we can know the meanning of a word from the words beside them.</p><h4 id=skip-gram-model>Skip-Gram Model</h4><p>Say we have a phrase &ldquo;natural language processing&rdquo;, take &ldquo;language&rdquo; as a central word then we have:
P(&ldquo;natural&rdquo;) = P(&ldquo;natural&rdquo;|&ldquo;language&rdquo;)
P(&ldquo;processing&rdquo;)=P(&ldquo;processing&rdquo;|&ldquo;language&rdquo;)
Generally, we have</p><p>$$
P(Context Word = o| Center Word = c) = \frac{e^{(u_o^Tv_c)}}{\sum\nolimits_{w\in{Vocab}}e^{(u_w^Tv_c)}}
$$</p><p>Loss function would be</p><p>$$
J(\theta) = -\frac{1}{T}logL(\theta) = -\frac{1}{T}\sum_{t=1}^T\sum_{-m\leq j\leq m&j\neq0}logP(w_{t+j}|w_t; \theta)
$$</p><p>Note that $L(\theta)$ is the maximum likelihood function.</p><h3 id=single-value-decomposition>Single Value Decomposition</h3><p>We use a word-to-document co-occurrence matrix.
Say we have two sentences, and the window size = 1.
I like deep learning.
I like natural language processing.</p><div class=table-wrapper><table><thead><tr><th></th><th>I</th><th>like</th><th>deep</th><th>learning</th><th>natural</th><th>language</th><th>processing</th></tr></thead><tbody><tr><td>I</td><td>0</td><td>2</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td></tr><tr><td>like</td><td>2</td><td>0</td><td>1</td><td>0</td><td>1</td><td>0</td><td>0</td></tr><tr><td>deep</td><td>0</td><td>1</td><td>0</td><td>1</td><td>0</td><td>0</td><td>0</td></tr><tr><td>learning</td><td>0</td><td>0</td><td>1</td><td>0</td><td>0</td><td>0</td><td>0</td></tr><tr><td>natural</td><td>0</td><td>1</td><td>0</td><td>0</td><td>0</td><td>1</td><td>0</td></tr><tr><td>language</td><td>0</td><td>0</td><td>0</td><td>0</td><td>1</td><td>0</td><td>1</td></tr><tr><td>processing</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>1</td><td>0</td></tr><tr><td>Then we can compress the matrix and store the vectors.</td><td></td><td></td><td></td><td></td><td></td><td></td><td></td></tr></tbody></table></div><h3 id=glove>GloVe</h3><p>As its name suggests, GloVe uses global information to build vectors.
$X_{ij}$ means how many times word j has shown up in the context of word i, then $X_i = \sum_kX_{ik}$ means all the words showing up in the context of word i. The probility of word j showing up in the context of word i is denoted as</p><p>$$
P_{ij} = P(j|i) = X_{ij} / X_i
$$</p><p>Loss Function:</p><p>$$
w_i * w_j = logP(i|j)
$$</p><p>$$
J = \sum_{i, j = 1}^Vf(X_{ij})(w_i^T\tilde{w_j} + b_i + \tilde{b_j} - logX_{ij})^2
$$</p><h2 id=named-entity-recognition>Named Entity Recognition</h2><p>We want to find the names in the text and classfy them, for example:<br>Only France [LOC] and Britain [LOC] back Fischler [PER] &rsquo;s proposal.</p><h3 id=window-classification>Window Classification</h3><p>We can use the words beside the target word to classify it.<br>Say the window size = 2 and the target word is Paris. We can have a window like:<br>museum in Paris are amazing<br>x1 x2 x3 x4 x5<br>$X_{window} = [x_1, x_2, x_3, x_4, x_5]$
If we have mulitple class, we can use a softmax classifier.<br>Say x denotes the input word vector and y is the target class among k classes, then we have</p><p>$$
P(y|x) = \frac{e^{(W_y<em>x)}}{\sum_{c=1}^ke^{(W_c</em>x)}}
$$</p><p>The loss function is a cross entropy function:</p><p>$$
J(\theta) = \frac{1}{N}\sum_{i = 1}^N-log(\frac{e^{(W_y<em>x)}}{\sum_{c = 1}^ke^{(W_c</em>x)}})
$$</p><h2 id=syntactic-structure-analaysis>Syntactic Structure Analaysis</h2><h3 id=constituency-parsing>Constituency Parsing</h3><p>Analyse the words as part of speech [POS], and then connect them as phrases and finally sentences.<br>Say we have a phrase like<br>the cute cat at the coffee shop<br>Its structure is [DET] [ADJ] [NOUN] [PREP] [DET] [NOUN], and Noun Phrase = { [DET] [ADJ] [NOUN] }, Preposition Phrase = { [PREP] [DET] [NOUN] }, and they together make a sentence.</p><h3 id=dependency-parsing>Dependency Parsing</h3><h4 id=transition-based-dependency-parsing>Transition-based Dependency Parsing</h4><p>Transition-based dependency parsing is very similar to a state machine, for a given sentence $S = {w_0w_1\cdots w_n}$, the state has three parts, $(\sigma, \beta, A)$.<br>$\sigma$ is a stack of words.<br>$\beta$ is a buffer of words.<br>A is a collection of dependency arc, every edge is like $(w_i, r, w_j)$, and r denotes the dependency between words.<br>At the initial state, $\sigma$ contains ROOT $w_0$ only, and $\beta$ contains the rest of the sentence, while A is an empty set.<br>There are 3 transition between states:</p><ul><li>SHIFT: pop the first word out of the buffer and put it in the stack.</li><li>LEFT-ARC: add $(w_j, r, w_i)$ to the collection of edges, and $w_j$ is on the top of the stack while $w_i$ is the second top one of the stack.</li><li>RIGHT-ARC: add $(w_i, r, w_j)$ to the collection of edges, and $w_i$ is on the top of the stack while $w_j$ is the second top one of the stack.</li></ul><h2 id=language-model>Language Model</h2><p>The model is about :<br>Given {$x_1, x_2, \cdots, x_n$}, predict $x_{n+1}$.</p><p>$$
P(x^{t + 1}, x^t, \cdots, x^1) = \sum_{t = 1}^TP(x^{t+1} | x^t, \cdots, x^1)
$$</p><h3 id=n-gram-model>N-Gram Model</h3><p>The most classic one is the N-Gram Model. For example, the bigram of &ldquo;natural language processing&rdquo; is &ldquo;natural language&rdquo; and &ldquo;language processing&rdquo;.<br>The basic assumption of the model is that</p><ul><li>the probability of n-gram is propotional to the probability of the word.</li><li>$P(x^{t + 1})$ is only related to the (n - 1) word before it.
$$
P(x^{t + 1} | x^t, \cdots, x^1) \approxeq \frac{count(x^{t + 1}, x^t, \cdots, x^{t - n + 2})}{count(x^t, \cdots, x^{t - n + 2})}
$$
Cons:</li><li>Sparsity Problem</li><li>The storage grows very fast as the n grows.</li></ul><h3 id=recurrent-neural-network>Recurrent Neural Network</h3><p>In a RNN, different parameters share a same matrix, and they are not limited to a fixed size window.<br>Hidden Layer:</p><p>$$
h_t = \sigma(W^{(hh)}h_{t - 1} + W^{ht}x_{[t]})
$$</p><p>Output:</p><p>$$
\hat{y_t} = softmax(W^{(S)}h_t)
$$</p><p>The loss function is cross entropy loss.<br>We can use perplexity to evalute our language model.</p><p>$$
perplexity = e^{J(\theta)}
$$</p><p>Pros:</p><ul><li>It can deal with input of any length.</li><li>We can use information with a long history.</li><li>The size of the model does not increase with input size.</li><li>The parameters of weights are shared efficiently.
Cons:</li><li>RNN is comparatively slow because it is not parallel.</li><li>Gradient vanishing makes long history information hard to capture.</li></ul><h2 id=gradient-vanishing-and-gradient-explosion>Gradient Vanishing and Gradient Explosion</h2><p>In $W^{(t - k)}$, if |W| is smaller than 1, as t - k incrrases, W is likely to become very small so it is very likely for us to loss information from the past.
In the opposite, if |W| is greater than 1, as t - k incrrases, W is likely to become very large and cause explosion.</p><h3 id=gradient-clipping>Gradient clipping</h3><p>If the gradient is greater than the preset threshold, we clip the gradient.</p><h2 id=long-short-term-memory-lstm>Long Short Term Memory (LSTM)</h2><p>We introduce cell states to store the long-term information, and they can be written and erased by control gate.
A general LSTM:
Input gate</p><p>$$
i_t = \sigma(W^{(i)}x_t + U^{(i)}h_{t - 1})
$$</p><p>Forget gate</p><p>$$
f_t = \sigma(W^{(f)}x_t + U^{(f)}h_{t - 1})
$$</p><p>Output gate</p><p>$$
o_t = \sigma(W^{(o)}x_t + U^{(o)}h_{t - 1})
$$</p><p>New memory cell</p><p>$$
\tilde{c_t} = tanh(W^{(c)}x_t + U^{(c)}h_{t - 1})
$$</p><p>Final memory cell</p><p>$$
c_t = f_t\circ c_{t - 1} + i_t\circ \tilde{c_t}
$$</p><p>Hidden state</p><p>$$
h_t = o_t\circ tanh(c_t)
$$</p><p>Gated Recurrent Unit (GRU)
GRU is very similar to LSTM, it combines forget gate and input gate into an update gate, and the cell state is combined into hidden state.
Update gate</p><p>$$
z_t = \sigma(W^{(z)}x_t + U^{(z)}h_{t - 1})
$$</p><p>Reset gate</p><p>$$
r_t = \sigma(W^{(r)}x_t + U^{(r)}h_{t - 1})
$$</p><p>New memory cell</p><p>$$
\tilde{h_t} = tanh(r_t\circ Uh_{t - 1} + Wx_t)
$$</p><p>Hidden state</p><p>$$
h_t = (1 - z_t)\circ \tilde{h_t} + z_t\circ h{t - 1}
$$</p><h2 id=neural-machine-translation>Neural Machine Translation</h2><p>Neural machine translation is generally a sequence to sequence model. It uses RNN as an encoder and another RNN as a decoder.<br>Loss function:</p><p>$$
J(\theta) = \frac{1}{T}\sum_{t = 1}^T-log\tilde{y_{x_{t + 1}}^{(t)}}
$$</p><p>We choose the word with highest probability as the next input of the decoder.</p><h3 id=bilingual-evaluation-study-bleu>Bilingual Evaluation Study (BLEU)</h3><p>Presion score: $p_n = \frac{# matched n-grams}{# n-grams in the candidate translation}$
Weight: $w = \frac{1}{2^n}$
Penalty: $\beta = e^{min(0, 1 - \frac{len_{ref}}{len_{MT}})}$
BLEU:</p><p>$$
BLEU = \beta\prod_{i = 1}^kp_n^{w_n}
$$</p><h3 id=attention>Attention</h3><p>We can use the attention method to focus on the specific context of the current word.</p><h4 id=process>Process:</h4><ul><li>Get the hidden states of the encoder $(h_1, h_2, h_T)$.</li><li>Say the hidden state of the current decoder is $s_{t - 1}, we can use it for the relationship between input index j and current output index. $e_{ij} = a(s_{t - 1}, h_j)$, in the form of the vector is $\vec{e_t} = (a(s_{t - 1}, h_1), \cdots, a(s_{t - 1}, h_T))$, a denotes a relationship operation between s and h.</li><li>Compute the attention by softmaxing $\vec{e_t}$: $\vec{a_t} = softmax(\vec{e_t})$</li><li>Compute the context vector: $\vec{c_t} = \sum_{j =1}^Ta_{ij}h_j$</li><li>Finally, we have the next hidden state of the decoder in the form $s_t = f(s_{t - 1}, y_{t - 1}, c_t)$, and the ouput would be $p(y_t|y_1, \cdots, y_{t - 1}, \vec{x}) = g(y_{i - 1}, s_i, c_i)$.</li></ul><h2 id=question-answering-system>Question Answering System</h2><p>We use the Stanford Question Answering Dataset.</p><h3 id=stanford-attention-reader>Stanford Attention Reader</h3><p>We get the features of the questions with a Bidirectional LSTM. For the answers, we can use a Bidirectional LSTM to get the features of every word, and $Attention = (\vec{feature_{q_i}}, \vec{feature_{w_i}})$, so that we can infer the start position and the end position of the answer.<br>The loss function is:</p><p>$$
L = -\sum logP^{start}(a_{start}) - \sum logP^{end}(a_{end})
$$</p><h3 id=birectional-attention-flow>Birectional Attention Flow</h3><p>The attention model here is bidirectional, because we have a query-to-context layer and a context to query layer.<br>Similar matrix:</p><p>$$
S_{ij} = w_{sim}^T[c_i; q_j, c_i \circ q_j]\in R
$$</p><p>Note that $c_i$, $q_i$ denote context vector and query vector respectively.<br>For the context to query attention,<br>Attention score:</p><p>$$
\alpha^i = softmax(S_{i,:})\in R^M \forall i\in{1,\cdots, N}
$$</p><p>Weighted vector:</p><p>$$
a_i = \sum_{j = 1}^M\alpha^i_jq_j \in R^{2h} \forall i\in{1,\cdots, N}
$$</p><p>For the query to context attention,</p><p>$$
m_i = max_jS_{ij } \in R \forall i\in{1,\cdots, N}
$$</p><p>$$
\beta = softmax(m)\in R^N
$$</p><p>$$
c&rsquo; = \sum_{i = 1}^N\beta^ic_i \in R^{2h}
$$</p><p>The output of the Attention Flow Layer would be</p><p>$$
b_i = [c_i; a_i; c_i\circ a_i; c_i\circ c&rsquo;] \in R^{8h} \forall i \in{1, \cdots, N}
$$</p><h2 id=convolutional-neural-network>Convolutional Neural Network</h2><p>If the embedding of a word is a k-dimensional vector, a sentence is a matrix. With n word, the matrix size is n * k. We usually need to pad the sentence to have a deeper convolutional network.<br>The initiative of pooling is the stablize the network, when the input changes little.</p><h3 id=quasi-recurrent-neural-network>Quasi Recurrent Neural Network</h3><p>Quaso Recurrent Neural Network is a combination of convolutional neural network and recurrent neural network. The main idea is to use a convolutional neural network to extract features and replace the pooling layer with dynamic average pooling.<br>For the feature extraction, the candidate vector, forget gate, output gate is:</p><p>$$
Z = tanh(W_z * X)
$$</p><p>$$
F = \sigma(W_f * X)
$$</p><p>$$
O = \sigma(W_o * X)
$$</p><p>The dynamic average pooling is:</p><p>$$
x_t = f_t\cdot c_{t - 1} + (1 - f_t)\cdot z_t
$$</p><p>$$
h_t = o_t\cdot c_t
$$</p><h2 id=subword-model>Subword Model</h2><h3 id=character-level-model>Character-level model</h3><p>We can use more layers of convolution, pooling and highway to solve the complexity problem.</p><h3 id=byte-pair-encoding>Byte Pair Encoding</h3><p>Byte pair encodes n-grams of the highest frequency until its vocabulary reaches the preset threshold.</p><h3 id=fastttext>FasttText</h3><p>It takes a word as bag of character n-gram.</p><h2 id=contextual-word-representation>Contextual Word Representation</h2><h3 id=embeddings-from-language-models-elmo>Embeddings from Language Models (ELMO)</h3><p>ELMO uses a bidirectional LSTM to compute the contextual embedding. The lower layer represents the basic grammar information, while the higher layer represents context information.<br>Forward language model:</p><p>$$
p(t_1, t_2, \cdots, t_N) = \prod_{k = 1}^Np(t_k|t_1, t_2, \cdots, t_{k - 1})
$$</p><p>Backword language model:</p><p>$$
p(t_1, t_2, \cdots, t_N) = \prod_{k = 1}^Np(t_k|t_{k + 1}, t_{k + 2}, \cdots, t_N)
$$</p><p>The initiative of the bidirectional LSTM is to maximaize:</p><p>$$
\sum_{k + 1}^N(logp(t_k|t_1, t_2, \cdots, t{k - 1}) + logp(t_k|t_{k + 1}, t_{k + 2}, \cdots, t_N))
$$</p><p>For the word at index k, it is represented by (2L + 1) vectors. One of them is an embedding not related to the context, while L layers of forward LSTM produces $\vec{h*{kj}} related to the former paragraph and L layers of backward LSTM produces $\vec{h*{kj}} for the rest of the paragraph.
For the following layers, we have:</p><p>$$
ELMO_k^{task} = E(R_k; \theta^{task}) = \gamma^{task}\sum_{j = 0}^Ls_j^{task}h_{k, j}^{LM}
$$</p><p>Note that $s^{task}$ is softmaxed and $\gamma^{task}$ is a scale parameter.</p><h3 id=generative-pre-training-gpt>Generative Pre Training (GPT)</h3><p>The basic structure of the GPT is a transformer, which is used for unsupervised learning for the text, and then we can use the embedding for supervised fine-tuning for the following tasks.<br>Note that it is one direction.</p><h3 id=bidirectional-encoder-representations-from-transformers>Bidirectional Encoder Representations from Transformers</h3><p>Bert is especially tuned for:</p><ul><li>Predict a word with k% missing.</li><li>Predict a following sentence.</li></ul><h3 id=transformer>Transformer</h3><h4 id=self-attention>Self Attention</h4><p>We compute the ralationship between words in self attention. The encoder has multiple self-attention layers, after which the input gets the context information for every word. The decoder has a similar structure, but it takes the output of itself and the output of encoder as its input.</p><h5 id=the-structure-of-self-attention>The Structure of Self Attention</h5><ul><li>MatMul</li><li>Scale</li><li>Mask</li><li>SoftMax</li><li>MatMul
For the encoder, the input is Query, Key and Value, and $d_k$ denotes that dimension of query and vectors.
$$
Attention(Q, K, V) = softmax(\frac{QK^T}{\sqrt{d_k}})V
$$</li></ul><h4 id=the-structure-of-a-transformer>The Structure of a transformer</h4><ul><li>Positional encoding</li><li>Multi-Head attention</li><li>Add & Normalize</li><li>Feed forward
Position encoding allows the model to learn about the position of a word in the sentence.<br>Multi-head attention contains multiple self-attention layers, and different head may learn different information of the features.<br>The Add part is very similar to Residual Connection, which can pass the information of the former layer to the next one.</li></ul></section><footer class=article-footer><section class=article-tags></section><section class=article-copyright><svg xmlns="http://www.w3.org/2000/svg" class="icon icon-tabler icon-tabler-copyright" width="24" height="24" viewBox="0 0 24 24" stroke-width="2" stroke="currentcolor" fill="none" stroke-linecap="round" stroke-linejoin="round"><path stroke="none" d="M0 0h24v24H0z"/><circle cx="12" cy="12" r="9"/><path d="M14.5 9a3.5 4 0 100 6"/></svg><span>Licensed under CC BY-NC-SA 4.0</span></section></footer></article><aside class=related-contents--wrapper><h2 class=section-title>Related contents</h2><div class=related-contents><div class="flex article-list--tile"><article><a href=/posts/golang-essentials/><div class=article-details><h2 class=article-title>Golang Essentials</h2></div></a></article></div></div></aside><footer class=site-footer><section class=copyright>&copy;
2020 -
2022 0x000216</section><section class=powerby>Built with <a href=https://gohugo.io/ target=_blank rel=noopener>Hugo</a><br>Theme <b><a href=https://github.com/CaiJimmy/hugo-theme-stack target=_blank rel=noopener data-version=3.11.0>Stack</a></b> designed by <a href=https://jimmycai.com target=_blank rel=noopener>Jimmy</a></section><script data-name=BMC-Widget src=https://cdn.buymeacoffee.com/widget/1.0.0/prod/widget.prod.min.js data-id=0x000216 data-description=coffee! data-message=coffee! data-color=#FF813F data-position=right data-x_margin=28 data-y_margin=18></script></footer><div class=pswp tabindex=-1 role=dialog aria-hidden=true><div class=pswp__bg></div><div class=pswp__scroll-wrap><div class=pswp__container><div class=pswp__item></div><div class=pswp__item></div><div class=pswp__item></div></div><div class="pswp__ui pswp__ui--hidden"><div class=pswp__top-bar><div class=pswp__counter></div><button class="pswp__button pswp__button--close" title="Close (Esc)"></button>
<button class="pswp__button pswp__button--share" title=Share></button>
<button class="pswp__button pswp__button--fs" title="Toggle fullscreen"></button>
<button class="pswp__button pswp__button--zoom" title="Zoom in/out"></button><div class=pswp__preloader><div class=pswp__preloader__icn><div class=pswp__preloader__cut><div class=pswp__preloader__donut></div></div></div></div></div><div class="pswp__share-modal pswp__share-modal--hidden pswp__single-tap"><div class=pswp__share-tooltip></div></div><button class="pswp__button pswp__button--arrow--left" title="Previous (arrow left)"></button>
<button class="pswp__button pswp__button--arrow--right" title="Next (arrow right)"></button><div class=pswp__caption><div class=pswp__caption__center></div></div></div></div></div><script src=https://cdn.jsdelivr.net/npm/photoswipe@4.1.3/dist/photoswipe.min.js integrity="sha256-ePwmChbbvXbsO02lbM3HoHbSHTHFAeChekF1xKJdleo=" crossorigin=anonymous defer></script><script src=https://cdn.jsdelivr.net/npm/photoswipe@4.1.3/dist/photoswipe-ui-default.min.js integrity="sha256-UKkzOn/w1mBxRmLLGrSeyB4e1xbrp4xylgAWb3M42pU=" crossorigin=anonymous defer></script><link rel=stylesheet href=https://cdn.jsdelivr.net/npm/photoswipe@4.1.3/dist/default-skin/default-skin.css integrity="sha256-c0uckgykQ9v5k+IqViZOZKc47Jn7KQil4/MP3ySA3F8=" crossorigin=anonymous><link rel=stylesheet href=https://cdn.jsdelivr.net/npm/photoswipe@4.1.3/dist/photoswipe.css integrity="sha256-SBLU4vv6CA6lHsZ1XyTdhyjJxCjPif/TRkjnsyGAGnE=" crossorigin=anonymous></main><aside class="sidebar right-sidebar sticky"><section class="widget archives"><div class=widget-icon><svg xmlns="http://www.w3.org/2000/svg" class="icon icon-tabler icon-tabler-hash" width="24" height="24" viewBox="0 0 24 24" stroke-width="2" stroke="currentcolor" fill="none" stroke-linecap="round" stroke-linejoin="round"><path stroke="none" d="M0 0h24v24H0z"/><line x1="5" y1="9" x2="19" y2="9"/><line x1="5" y1="15" x2="19" y2="15"/><line x1="11" y1="4" x2="7" y2="20"/><line x1="17" y1="4" x2="13" y2="20"/></svg></div><h2 class="widget-title section-title">Table of contents</h2><div class=widget--toc><nav id=TableOfContents><ol><li><a href=#how-to-define-the-meaning-of-a-word>How to define the meaning of a word</a><ol><li><a href=#one-hot-vector>One-Hot Vector</a><ol><li><a href=#cons>Cons:</a></li></ol></li><li><a href=#word2vec>Word2Vec</a><ol><li><a href=#skip-gram-model>Skip-Gram Model</a></li></ol></li><li><a href=#single-value-decomposition>Single Value Decomposition</a></li><li><a href=#glove>GloVe</a></li></ol></li><li><a href=#named-entity-recognition>Named Entity Recognition</a><ol><li><a href=#window-classification>Window Classification</a></li></ol></li><li><a href=#syntactic-structure-analaysis>Syntactic Structure Analaysis</a><ol><li><a href=#constituency-parsing>Constituency Parsing</a></li><li><a href=#dependency-parsing>Dependency Parsing</a><ol><li><a href=#transition-based-dependency-parsing>Transition-based Dependency Parsing</a></li></ol></li></ol></li><li><a href=#language-model>Language Model</a><ol><li><a href=#n-gram-model>N-Gram Model</a></li><li><a href=#recurrent-neural-network>Recurrent Neural Network</a></li></ol></li><li><a href=#gradient-vanishing-and-gradient-explosion>Gradient Vanishing and Gradient Explosion</a><ol><li><a href=#gradient-clipping>Gradient clipping</a></li></ol></li><li><a href=#long-short-term-memory-lstm>Long Short Term Memory (LSTM)</a></li><li><a href=#neural-machine-translation>Neural Machine Translation</a><ol><li><a href=#bilingual-evaluation-study-bleu>Bilingual Evaluation Study (BLEU)</a></li><li><a href=#attention>Attention</a><ol><li><a href=#process>Process:</a></li></ol></li></ol></li><li><a href=#question-answering-system>Question Answering System</a><ol><li><a href=#stanford-attention-reader>Stanford Attention Reader</a></li><li><a href=#birectional-attention-flow>Birectional Attention Flow</a></li></ol></li><li><a href=#convolutional-neural-network>Convolutional Neural Network</a><ol><li><a href=#quasi-recurrent-neural-network>Quasi Recurrent Neural Network</a></li></ol></li><li><a href=#subword-model>Subword Model</a><ol><li><a href=#character-level-model>Character-level model</a></li><li><a href=#byte-pair-encoding>Byte Pair Encoding</a></li><li><a href=#fastttext>FasttText</a></li></ol></li><li><a href=#contextual-word-representation>Contextual Word Representation</a><ol><li><a href=#embeddings-from-language-models-elmo>Embeddings from Language Models (ELMO)</a></li><li><a href=#generative-pre-training-gpt>Generative Pre Training (GPT)</a></li><li><a href=#bidirectional-encoder-representations-from-transformers>Bidirectional Encoder Representations from Transformers</a></li><li><a href=#transformer>Transformer</a><ol><li><a href=#self-attention>Self Attention</a></li><li><a href=#the-structure-of-a-transformer>The Structure of a transformer</a></li></ol></li></ol></li></ol></nav></div></section></aside></div><script src=https://cdn.jsdelivr.net/npm/node-vibrant@3.1.5/dist/vibrant.min.js integrity="sha256-5NovOZc4iwiAWTYIFiIM7DxKUXKWvpVEuMEPLzcm5/g=" crossorigin=anonymous></script><script type=text/javascript src=/ts/main.js defer></script>
<script>(function(){const e=document.createElement("link");e.href="https://fonts.googleapis.com/css2?family=Lato:wght@300;400;700&display=swap",e.type="text/css",e.rel="stylesheet",document.head.appendChild(e)})()</script></body></html>