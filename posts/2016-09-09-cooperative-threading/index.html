<!doctype html><html lang=en-us dir=ltr><head><meta charset=utf-8><meta name=viewport content="width=device-width,initial-scale=1"><meta name=description content="byuu.net design cooperative-threading Cooperative Threading - Overview2019-10-05 (updated 2019-10-17) Perhaps the largest challenge to writing an emulator is grappling with programming code being inherently serial, whereas the emulated systems often contain many, sometimes even dozens, of parallel processes all running at the same time.
In fact, most of the overhead of writing a retro video game system emulator is managing the synchronization between components in this environment.
In this article, I&amp;rsquo;ll go into the approach I chose for my emulators: cooperative multi-threading."><title>Cooperative Threading</title><link rel=canonical href=https://Nexus-Security.github.io/posts/2016-09-09-cooperative-threading/><link rel=stylesheet href=/scss/style.min.7dbfdd4b0c439bdacf631096fda79b869b5850a9d35a3f67b9a557f3010f3972.css><meta property="og:title" content="Cooperative Threading"><meta property="og:description" content="byuu.net design cooperative-threading Cooperative Threading - Overview2019-10-05 (updated 2019-10-17) Perhaps the largest challenge to writing an emulator is grappling with programming code being inherently serial, whereas the emulated systems often contain many, sometimes even dozens, of parallel processes all running at the same time.
In fact, most of the overhead of writing a retro video game system emulator is managing the synchronization between components in this environment.
In this article, I&amp;rsquo;ll go into the approach I chose for my emulators: cooperative multi-threading."><meta property="og:url" content="https://Nexus-Security.github.io/posts/2016-09-09-cooperative-threading/"><meta property="og:site_name" content="0x000216"><meta property="og:type" content="article"><meta property="article:section" content="Posts"><meta property="article:published_time" content="2019-11-13T05:23:00+01:00"><meta property="article:modified_time" content="2019-11-13T05:23:00+01:00"><meta name=twitter:title content="Cooperative Threading"><meta name=twitter:description content="byuu.net design cooperative-threading Cooperative Threading - Overview2019-10-05 (updated 2019-10-17) Perhaps the largest challenge to writing an emulator is grappling with programming code being inherently serial, whereas the emulated systems often contain many, sometimes even dozens, of parallel processes all running at the same time.
In fact, most of the overhead of writing a retro video game system emulator is managing the synchronization between components in this environment.
In this article, I&amp;rsquo;ll go into the approach I chose for my emulators: cooperative multi-threading."></head><body class=article-page><script>(function(){const e="StackColorScheme";localStorage.getItem(e)||localStorage.setItem(e,"light")})()</script><script>(function(){const t="StackColorScheme",e=localStorage.getItem(t),n=window.matchMedia("(prefers-color-scheme: dark)").matches===!0;e=="dark"||e==="auto"&&n?document.documentElement.dataset.scheme="dark":document.documentElement.dataset.scheme="light"})()</script><div class="container main-container flex on-phone--column extended"><aside class="sidebar left-sidebar sticky"><button class="hamburger hamburger--spin" type=button id=toggle-menu aria-label="Toggle Menu">
<span class=hamburger-box><span class=hamburger-inner></span></span></button><header><figure class=site-avatar><a href=/><img src=/img/avatar_hue825486955cd7c56d95e38b4bd2a8e3c_229979_300x0_resize_box_3.png width=300 height=300 class=site-logo loading=lazy alt=Avatar></a></figure><div class=site-meta><h1 class=site-name><a href=/>0x000216</a></h1><h2 class=site-description>Where Do Russian Hackers Store Their Exploits? ðŸ¤“ /ussr/bin/ ðŸ˜‹</h2></div></header><ol class=social-menu><li><a href=https://github.com/Nexus-Security target=_blank title=GitHub><svg width="72" height="72" viewBox="0 0 72 72" xmlns="http://www.w3.org/2000/svg"><g fill="none" fill-rule="evenodd"><path d="M36 72c19.882251.0 36-16.117749 36-36 0-19.882251-16.117749-36-36-36-19.882251 365231026e-23-36 16.117749-36 36C24348735e-22 55.882251 16.117749 72 36 72z" fill="#3e75c3"/><path d="M35.9985 12C22.746 12 12 22.7870921 12 36.096644c0 10.6440272 6.876 19.6751861 16.4145 22.8617681C29.6145 59.1797862 30.0525 58.4358488 30.0525 57.7973276 30.0525 57.2250681 30.0315 55.7100863 30.0195 53.6996482c-6.6765 1.4562499-8.085-3.2302544-8.085-3.2302544-1.0905-2.7829884-2.664-3.5239139-2.664-3.5239139C17.091 45.4500754 19.4355 45.4801943 19.4355 45.4801943c2.4075.1701719 3.675 2.4833051 3.675 2.4833051 2.142 3.6820383 5.6175 2.6188404 6.9855 2.0014024C30.3135 48.4077535 30.9345 47.3460615 31.62 46.7436831 26.2905 46.1352808 20.688 44.0691228 20.688 34.8361671c0-2.6308879.9345-4.781379 2.4705-6.4665327C22.911 27.7597262 22.0875 25.3110578 23.3925 21.9934585c0 0 2.016-.6475568 6.6 2.4697516C31.908 23.9285993 33.96 23.6620468 36.0015 23.6515052 38.04 23.6620468 40.0935 23.9285993 42.0105 24.4632101c4.581-3.1173084 6.5925-2.4697516 6.5925-2.4697516C49.9125 25.3110578 49.089 27.7597262 48.8415 28.3696344 50.3805 30.0547881 51.309 32.2052792 51.309 34.8361671c0 9.2555448-5.6115 11.29309-10.9575 11.8894446.860999999999997.7439374 1.629 2.2137408 1.629 4.4621184C41.9805 54.4089489 41.9505 57.0067059 41.9505 57.7973276 41.9505 58.4418726 42.3825 59.1918338 43.6005 58.9554002 53.13 55.7627944 60 46.7376593 60 36.096644 60 22.7870921 49.254 12 35.9985 12" fill="#fff"/></g></svg></a></li><li><a href=mailto:0x000216@gmail.com target=_blank title=Email><svg id="Layer_1" xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink" viewBox="0 0 512 512" style="enable-background:new 0 0 512 512"><path style="fill:#e6f3ff" d="M512 105.739v300.522c0 27.715-22.372 50.087-50.087 50.087H50.087C22.372 456.348.0 433.976.0 406.261V105.739c0-.89.0-1.781.111-2.671 1.336-25.6 21.704-45.969 47.304-47.304.89-.111 1.781-.111 2.671-.111h411.826c.89.0 1.892.0 2.783.111 25.489 1.336 45.857 21.704 47.193 47.193C512 103.847 512 104.849 512 105.739z"/><path style="fill:#cfdbe6" d="M464.696 55.763c-.892-.111-1.891-.111-2.783-.111H256v400.696h205.913c27.715.0 50.087-22.372 50.087-50.087V105.739c0-.89.0-1.892-.111-2.783C510.553 77.468 490.184 57.099 464.696 55.763z"/><path style="fill:#ff4b26" d="M511.889 102.957c-1.336-25.489-21.704-45.857-47.193-47.193C382.89 137.569 336.951 183.509 256 264.459 225.291 233.732 77.61 85.958 47.416 55.763c-25.6 1.336-45.969 21.704-47.304 47.304C0 103.958.0 104.849.0 105.739v300.522c0 27.715 22.372 50.087 50.087 50.087h16.696V169.739l165.621 165.51c6.456 6.567 15.026 9.795 23.597 9.795 8.57.0 17.141-3.228 23.597-9.795l165.621-165.621v286.72h16.696c27.715.0 50.087-22.372 50.087-50.087V105.739C512 104.849 512 103.847 511.889 102.957z"/><path style="fill:#d93f21" d="M279.596 335.249l165.621-165.621v286.72h16.696c27.715.0 50.087-22.372 50.087-50.087V105.739c0-.89.0-1.892-.111-2.783-1.336-25.489-21.704-45.857-47.193-47.193C382.891 137.569 336.951 183.509 256 264.459v80.584C264.57 345.043 273.141 341.816 279.596 335.249z"/><g/><g/><g/><g/><g/><g/><g/><g/><g/><g/><g/><g/><g/><g/><g/></svg></a></li><li><a href=https://www.buymeacoffee.com/0x000216 target=_blank title=RSS><svg id="Layer_1" xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink" viewBox="0 0 512 512" style="enable-background:new 0 0 512 512"><path style="fill:#f78c20" d="M78.333 355.334C35.14 355.334.0 390.474.0 433.667S35.14 512 78.333 512s78.333-35.14 78.333-78.333-35.14-78.333-78.333-78.333z"/><g><path style="fill:#ffa929" d="M78.333 381.445c-28.795.0-52.222 23.427-52.222 52.222s23.427 52.222 52.222 52.222 52.222-23.427 52.222-52.222-23.427-52.222-52.222-52.222z"/><path style="fill:#ffa929" d="M477.918 264.861c-21.843-51.641-53.111-98.019-92.936-137.842-39.824-39.824-86.201-71.093-137.843-92.935C193.669 11.468 136.874.0 78.333.0c-4.807.0-8.704 3.897-8.704 8.704v85.519c0 4.807 3.897 8.704 8.704 8.704 182.37.0 330.74 148.369 330.74 330.74.0 4.807 3.897 8.704 8.704 8.704h85.52c4.807.0 8.704-3.897 8.704-8.704C512 375.126 500.533 318.331 477.918 264.861z"/><path style="fill:#ffa929" d="M78.333 163.853c-4.807.0-8.704 3.897-8.704 8.704v95.74c0 4.807 3.897 8.704 8.704 8.704 86.386.0 156.666 70.281 156.666 156.666.0 4.807 3.897 8.704 8.704 8.704h95.74c4.807.0 8.704-3.897 8.704-8.704.0-72.07-28.065-139.826-79.027-190.787-50.961-50.961-118.717-79.027-190.787-79.027z"/></g><g><path style="fill:#f78c20" d="M78.333 242.186c-2.918.0-5.817.076-8.704.206v25.905c0 4.807 3.897 8.704 8.704 8.704 86.386.0 156.666 70.281 156.666 156.666.0 4.807 3.897 8.704 8.704 8.704h25.905c.129-2.886.206-5.786.206-8.704.0-105.752-85.729-191.481-191.481-191.481z"/><path style="fill:#f78c20" d="M78.333 68.113c-2.91.0-5.81.042-8.704.11v26.001c0 4.807 3.897 8.704 8.704 8.704 182.37.0 330.74 148.369 330.74 330.74.0 4.807 3.897 8.704 8.704 8.704h26.001c.067-2.894.11-5.793.11-8.704C443.887 231.777 280.223 68.113 78.333 68.113z"/></g><g/><g/><g/><g/><g/><g/><g/><g/><g/><g/><g/><g/><g/><g/><g/></svg></a></li><li><a href target=_blank title=Slack><svg width="256" height="256" viewBox="0 0 256 256" xmlns="http://www.w3.org/2000/svg" preserveAspectRatio="xMidYMid"><path d="M165.964 15.838c-3.89-11.975-16.752-18.528-28.725-14.636-11.975 3.89-18.528 16.752-14.636 28.725l58.947 181.365c4.048 11.187 16.132 17.473 27.732 14.135 12.1-3.483 19.475-16.334 15.614-28.217L165.964 15.838" fill="#dfa22f"/><path d="M74.626 45.516C70.734 33.542 57.873 26.989 45.9 30.879 33.924 34.77 27.37 47.631 31.263 59.606l58.948 181.366c4.047 11.186 16.132 17.473 27.732 14.132 12.099-3.481 19.474-16.332 15.613-28.217L74.626 45.516" fill="#3cb187"/><path d="M240.162 166.045c11.975-3.89 18.526-16.75 14.636-28.726-3.89-11.973-16.752-18.527-28.725-14.636L44.708 181.632c-11.187 4.046-17.473 16.13-14.135 27.73 3.483 12.099 16.334 19.475 28.217 15.614l181.372-58.93" fill="#ce1e5b"/><path d="M82.508 217.27l43.347-14.084-14.086-43.352-43.35 14.09 14.089 43.347" fill="#392538"/><path d="M173.847 187.591c16.388-5.323 31.62-10.273 43.348-14.084l-14.088-43.36-43.35 14.09 14.09 43.354" fill="#bb242a"/><path d="M210.484 74.706c11.974-3.89 18.527-16.751 14.637-28.727-3.89-11.973-16.752-18.526-28.727-14.636L15.028 90.293C3.842 94.337-2.445 106.422.896 118.022c3.481 12.098 16.332 19.474 28.217 15.613l181.371-58.93" fill="#72c5cd"/><path d="M52.822 125.933c11.805-3.836 27.025-8.782 43.354-14.086-5.323-16.39-10.273-31.622-14.084-43.352l-43.36 14.092 14.09 43.346" fill="#248c73"/><path d="M144.16 96.256l43.356-14.088a546179.21 546179.21.0 00-14.089-43.36L130.07 52.9l14.09 43.356" fill="#62803a"/></svg></a></li><li><a href=https://www.buymeacoffee.com/0x000216 target=_blank title=Minds><svg id="Capa_1" xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink" viewBox="0 0 512 512" style="enable-background:new 0 0 512 512"><g><g><path style="fill:#ffe1b2" d="M256 33.085C245.078 13.38 224.079.0 2e2.0c-23.781.0-45.57 13.293-56.594 34.184C115.711 41.602 96 66.977 96 96c0 .059.0.113.0.172-9.977 7.512-16 19.301-16 31.828.0 1.316.078 2.637.234 3.992C60.211 145.266 48 167.758 48 192c0 14.07 4.039 27.543 11.719 39.262C57.273 236.512 56 242.207 56 248c0 2.738.281 5.445.828 8.098C36.672 267.308 24 288.539 24 312c0 27.973 18.305 52.34 44.109 60.785C65.398 378.828 64 385.324 64 392c0 21.098 13.805 39.508 33.539 45.727C103.891 466.746 129.828 488 160 488c4.617.0 9.227-.512 13.766-1.527C181.992 502 198.141 512 216 512c16.687.0 31.396-8.567 40-21.523V33.085z"/></g><g><g><path style="fill:#ffb980" d="M264 256c-4.422.0-8-3.582-8-8 0-22.055-17.945-40-40-40-8.008.0-15.734 2.355-22.336 6.812-3.023 2.043-7.055 1.781-9.797-.652-3.156-2.809-8.477-6.16-15.867-6.16-4.422.0-8-3.582-8-8s3.578-8 8-8c7.711.0 15.234 2.293 21.719 6.539C197.773 194.246 206.758 192 216 192c30.875.0 56 25.121 56 56C272 252.418 268.422 256 264 256z"/></g></g><g><g><path style="fill:#ffb980" d="M120 120c18.977.0 36.875 7.312 50.414 20.594 3.141 3.09 8.203 3.047 11.312-.109 3.094-3.152 3.047-8.219-.109-11.312C165.07 112.941 143.187 104 120 104c-13.046.0-25.395 2.93-36.542 8.046C81.253 117.019 80 122.423 80 128c0 1.316.078 2.637.234 3.992-.094.062-.173.139-.267.202C91.423 124.501 105.193 120 120 120z"/></g></g><g><g><path style="fill:#ffb980" d="M216 360c0-4.418-3.578-8-8-8s-8 3.582-8 8c0 17.645-14.352 32-32 32-14.211.0-26.82-9.648-30.664-23.465-.703-2.512-2.578-4.523-5.039-5.395-2.453-.871-5.188-.492-7.305 1.02C114.094 371.906 101.305 376 88 376c-6.948.0-13.625-1.149-19.894-3.207-2.214 4.939-3.501 10.19-3.916 15.586C71.714 390.73 79.711 392 88 392c13.297.0 26.187-3.266 37.773-9.52C133.969 397.894 150.141 408 168 408c26.469.0 48-21.531 48-48z"/></g></g><g><path style="fill:#fdc88e" d="M488 312c0-23.461-12.672-44.691-32.828-55.902.547-2.652.828-5.359.828-8.098.0-5.793-1.273-11.488-3.719-16.738C459.961 219.543 464 206.07 464 192c0-24.242-12.211-46.734-32.234-60.008.156-1.355.234-2.676.234-3.992.0-12.527-6.023-24.316-16-31.828.0-.059.0-.113.0-.172.0-29.023-19.711-54.398-47.406-61.816C357.57 13.293 335.781.0 312 0c-24.08.0-45.078 13.38-56 33.085v457.391C264.604 503.433 279.313 512 296 512c17.859.0 34.008-10 42.234-25.527C342.773 487.488 347.383 488 352 488c30.172.0 56.109-21.254 62.461-50.273C434.195 431.508 448 413.097 448 392c0-6.676-1.398-13.172-4.109-19.215C469.695 364.34 488 339.973 488 312z"/></g><g><path style="fill:#f8ab6b" d="M272.008 151.199C272 151.465 272 151.734 272 152c0 26.469 21.531 48 48 48s48-21.531 48-48c0-4.418-3.578-8-8-8s-8 3.582-8 8c0 17.645-14.352 32-32 32s-32-14.355-32-32c0-2.184.219-4.359.656-6.465.492-2.395-.133-4.883-1.703-6.754-1.57-1.871-4.016-3.066-6.352-2.859-.453.012-.891.059-.602.078-13.234.0-24-10.766-24-24v31.813C260.673 147.348 266.061 149.988 272.008 151.199z"/></g><g><path style="fill:#f8ab6b" d="M296 328c9.242.0 18.219-2.246 26.281-6.539C328.765 325.707 336.289 328 344 328c4.422.0 8-3.582 8-8s-3.578-8-8-8c-7.391.0-12.711-3.352-15.867-6.16-2.742-2.434-6.766-2.695-9.797-.656C311.726 309.644 304 312 296 312c-22.055.0-40-17.945-40-40v39.116C266.174 321.517 280.337 328 296 328z"/></g><g><g><path style="fill:#f8ab6b" d="M431.765 131.992c.156-1.355.234-2.676.234-3.992.0-5.577-1.253-10.981-3.458-15.954C417.395 106.93 405.046 104 392 104c-4.422.0-8 3.582-8 8s3.578 8 8 8c14.807.0 28.577 4.501 40.032 12.194C431.939 132.131 431.859 132.054 431.765 131.992z"/></g></g><g><g><path style="fill:#f8ab6b" d="M447.81 388.38c-.415-5.396-1.702-10.647-3.916-15.586C437.624 374.85 430.948 376 424 376c-13.578.0-26.594-4.266-37.641-12.332-2.07-1.5-4.719-1.93-7.133-1.168-2.43.77-4.344 2.648-5.164 5.059C369.101 382.176 355.414 392 340 392c-4.422.0-8 3.582-8 8s3.578 8 8 8c18.875.0 35.961-10.191 45.094-26.156C396.976 388.512 410.258 392 424 392 432.288 392 440.285 390.73 447.81 388.38z"/></g></g></g><g/><g/><g/><g/><g/><g/><g/><g/><g/><g/><g/><g/><g/><g/><g/></svg></a></li><li><a href=https://www.buymeacoffee.com/0x000216 target=_blank title=Coffee><svg id="Capa_1" xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink" viewBox="0 0 340 340" style="enable-background:new 0 0 340 340"><g id="XMLID_18_"><polygon id="XMLID_138_" style="fill:#dedde0" points="76.429,290 80,340 170,340 170,290"/><polygon id="XMLID_169_" style="fill:#dedde0" points="170,80 61.429,80 65,130 170,130"/><polygon id="XMLID_197_" style="fill:#acabb1" points="170,290 170,340 260,340 263.571,290"/><polygon id="XMLID_221_" style="fill:#acabb1" points="170,80 170,130 275,130 278.571,80"/><path id="XMLID_222_" style="fill:#ffda44" d="M170 260c-22.091.0-40-22.386-40-50s17.909-50 40-50v-30H65 50l10 160h16.429H170V260z"/><path id="XMLID_33_" style="fill:#ff9811" d="M170 130v30c22.091.0 40 22.386 40 50s-17.909 50-40 50v30h93.571H280l10-160h-15H170z"/><path id="XMLID_223_" style="fill:#50412e" d="M210 210c0-27.614-17.909-50-40-50v1e2c22.091.0 40-22.386 40-50z"/><path id="XMLID_224_" style="fill:#786145" d="M130 210c0 27.614 17.909 50 40 50V160c-22.091.0-40 22.386-40 50z"/><polygon id="XMLID_225_" style="fill:#50412e" points="278.571,80 300,80 300,40 260,40 260,0 80,0 80,40 40,40 40,80 61.429,80 170,80"/></g><g/><g/><g/><g/><g/><g/><g/><g/><g/><g/><g/><g/><g/><g/><g/></svg></a></li></ol><ol class=menu id=main-menu><li><a href=/><svg xmlns="http://www.w3.org/2000/svg" class="icon icon-tabler icon-tabler-home" width="24" height="24" viewBox="0 0 24 24" stroke-width="2" stroke="currentcolor" fill="none" stroke-linecap="round" stroke-linejoin="round"><path stroke="none" d="M0 0h24v24H0z"/><polyline points="5 12 3 12 12 3 21 12 19 12"/><path d="M5 12v7a2 2 0 002 2h10a2 2 0 002-2v-7"/><path d="M9 21v-6a2 2 0 012-2h2a2 2 0 012 2v6"/></svg><span>Home</span></a></li><li><a href=/about/><svg xmlns="http://www.w3.org/2000/svg" class="icon icon-tabler icon-tabler-user" width="24" height="24" viewBox="0 0 24 24" stroke-width="2" stroke="currentcolor" fill="none" stroke-linecap="round" stroke-linejoin="round"><path stroke="none" d="M0 0h24v24H0z"/><circle cx="12" cy="7" r="4"/><path d="M6 21v-2a4 4 0 014-4h4a4 4 0 014 4v2"/></svg><span>About Us</span></a></li><li><a href=/termsofservice/><svg xmlns="http://www.w3.org/2000/svg" class="icon icon-tabler icon-tabler-pencil" width="24" height="24" viewBox="0 0 24 24" stroke-width="2" stroke="currentcolor" fill="none" stroke-linecap="round" stroke-linejoin="round"><path stroke="none" d="M0 0h24v24H0z" fill="none"/><path d="M4 20h4L18.5 9.5a1.5 1.5.0 00-4-4L4 16v4"/><line x1="13.5" y1="6.5" x2="17.5" y2="10.5"/></svg><span>Terms Of Service</span></a></li><li><a href=/privacypolicy/><svg xmlns="http://www.w3.org/2000/svg" class="icon icon-tabler icon-tabler-archive" width="24" height="24" viewBox="0 0 24 24" stroke-width="2" stroke="currentcolor" fill="none" stroke-linecap="round" stroke-linejoin="round"><path stroke="none" d="M0 0h24v24H0z"/><rect x="3" y="4" width="18" height="4" rx="2"/><path d="M5 8v10a2 2 0 002 2h10a2 2 0 002-2V8"/><line x1="10" y1="12" x2="14" y2="12"/></svg><span>Privacy Policy</span></a></li><li><a href=/disclaimer/><svg xmlns="http://www.w3.org/2000/svg" class="icon icon-tabler icon-tabler-link" width="24" height="24" viewBox="0 0 24 24" stroke-width="2" stroke="currentcolor" fill="none" stroke-linecap="round" stroke-linejoin="round"><path stroke="none" d="M0 0h24v24H0z"/><path d="M10 14a3.5 3.5.0 005 0l4-4a3.5 3.5.0 00-5-5l-.5.5"/><path d="M14 10a3.5 3.5.0 00-5 0l-4 4a3.5 3.5.0 005 5l.5-.5"/></svg><span>Disclaimer</span></a></li><li><a href=/contact/><svg xmlns="http://www.w3.org/2000/svg" class="icon icon-tabler icon-tabler-mail" width="44" height="44" viewBox="0 0 24 24" stroke-width="1.5" stroke="currentcolor" fill="none" stroke-linecap="round" stroke-linejoin="round"><path stroke="none" d="M0 0h24v24H0z" fill="none"/><rect x="3" y="5" width="18" height="14" rx="2"/><polyline points="3 7 12 13 21 7"/></svg><span>Contact Us</span></a></li><div class=menu-bottom-section><li id=dark-mode-toggle><svg xmlns="http://www.w3.org/2000/svg" class="icon icon-tabler icon-tabler-toggle-left" width="24" height="24" viewBox="0 0 24 24" stroke-width="2" stroke="currentcolor" fill="none" stroke-linecap="round" stroke-linejoin="round"><path stroke="none" d="M0 0h24v24H0z"/><circle cx="8" cy="12" r="2"/><rect x="2" y="6" width="20" height="12" rx="6"/></svg><svg xmlns="http://www.w3.org/2000/svg" class="icon icon-tabler icon-tabler-toggle-right" width="24" height="24" viewBox="0 0 24 24" stroke-width="2" stroke="currentcolor" fill="none" stroke-linecap="round" stroke-linejoin="round"><path stroke="none" d="M0 0h24v24H0z"/><circle cx="16" cy="12" r="2"/><rect x="2" y="6" width="20" height="12" rx="6"/></svg><span>Dark Mode</span></li></div></ol></aside><main class="main full-width"><article class=main-article><header class=article-header><div class=article-details><div class=article-title-wrapper><h2 class=article-title><a href=/posts/2016-09-09-cooperative-threading/>Cooperative Threading</a></h2></div><footer class=article-time><div><svg xmlns="http://www.w3.org/2000/svg" class="icon icon-tabler icon-tabler-calendar-time" width="56" height="56" viewBox="0 0 24 24" stroke-width="2" stroke="currentcolor" fill="none" stroke-linecap="round" stroke-linejoin="round"><path stroke="none" d="M0 0h24v24H0z"/><path d="M11.795 21H5a2 2 0 01-2-2V7a2 2 0 012-2h12a2 2 0 012 2v4"/><circle cx="18" cy="18" r="4"/><path d="M15 3v4"/><path d="M7 3v4"/><path d="M3 11h16"/><path d="M18 16.496V18l1 1"/></svg><time class=article-time--published>Nov 13, 2019</time></div><div><svg xmlns="http://www.w3.org/2000/svg" class="icon icon-tabler icon-tabler-clock" width="24" height="24" viewBox="0 0 24 24" stroke-width="2" stroke="currentcolor" fill="none" stroke-linecap="round" stroke-linejoin="round"><path stroke="none" d="M0 0h24v24H0z"/><circle cx="12" cy="12" r="9"/><polyline points="12 7 12 12 15 15"/></svg><time class=article-time--reading>14 minute read</time></div></footer></div></header><section class=article-content><ol><li><a class=link href=https://byuu.net target=_blank rel=noopener>byuu.net</a></li><li><a class=link href=https://byuu.net/design target=_blank rel=noopener>design</a></li><li><a class=link href=https://byuu.net/design/cooperative-threading target=_blank rel=noopener>cooperative-threading</a></li></ol><h1 id=cooperative-threading---overview2019-10-05-updated-2019-10-17>Cooperative Threading - Overview2019-10-05 (updated 2019-10-17)</h1><p>Perhaps the largest challenge to writing an emulator is grappling with programming code being inherently serial, whereas the emulated systems often contain many, sometimes even dozens, of parallel processes all running at the same time.</p><p>In fact, most of the overhead of writing a retro video game system emulator is managing the synchronization between components in this environment.</p><p>In this article, I&rsquo;ll go into the approach I chose for my emulators: cooperative multi-threading. Please note that this is but one of several approaches, and I am not here to advocate for one or the other. I will do my best to present an unbiased look at its pros and cons.</p><h3 id=synchronization-overview>Synchronization Overview</h3><p>What we are trying to accomplish here is synchronization between emulated components.</p><p>Let&rsquo;s say we are emulating the base Super Nintendo, which has four major components: the general-purpose CPU, the audio APU, the video PPU, and the sound output DSP. Note that I&rsquo;m simplifying things a bit here (eg there are two PPU processors) for the sake of example.</p><p>Each of these chips run at clock rates of around ~20MHz.</p><p>Our emulator needs to run each chip in serial, or one at a time; whereas the real SNES would run all of these chips in parallel, or all at the same time.</p><p>After running one chip for a certain amount of time, we synchronize the other chips by running them for a period of time in order to catch up to the first chip.</p><p>The more accurate you want your emulation to be, the more fine-grained you want this synchronization to be.</p><p>You could execute one CPU opcode at a time, render one scanline at a time, and produce one audio sample at a time. And you would have a very fast SNES emulator which was about as accurate as those released in the late 1990s.</p><p>Or you could execute one CPU opcode cycle at a time, render one pixel at a time, and split audio sample generation into 32 distinct cycle stages. And you would likely have an emulator as demanding as bsnes.</p><p>The more fine-grained your emulation becomes, the more complex it gets to switch from one emulated component to another. Imagine you are emulating a CPU instruction, and after every cycle you need to switch to the video renderer to draw just one pixel and then return back to the CPU. That&rsquo;s a massive increase in technical complexity, which is what cooperative threading can help eliminate.</p><h3 id=preemptive-threading>Preemptive Threading</h3><p>First we must address the obvious: modern processors have many cores, and even in the rare event of single-core systems, modern operating systems provide threading facilities to allow multiple tasks to run in parallel in the form of threads. Can&rsquo;t we use these to write emulators?</p><p>The answer is, unfortunately, no.</p><p>Emulating the SNES accurately involves tens of millions of synchronizations, or context switches, per emulated second.</p><p>If your CPU only has one actual core, all of your threads will technically be running in serial anyway, and the OS will be performing context switches in kernel mode. Userland &lt;> kernel mode transitions are <em>extremely</em> painful for CPUs, and in the best case scenario, you might manage to do this a few hundred thousand times per second. Presuming you weren&rsquo;t emulating anything at all.</p><p>In the case of multi-core CPUs, things do get a bit better, but whether using a single core or multiple cores, you have to grapple with the fact that your PC is running at a different clock rate (let&rsquo;s say 3GHz) than the SNES (let&rsquo;s say 20MHz.) The way you&rsquo;d keep components in sync here would be to add locks, or mutexes, or semaphores, in order to make one thread wait for other threads after each slice of time has passed in each emulated thread.</p><p>These locks are immensely painful as well. Even if you bring them down to atomic instructions such as intelocked increment and interlocked decrement, you&rsquo;re still contending with modern CPUs being massively parallelized with huge pipelines and slow communication links between each CPU core.</p><p>Not to mention in a real, non-hypothetical scenario, it&rsquo;s trivial to end up with 16+ emulated threads (such as emulating a Sega CD 32X machine), and demanding that many cores to run an emulator would be a tough sell in 2019.</p><h3 id=state-machines>State Machines</h3><p>The traditional approach to retro emulator development is the use of state machines. As an example, here&rsquo;s a greatly simplified example of how one might split up a cycle-based CPU interpreter using state machines:</p><div class=highlight><div class=chroma><table class=lntable><tr><td class=lntd><pre tabindex=0 class=chroma><code><span class=lnt>1
</span></code></pre></td><td class=lntd><pre tabindex=0 class=chroma><code class=language-fallback data-lang=fallback><span class=line><span class=cl>void CPU::executeInstructionCycle() { cycle++; if(cycle == 1) { opcode = readMemory(PC++); return; } if(FlagM) switch(opcode) { //8-bit accumulator instructions case 0xb9: switch(cycle) { case 2: address = readMemory(PC++); return; case 3: address = readMemory(PC++) | address &lt;&lt; 8; return; case 4: //possible penalty cycle when crossing 8-bit page boundaries: if(address &gt;&gt; 8 != address + Y &gt;&gt; 8) { return; } cycle++; //cycle 4 not needed; fall through to cycle 5 case 5: A = readMemory(address + Y); cycle = 0; //end of instruction; start a new instruction next time return; } } }
</span></span></code></pre></td></tr></table></div></div><p>Splitting up a video renderer into individual pixel timeslices, or an audio generator into individual cycle timeslices, each has their own complexities as well.</p><p>Now let me add yet another horror: on the SNES, the above is not enough. Each CPU cycle consists of 6 - 12 master clock cycles, where the address bus is set to a given address, and after a certain amount of time, other chips will acknowledge the data on the bus. This is known as bus hold delays.</p><p>Basically, the CPU reading from the PPU cannot expect the PPU to respond with data instantaneously: it takes time. And if the CPU is writing to the PPU, the PPU needs some time as well.</p><p>So you would need to add a third-level state machine for every call to readMemory() above in order to split the reads into clock cycles:</p><div class=highlight><div class=chroma><table class=lntable><tr><td class=lntd><pre tabindex=0 class=chroma><code><span class=lnt>1
</span></code></pre></td><td class=lntd><pre tabindex=0 class=chroma><code class=language-fallback data-lang=fallback><span class=line><span class=cl>void CPU::executeInstructionCycle() { if(cycle == 1) { opcode = readMemory(PC++); cycle = 2; return; } if(FlagM) switch(opcode) { //8-bit accumulator instructions case 0xb9: switch(cycle) { case 2: switch(subcycle) { case 1: subcycle = 2; return; case 2: address = readMemory(PC++); subcycle = 1; return; } case 3: switch(subcycle) { case 1: subcycle = 2; return; case 2: address = readMemory(PC++) | address &lt;&lt; 8; subcycle = 1; return; } case 4: //possible penalty cycle when crossing 8-bit page boundaries: if(address &gt;&gt; 8 != address + Y &gt;&gt; 8) { return; } cycle++; //cycle 4 not needed; fall through to cycle 5 case 5: switch(subcycle) { case 1: subcycle = 2; return; case 2: A = readMemory(address + Y); subcycle = 1; cycle = 0; //end of instruction; start a new instruction next time return; } } } }
</span></span></code></pre></td></tr></table></div></div><p>And keep in mind, I&rsquo;m still greatly simplifying things here for the purpose of illustration.</p><h3 id=cooperative-threading>Cooperative Threading</h3><p>Enter cooperative threading: the idea is similar to preemptive threading, but instead of the kernel handling the threads, the userland process handles them instead.</p><p>Cooperative threads all run in serial: that is, only one runs at a time. When needed, a context switch will swap to another thread, which will resume exactly where it last left off.</p><p>Unlike coroutines, each cooperative thread has its own stack frame. This means that one thread can be several layers of function calls deep inside of a stack frame.</p><p>The state machine code you saw above is now <em>implicit</em>: it&rsquo;s part of the stack frame.</p><p>Thus, if we were to rewrite the code above, it would now look like this:</p><div class=highlight><div class=chroma><table class=lntable><tr><td class=lntd><pre tabindex=0 class=chroma><code><span class=lnt>1
</span></code></pre></td><td class=lntd><pre tabindex=0 class=chroma><code class=language-fallback data-lang=fallback><span class=line><span class=cl>void CPU::executeInstruction() { opcode = readMemory(PC++); if(FlagM) switch(opcode) { //8-bit accumulator instructions case 0xb9: address = readMemory(PC++); address = readMemory(PC++) | address &lt;&lt; 8; if(address &gt;&gt; 8 != address + Y &gt;&gt; 8) wait(6); A = readMemory(address + Y); } }
</span></span></code></pre></td></tr></table></div></div><p>Yes, it&rsquo;s really that dramatic of a difference.</p><p>How does it work? For that we look inside the readMemory() function:</p><div class=highlight><div class=chroma><table class=lntable><tr><td class=lntd><pre tabindex=0 class=chroma><code><span class=lnt>1
</span></code></pre></td><td class=lntd><pre tabindex=0 class=chroma><code class=language-fallback data-lang=fallback><span class=line><span class=cl>uint8\_t CPU::readMemory(uint16\_t address) { wait(2); uint8\_t data = memory\[address\]; wait(4); return data; }
</span></span></code></pre></td></tr></table></div></div><p>The trick is the call to wait():</p><div class=highlight><div class=chroma><table class=lntable><tr><td class=lntd><pre tabindex=0 class=chroma><code><span class=lnt>1
</span></code></pre></td><td class=lntd><pre tabindex=0 class=chroma><code class=language-fallback data-lang=fallback><span class=line><span class=cl>void CPU::wait(uint clockCycles) { apuCounter += clockCycles; while(apuCounter &gt; 0) yield(apu); }
</span></span></code></pre></td></tr></table></div></div><p>wait() just consumes clock cycles, and every time it does, if the CPU gets into a state where it&rsquo;s ahead of the APU, it yields execution to the APU, which then causes the APU to resume where it last left off. Once the APU has caught up and is now ahead of the CPU, it yields back to the CPU, and now the CPU resumes immediately after its last yield() point.</p><p>If you think of traditional programming functions as calls, and return statements as returning back to the call site, then you can think of cooperative threading as jumps. Instead of returning back, the APU jumps back to the CPU.</p><p>Technically, this is referred to as symmetric cooperative threads. There are also asymmetric cooperative threads that function like the typical call and return style, but they&rsquo;re not suitable in my opinion to emulation where many threads all run in parallel: the CPU may jump to the APU, and then the APU may jump to the PPU rather than back to the CPU.</p><h3 id=cooperative-threading-optimizations>Cooperative Threading Optimizations</h3><p>One thing you might have noticed: why are we always synchronizing the CPU to eg the APU after every clock cycle? What if the CPU was just reading from its own internal memory, which the APU has no access to?</p><p>When it comes to a state machine, it&rsquo;s an all-or-nothing approach: you have to split your emulated processors into the smallest timeslices possible and step by them.</p><p>Your main emulator scheduler could see that the CPU is not trying to access the APU on a given cycle, and run it more cycles before starting to run the APU a bit, but in practice you&rsquo;re already paying the overhead through the state machine anyway.</p><p>But cooperative threads present an opportunity for an optimization that I call just-in-time synchronization. Let&rsquo;s flesh out the earlier example more:</p><div class=highlight><div class=chroma><table class=lntable><tr><td class=lntd><pre tabindex=0 class=chroma><code><span class=lnt>1
</span></code></pre></td><td class=lntd><pre tabindex=0 class=chroma><code class=language-fallback data-lang=fallback><span class=line><span class=cl>uint8\_t CPU::readMemory(uint16\_t address) { wait(2); if(address &gt;= 0x2140 &amp;&amp; address &lt;= 0x2143) { //this read is going to access shared memory with the APU. //in other words, the APU might change this value before we read it. //as such, we \*must\* catch up the APU to the CPU here: while(apuCounter &gt; 0) yield(apu); } uint8\_t data = memory\[address\]; wait(4); return data; }
</span></span></code></pre></td></tr></table></div></div><p>void CPU::wait(uint clockCycles) { apuCounter += clockCycles; }</p><div class=highlight><div class=chroma><table class=lntable><tr><td class=lntd><pre tabindex=0 class=chroma><code><span class=lnt> 1
</span><span class=lnt> 2
</span><span class=lnt> 3
</span><span class=lnt> 4
</span><span class=lnt> 5
</span><span class=lnt> 6
</span><span class=lnt> 7
</span><span class=lnt> 8
</span><span class=lnt> 9
</span><span class=lnt>10
</span><span class=lnt>11
</span><span class=lnt>12
</span><span class=lnt>13
</span><span class=lnt>14
</span><span class=lnt>15
</span><span class=lnt>16
</span><span class=lnt>17
</span><span class=lnt>18
</span><span class=lnt>19
</span><span class=lnt>20
</span><span class=lnt>21
</span><span class=lnt>22
</span><span class=lnt>23
</span><span class=lnt>24
</span><span class=lnt>25
</span><span class=lnt>26
</span><span class=lnt>27
</span><span class=lnt>28
</span><span class=lnt>29
</span><span class=lnt>30
</span><span class=lnt>31
</span><span class=lnt>32
</span><span class=lnt>33
</span><span class=lnt>34
</span><span class=lnt>35
</span><span class=lnt>36
</span><span class=lnt>37
</span><span class=lnt>38
</span><span class=lnt>39
</span><span class=lnt>40
</span><span class=lnt>41
</span><span class=lnt>42
</span><span class=lnt>43
</span><span class=lnt>44
</span><span class=lnt>45
</span><span class=lnt>46
</span><span class=lnt>47
</span><span class=lnt>48
</span><span class=lnt>49
</span><span class=lnt>50
</span><span class=lnt>51
</span><span class=lnt>52
</span><span class=lnt>53
</span><span class=lnt>54
</span><span class=lnt>55
</span><span class=lnt>56
</span><span class=lnt>57
</span><span class=lnt>58
</span><span class=lnt>59
</span><span class=lnt>60
</span><span class=lnt>61
</span><span class=lnt>62
</span><span class=lnt>63
</span><span class=lnt>64
</span><span class=lnt>65
</span><span class=lnt>66
</span><span class=lnt>67
</span><span class=lnt>68
</span><span class=lnt>69
</span><span class=lnt>70
</span><span class=lnt>71
</span><span class=lnt>72
</span><span class=lnt>73
</span><span class=lnt>74
</span><span class=lnt>75
</span><span class=lnt>76
</span><span class=lnt>77
</span><span class=lnt>78
</span><span class=lnt>79
</span><span class=lnt>80
</span><span class=lnt>81
</span><span class=lnt>82
</span><span class=lnt>83
</span><span class=lnt>84
</span><span class=lnt>85
</span><span class=lnt>86
</span><span class=lnt>87
</span><span class=lnt>88
</span><span class=lnt>89
</span><span class=lnt>90
</span></code></pre></td><td class=lntd><pre tabindex=0 class=chroma><code class=language-fallback data-lang=fallback><span class=line><span class=cl>
</span></span><span class=line><span class=cl>What we&#39;ve done here is made it so the CPU will keep on running until it tries to read from a shared memory region with the APU. Only then will it catch up the APU to the CPU before performing the read.
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>If either the CPU doesn&#39;t need to synchronize to the APU often, _or vice versa_, then the number of context switches per second drops _dramatically_.
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>In the case of the SNES, my emulator [bsnes](https://bsnes.byuu.org) only has to synchronize the CPU and APU a few thousand times a second, instead of millions of times a second.
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>### Cooperative Threading Limitations
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>But what happens if you have two CPUs that share all of the memory address space with one another? Then unfortunately you wouldn&#39;t be able to know if the APU might modify the memory before the CPU (that&#39;s currently ahead) tries to read from it, and so you&#39;d always have to synchronize.
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>There are tricks to this, such as implementing a rollback mechanism as the Exodus emulator by Nemesis does, but you begin to lose the code simplification benefits of this approach in that case.
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>In the worst case, cooperative threading reverts to the same level of overhead as a state machine would.
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>### Pipeline Stalls
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>In fact, in the worst case, cooperative threading can be somewhat slower than the state machine approach.
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>Modern CPUs love to do many operations out-of-order, and build deep pipelines of queued instructions.
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>When cooperative threading changes the stack pointer, it tends to throw them into a panic.
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>As an analogy, say you have an assembly line. Along the line, the product is slowly assembled. Now say that in the middle of assembling a steady stream of products, an order comes in for a different products, and it cannot wait, but you only had one assembly line! You would have to stop the assembly line, take all the items off, and start on the new products. Now imagine you had to constantly shift back and forth between several products, tens of millions of times a second, and you start to understand why cycle-accurate retro gaming emulation is so demanding.
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>Now to be fair, state machines also suffer from this very same problem: it is still like grinding gears to a halt during operation to switch between one area of instructions (CPU emulation) and another (APU emulation): you&#39;re burning out your L1 instruction cache, etc.
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>But the need for cooperative threading to modify the stack pointer is a real penalty. In my admittedly non-scientific personal observations, if you can emulate a processor using a single-level state machine, it tends to run faster than a cooperative-threaded approach by a good margin. Whereas when you get into two (or even three or more) levels of state machines, such as the CPU opcode example abaove, the cooperative threading model comes out as the clear winner on performance. And if you are able to perform the just-in-time synchronization I previously discussed, there&#39;s no contest.
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>### Optimal Design
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>Thus, if your goal is maximum efficiency, an emulator should probably be designed to use cooperative threading where it works best, and state machines where it does not.
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>Me, I&#39;m really big on consistency, and so for my [higan emulator](https://higan.byuu.org), I chose to emulate every processor using cooperative threads. I knowingly pay a performance penalty to ensure consistency and code simplicity throughout the codebase. But of course, you don&#39;t _have_ to do that.
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>### Serialization
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>And now the elephant in the room: probably the biggest concern to emulator developers with cooperative threading is serialization: the act of saving and loading save states.
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>Save states are not only useful to save and restore your progress at any point (great for cheating a bit at those unforgiving games, too), they are also the building blocks for features like real-time rewind and run-ahead input latency reduction (each of which warrants its own separate article.)
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>When using state machines, serialization just means saving and restoring all of the variables used for it: opcode, cycle, and subcycle in the earlier example.
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>But when using cooperative threading, that information is, once again, implicit: it&#39;s stored on the individual stacks in the form of call frames and CPU context registers.
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>Implementing serialization with cooperative threading is possible, but it&#39;s a complicated topic, for which I have penned a separate article devoted to the topic. You can read that article here:
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>[Cooperative Threading - Synchronization](https://byuu.net/design/cooperative-synchronization)
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>### Code
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>So if you&#39;ve read this far and you&#39;re still considering giving cooperative threading a go, you&#39;ll need an implementation of it for your chosen language.
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>C++20 has proposed a coroutines extension, but since as of the time of writing this article, I don&#39;t have a working, mature implementation of it, I cannot say if it will be useful in emulation or not. Probably the biggest limitation for it is that C++20 coroutines are stackless, which vastly limits their potential for anything of even moderate complexity.
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>Alternatively, I&#39;ve developed my own cooperative threading library in C89 code, which can thusly be used in C++ code as well, which I call [libco](https://code.byuu.org/libco). You can browse its [source code here](https://github.com/byuu/bsnes/tree/master/libco).
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>Basically it requires dropping down to the assembler-level for each supported architecture to implement the context switching, as modifying the CPU context registers and stack frame directly is not permitted by most sane programming languages. But it&#39;s really not hard: an x86 context switch is only about twelve lines of assembler code. 32-bit ARM is a mere three.
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>libco is ISC licensed and supports a very large amount of CPU architectures, so you can skip having to do all of this work yourself if you want.
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>There are of course as many cooperative threading libraries as there are people talking about cooperative threading, so feel free to use another, or make your own.
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>For most languages there&#39;s probably an implementation of it somewhere. But perhaps not always.
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>### Closing
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>In any case, I really believe cooperative threading gives my emulators a unique advantage in having easier to read code by removing the burden of managing state machines, and it makes it trivial to be clock-cycle accurate as well.
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>Again, I&#39;m not saying this is the best way to write a retro emulator, but it&#39;s served me very well.
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>* * *
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>### Share
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>If you enjoyed the article, please help spread the word about it, if you don&#39;t mind. Thank you!
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>[![Twitter](https://byuu.net/images/social/twitter.svg)](https://twitter.com/home?status=https://byuu.net/design/cooperative-threading) [![Pinterest](https://byuu.net/images/social/pinterest.svg)](https://pinterest.com/pin/create/button/?url=https://byuu.net/design/cooperative-threading&amp;media=https://byuu.net/images/about/byuu.png&amp;description=Cooperative%20Threading%20-%20Overview) [![Facebook](https://byuu.net/images/social/facebook.svg)](https://facebook.com/sharer/sharer.php?u=https://byuu.net/design/cooperative-threading)
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>* * *
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>3 Comments
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>* * *
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>### Post Comment
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>  
</span></span><span class=line><span class=cl>  
</span></span><span class=line><span class=cl>from Hacker News https://ift.tt/2ALZL4A
</span></span></code></pre></td></tr></table></div></div></section><footer class=article-footer><section class=article-copyright><svg xmlns="http://www.w3.org/2000/svg" class="icon icon-tabler icon-tabler-copyright" width="24" height="24" viewBox="0 0 24 24" stroke-width="2" stroke="currentcolor" fill="none" stroke-linecap="round" stroke-linejoin="round"><path stroke="none" d="M0 0h24v24H0z"/><circle cx="12" cy="12" r="9"/><path d="M14.5 9a3.5 4 0 100 6"/></svg><span>Licensed under CC BY-NC-SA 4.0</span></section></footer></article><footer class=site-footer><script async src="https://pagead2.googlesyndication.com/pagead/js/adsbygoogle.js?client=ca-pub-3987358164777632" crossorigin=anonymous></script>
<script data-name=BMC-Widget src=https://cdn.buymeacoffee.com/widget/1.0.0/prod/widget.prod.min.js data-id=0x000216 data-description=coffee! data-message=coffee! data-color=#FF813F data-position=right data-x_margin=28 data-y_margin=18></script></footer><div class=pswp tabindex=-1 role=dialog aria-hidden=true><div class=pswp__bg></div><div class=pswp__scroll-wrap><div class=pswp__container><div class=pswp__item></div><div class=pswp__item></div><div class=pswp__item></div></div><div class="pswp__ui pswp__ui--hidden"><div class=pswp__top-bar><div class=pswp__counter></div><button class="pswp__button pswp__button--close" title="Close (Esc)"></button>
<button class="pswp__button pswp__button--share" title=Share></button>
<button class="pswp__button pswp__button--fs" title="Toggle fullscreen"></button>
<button class="pswp__button pswp__button--zoom" title="Zoom in/out"></button><div class=pswp__preloader><div class=pswp__preloader__icn><div class=pswp__preloader__cut><div class=pswp__preloader__donut></div></div></div></div></div><div class="pswp__share-modal pswp__share-modal--hidden pswp__single-tap"><div class=pswp__share-tooltip></div></div><button class="pswp__button pswp__button--arrow--left" title="Previous (arrow left)"></button>
<button class="pswp__button pswp__button--arrow--right" title="Next (arrow right)"></button><div class=pswp__caption><div class=pswp__caption__center></div></div></div></div></div><script src=https://cdn.jsdelivr.net/npm/photoswipe@4.1.3/dist/photoswipe.min.js integrity="sha256-ePwmChbbvXbsO02lbM3HoHbSHTHFAeChekF1xKJdleo=" crossorigin=anonymous defer></script><script src=https://cdn.jsdelivr.net/npm/photoswipe@4.1.3/dist/photoswipe-ui-default.min.js integrity="sha256-UKkzOn/w1mBxRmLLGrSeyB4e1xbrp4xylgAWb3M42pU=" crossorigin=anonymous defer></script><link rel=stylesheet href=https://cdn.jsdelivr.net/npm/photoswipe@4.1.3/dist/default-skin/default-skin.css integrity="sha256-c0uckgykQ9v5k+IqViZOZKc47Jn7KQil4/MP3ySA3F8=" crossorigin=anonymous><link rel=stylesheet href=https://cdn.jsdelivr.net/npm/photoswipe@4.1.3/dist/photoswipe.css integrity="sha256-SBLU4vv6CA6lHsZ1XyTdhyjJxCjPif/TRkjnsyGAGnE=" crossorigin=anonymous></main><aside class="sidebar right-sidebar sticky"><section class="widget archives"><div class=widget-icon><svg xmlns="http://www.w3.org/2000/svg" class="icon icon-tabler icon-tabler-hash" width="24" height="24" viewBox="0 0 24 24" stroke-width="2" stroke="currentcolor" fill="none" stroke-linecap="round" stroke-linejoin="round"><path stroke="none" d="M0 0h24v24H0z"/><line x1="5" y1="9" x2="19" y2="9"/><line x1="5" y1="15" x2="19" y2="15"/><line x1="11" y1="4" x2="7" y2="20"/><line x1="17" y1="4" x2="13" y2="20"/></svg></div><h2 class="widget-title section-title">Table of contents</h2><div class=widget--toc><nav id=TableOfContents><ol><li><ol><li><a href=#synchronization-overview>Synchronization Overview</a></li><li><a href=#preemptive-threading>Preemptive Threading</a></li><li><a href=#state-machines>State Machines</a></li><li><a href=#cooperative-threading>Cooperative Threading</a></li><li><a href=#cooperative-threading-optimizations>Cooperative Threading Optimizations</a></li></ol></li></ol></nav></div></section></aside></div><script src=https://cdn.jsdelivr.net/npm/node-vibrant@3.1.5/dist/vibrant.min.js integrity="sha256-5NovOZc4iwiAWTYIFiIM7DxKUXKWvpVEuMEPLzcm5/g=" crossorigin=anonymous></script><script type=text/javascript src=/ts/main.js defer></script>
<script>(function(){const e=document.createElement("link");e.href="https://fonts.googleapis.com/css2?family=Lato:wght@300;400;700&display=swap",e.type="text/css",e.rel="stylesheet",document.head.appendChild(e)})()</script></body></html>