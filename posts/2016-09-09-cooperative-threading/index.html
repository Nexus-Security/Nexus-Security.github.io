<!doctype html><html lang=en-us dir=ltr><head><meta charset=utf-8><meta name=viewport content="width=device-width,initial-scale=1"><meta name=description content="byuu.net design cooperative-threading Cooperative Threading - Overview2019-10-05 (updated 2019-10-17) Perhaps the largest challenge to writing an emulator is grappling with programming code being inherently serial, whereas the emulated systems often contain many, sometimes even dozens, of parallel processes all running at the same time.
In fact, most of the overhead of writing a retro video game system emulator is managing the synchronization between components in this environment.
In this article, I&amp;rsquo;ll go into the approach I chose for my emulators: cooperative multi-threading."><title>Cooperative Threading</title><link rel=canonical href=https://Nexus-Security.github.io/posts/2016-09-09-cooperative-threading/><link rel=stylesheet href=/scss/style.min.450926226e724574a6b936335ea06111f8aeb253d932c86cb2cc807341cd2889.css><meta property="og:title" content="Cooperative Threading"><meta property="og:description" content="byuu.net design cooperative-threading Cooperative Threading - Overview2019-10-05 (updated 2019-10-17) Perhaps the largest challenge to writing an emulator is grappling with programming code being inherently serial, whereas the emulated systems often contain many, sometimes even dozens, of parallel processes all running at the same time.
In fact, most of the overhead of writing a retro video game system emulator is managing the synchronization between components in this environment.
In this article, I&amp;rsquo;ll go into the approach I chose for my emulators: cooperative multi-threading."><meta property="og:url" content="https://Nexus-Security.github.io/posts/2016-09-09-cooperative-threading/"><meta property="og:site_name" content="ZYChimne"><meta property="og:type" content="article"><meta property="article:section" content="Posts"><meta property="article:published_time" content="2019-11-13T05:23:00+01:00"><meta property="article:modified_time" content="2019-11-13T05:23:00+01:00"><meta name=twitter:title content="Cooperative Threading"><meta name=twitter:description content="byuu.net design cooperative-threading Cooperative Threading - Overview2019-10-05 (updated 2019-10-17) Perhaps the largest challenge to writing an emulator is grappling with programming code being inherently serial, whereas the emulated systems often contain many, sometimes even dozens, of parallel processes all running at the same time.
In fact, most of the overhead of writing a retro video game system emulator is managing the synchronization between components in this environment.
In this article, I&amp;rsquo;ll go into the approach I chose for my emulators: cooperative multi-threading."></head><body class=article-page><script>(function(){const e="StackColorScheme";localStorage.getItem(e)||localStorage.setItem(e,"auto")})()</script><script>(function(){const t="StackColorScheme",e=localStorage.getItem(t),n=window.matchMedia("(prefers-color-scheme: dark)").matches===!0;e=="dark"||e==="auto"&&n?document.documentElement.dataset.scheme="dark":document.documentElement.dataset.scheme="light"})()</script><div class="container main-container flex on-phone--column extended"><aside class="sidebar left-sidebar sticky"><button class="hamburger hamburger--spin" type=button id=toggle-menu aria-label="Toggle Menu">
<span class=hamburger-box><span class=hamburger-inner></span></span></button><header><figure class=site-avatar><a href=/><img src=/img/avatar_hue825486955cd7c56d95e38b4bd2a8e3c_229979_300x0_resize_box_3.png width=300 height=300 class=site-logo loading=lazy alt=Avatar></a></figure><div class=site-meta><h1 class=site-name><a href=/>ZYChimne</a></h1><h2 class=site-description>Computer Science, Wuhan University</h2></div></header><ol class=social-menu><li><a href=https://github.com/Nexus-Security target=_blank title=GitHub><svg xmlns="http://www.w3.org/2000/svg" class="icon icon-tabler icon-tabler-brand-github" width="24" height="24" viewBox="0 0 24 24" stroke-width="2" stroke="currentcolor" fill="none" stroke-linecap="round" stroke-linejoin="round"><path stroke="none" d="M0 0h24v24H0z" fill="none"/><path d="M9 19c-4.3 1.4-4.3-2.5-6-3m12 5v-3.5c0-1 .1-1.4-.5-2 2.8-.3 5.5-1.4 5.5-6a4.6 4.6.0 00-1.3-3.2 4.2 4.2.0 00-.1-3.2s-1.1-.3-3.5 1.3a12.3 12.3.0 00-6.2.0C6.5 2.8 5.4 3.1 5.4 3.1a4.2 4.2.0 00-.1 3.2A4.6 4.6.0 004 9.5c0 4.6 2.7 5.7 5.5 6-.6.6-.6 1.2-.5 2V21"/></svg></a></li><li><a href=mailto:0x000216@gmail.com target=_blank title=Email><svg xmlns="http://www.w3.org/2000/svg" class="icon icon-tabler icon-tabler-brand-gmail" width="44" height="44" viewBox="0 0 24 24" stroke-width="1.5" stroke="#2c3e50" fill="none" stroke-linecap="round" stroke-linejoin="round"><path stroke="none" d="M0 0h24v24H0z" fill="none"/><path d="M16 20h3a1 1 0 001-1V5a1 1 0 00-1-1h-3v16z"/><path d="M5 20h3V4H5A1 1 0 004 5v14a1 1 0 001 1z"/><path d="M16 4l-4 4-4-4"/><path d="M4 6.5l8 7.5 8-7.5"/></svg></a></li><li><a href=https://www.buymeacoffee.com/0x000216 target=_blank title=Coffee><svg width="884" height="1279" viewBox="0 0 884 1279" xmlns:xlink="http://www.w3.org/1999/xlink" xmlns="http://www.w3.org/2000/svg"><defs><path d="M0 0H884V1279H0V0z" id="path_1"/><clipPath id="mask_1"><use xlink:href="#path_1"/></clipPath></defs><g id="buymeacoffee"><path d="M0 0H884V1279H0V0z" id="Background" fill="none" fill-rule="evenodd" stroke="none"/><g clip-path="url(#mask_1)"><path d="M791.109 297.518 790.231 297.002 788.201 296.383C789.018 297.072 790.04 297.472 791.109 297.518z" id="Shape" fill="#0d0c22" stroke="none"/><path d="M803.916 388.891l-1 1 1-1z" id="Shape" fill="#0d0c22" stroke="none"/><path d="M792.113 297.647C791.776 297.581 791.447 297.462 791.127 297.29 791.108 297.622 791.108 297.958 791.127 298.29 791.488 298.216 791.83 297.995 792.113 297.647z" id="Shape" fill="#0d0c22" stroke="none"/><path d="M791.113 298.447h1v-1l-1 1z" id="Shape" fill="#0d0c22" stroke="none"/><path d="M803.111 388.726 804.591 387.883 805.142 387.573 805.641 387.04C804.702 387.444 803.846 388.016 803.111 388.726z" id="Shape" fill="#0d0c22" stroke="none"/><path d="M793.669 299.515 792.223 298.138 791.243 297.605C791.77 298.535 792.641 299.221 793.669 299.515z" id="Shape" fill="#0d0c22" stroke="none"/><path d="M430.019 1186.18C428.864 1186.68 427.852 1187.46 427.076 1188.45L427.988 1187.87C428.608 1187.3 429.485 1186.63 430.019 1186.18z" id="Shape" fill="#0d0c22" stroke="none"/><path d="M641.681 1144.63C641.681 1143.33 640.424 1143.57 640.729 1148.21 640.729 1147.84 641.035 1147.46 641.171 1147.1 641.341 1146.27 641.477 1145.46 641.681 1144.63z" id="Shape" fill="#0d0c22" stroke="none"/><path d="M619.284 1186.18C618.129 1186.68 617.118 1187.46 616.342 1188.45L617.254 1187.87C617.873 1187.3 618.751 1186.63 619.284 1186.18z" id="Shape" fill="#0d0c22" stroke="none"/><path d="M281.304 1196.06C280.427 1195.3 279.354 1194.8 278.207 1194.61 279.136 1195.06 280.065 1195.51 280.684 1195.85L281.304 1196.06z" id="Shape" fill="#0d0c22" stroke="none"/><path d="M247.841 1164.01C247.704 1162.66 247.288 1161.35 246.619 1160.16 247.093 1161.39 247.489 1162.66 247.806 1163.94L247.841 1164.01z" id="Shape" fill="#0d0c22" stroke="none"/><path d="M472.623 590.836c-45.941 19.667-98.077 41.966-165.647 41.966C278.71 632.746 250.58 628.868 223.353 621.274l46.733 479.806C271.74 1121.13 280.876 1139.83 295.679 1153.46 310.482 1167.09 329.87 1174.65 349.992 1174.65 349.992 1174.65 416.254 1178.09 438.365 1178.09 462.161 1178.09 533.516 1174.65 533.516 1174.65c20.12.0 39.503-7.57000000000016 54.303-21.2C602.619 1139.82 611.752 1121.13 613.406 1101.08l50.053-530.204C641.091 563.237 618.516 558.161 593.068 558.161 549.054 558.144 513.591 573.303 472.623 590.836z" id="Shape" fill="#fff" stroke="none"/><path d="M78.6885 386.132 79.4799 386.872 79.9962 387.182C79.5987 386.787 79.1603 386.435 78.6885 386.132z" id="Shape" fill="#0d0c22" stroke="none"/><path d="M879.567 341.849 872.53 306.352C866.215 274.503 851.882 244.409 819.19 232.898 808.711 229.215 796.821 227.633 788.786 220.01 780.751 212.388 778.376 200.55 776.518 189.572 773.076 169.423 769.842 149.257 766.314 129.143 763.269 111.85 760.86 92.4243 752.928 76.56c-10.324-21.3016-31.746-33.7591-53.048-42.001C688.965 30.4844 677.826 27.0375 666.517 24.2352 613.297 10.1947 557.342 5.03277 502.591 2.09047 436.875-1.53577 370.983-.443233 305.422 5.35968 256.625 9.79894 205.229 15.1674 158.858 32.0469c-16.948 6.1771-34.413 13.593-47.3 26.6872C95.7448 74.8221 90.5829 99.7026 102.128 119.765c8.208 14.247 22.111 24.313 36.857 30.972C158.192 159.317 178.251 165.846 198.829 170.215c57.297 12.664 116.642 17.636 175.178 19.753C438.887 192.586 503.87 190.464 568.44 183.618 584.408 181.863 600.347 179.758 616.257 177.304 634.995 174.43 647.022 149.928 641.499 132.859 634.891 112.453 617.134 104.538 597.055 107.618 594.095 108.082 591.153 108.512 588.193 108.942L586.06 109.252C579.257 110.113 572.455 110.915 565.653 111.661c-14.052 1.514-28.138 2.753-42.259 3.717C491.768 117.58 460.057 118.595 428.363 118.647c-31.144.0-62.305-.878-93.38-2.92500000000001C320.805 114.793 306.661 113.611 292.552 112.177 286.134 111.506 279.733 110.801 273.333 110.009L267.241 109.235 265.917 109.046 259.602 108.134C246.697 106.189 233.792 103.953 221.025 101.251 219.737 100.965 218.584 100.249 217.758 99.2193 216.932 98.1901 216.482 96.9099 216.482 95.5903 216.482 94.2706 216.932 92.9904 217.758 91.9612 218.584 90.9319 219.737 90.2152 221.025 89.9293H221.266C232.33 87.5721 243.479 85.5589 254.663 83.8038 258.392 83.2188 262.131 82.6453 265.882 82.0832H265.985C272.988 81.6186 280.026 80.3625 286.994 79.5366 347.624 73.2302 408.614 71.0801 469.538 73.1014 499.115 73.9618 528.676 75.6996 558.116 78.6935 564.448 79.3474 570.746 80.0357 577.043 80.8099 579.452 81.1025 581.878 81.4465 584.305 81.7391L589.191 82.4445C603.438 84.5667 617.61 87.1419 631.708 90.1703 652.597 94.7128 679.422 96.1925 688.713 119.077 691.673 126.338 693.015 134.408 694.649 142.03L696.731 151.752C696.786 151.926 696.826 152.105 696.852 152.285 701.773 175.227 706.7 198.169 711.632 221.111 711.994 222.806 712.002 224.557 711.657 226.255 711.312 227.954 710.621 229.562 709.626 230.982 708.632 232.401 707.355 233.6 705.877 234.504 704.398 235.408 702.75 235.997 701.033 236.236H700.895L697.884 236.649 694.908 237.044C685.478 238.272 676.038 239.419 666.586 240.486 647.968 242.608 629.322 244.443 610.648 245.992 573.539 249.077 536.356 251.102 499.098 252.066 480.114 252.57 461.135 252.806 442.162 252.771 366.643 252.712 291.189 248.322 216.173 239.625 208.051 238.662 199.93 237.629 191.808 236.58 198.106 237.389 187.231 235.96 185.029 235.651 179.867 234.928 174.705 234.177 169.543 233.397 152.216 230.798 134.993 227.598 117.7 224.793 96.7944 221.352 76.8005 223.073 57.8906 233.397c-15.5221 8.494-28.0851 21.519-36.013 37.338-8.1559 16.862-10.582 35.221-14.22974 53.34-3.64777 18.118-9.32591 37.613-7.17511 56.213C5.10128 420.431 33.165 453.054 73.5313 460.35 111.506 467.232 149.687 472.807 187.971 477.556 338.361 495.975 490.294 498.178 641.155 484.129 653.44 482.982 665.708 481.732 677.959 480.378 681.786 479.958 685.658 480.398 689.292 481.668S696.23 485.005 698.962 487.717 703.784 493.718 705.08 497.342C706.377 500.967 706.846 504.836 706.453 508.665L702.633 545.797c-7.697 75.031-15.394 150.057-23.091 225.077-8.029 78.783-16.111 157.56-24.244 236.326C653.004 1029.39 650.71 1051.57 648.416 1073.74 646.213 1095.58 645.904 1118.1 641.757 1139.68 635.218 1173.61 612.248 1194.45 578.73 1202.07 548.022 1209.06 516.652 1212.73 485.161 1213.01 450.249 1213.2 415.355 1211.65 380.443 1211.84 343.173 1212.05 297.525 1208.61 268.756 1180.87 243.479 1156.51 239.986 1118.36 236.545 1085.37 231.957 1041.7 227.409 998.039 222.9 954.381L197.607 711.615 181.244 554.538C180.968 551.94 180.693 549.376 180.435 546.76 178.473 528.023 165.207 509.681 144.301 510.627 126.407 511.418 106.069 526.629 108.168 546.76l12.13 116.454 25.087 240.89C152.532 972.528 159.661 1040.96 166.773 1109.41 168.15 1122.52 169.44 1135.67 170.885 1148.78 178.749 1220.43 233.465 1259.04 301.224 1269.91 340.799 1276.28 381.337 1277.59 421.497 1278.24 472.979 1279.07 524.977 1281.05 575.615 1271.72 650.653 1257.95 706.952 1207.85 714.987 1130.13 717.282 1107.69 719.576 1085.25 721.87 1062.8 729.498 988.559 737.115 914.313 744.72 840.061l24.881-242.61 11.408-111.188C781.577 480.749 783.905 475.565 787.649 471.478 791.392 467.391 796.352 464.617 801.794 463.567 823.25 459.386 843.761 452.245 859.023 435.916c24.295-25.998 29.13-59.895 20.544-94.067zM72.7365 365.835C73.247 365.68 72.3065 368.484 71.9034 369.792 71.8229 367.813 71.984 366.058 72.7365 365.835zm1.7756 16.105C74.6842 381.819 75.2003 382.508 75.7337 383.334 74.925 382.576 74.4089 382.009 74.4949 381.94H74.5121zM76.5597 384.641C77.6004 385.897 78.1569 386.689 76.5597 384.641zM80.7002 387.979h.2727C80.9729 388.313 81.473 388.645 81.6548 388.979 81.3533 388.612 81.0186 388.277 80.6548 387.979H80.7002zM800.796 382.989C793.088 390.319 781.473 393.726 769.996 395.43c-128.704 19.099-259.283 28.769-389.399 24.502C287.476 416.749 195.336 406.407 103.144 393.382 94.1102 392.109 84.3197 390.457 78.1082 383.798c-11.7004-12.561-5.9534-37.854-2.9079-53.03C77.9878 316.865 83.3218 298.334 99.8572 296.355 125.667 293.327 155.64 304.218 181.175 308.09 211.917 312.781 242.774 316.538 273.745 319.36 405.925 331.405 540.325 329.529 671.92 311.91 695.905 308.686 719.805 304.941 743.619 300.674 764.835 296.871 788.356 289.731 801.175 311.703 809.967 326.673 811.137 346.701 809.778 363.615 809.359 370.984 806.139 377.915 800.779 382.989H800.796z" id="Shape" fill="#fff" fill-rule="evenodd" stroke="none"/></g></g></svg></a></li></ol><ol class=menu id=main-menu><li><a href=/><svg xmlns="http://www.w3.org/2000/svg" class="icon icon-tabler icon-tabler-home" width="24" height="24" viewBox="0 0 24 24" stroke-width="2" stroke="currentcolor" fill="none" stroke-linecap="round" stroke-linejoin="round"><path stroke="none" d="M0 0h24v24H0z"/><polyline points="5 12 3 12 12 3 21 12 19 12"/><path d="M5 12v7a2 2 0 002 2h10a2 2 0 002-2v-7"/><path d="M9 21v-6a2 2 0 012-2h2a2 2 0 012 2v6"/></svg><span>Home</span></a></li><li><a href=/about-me/><svg xmlns="http://www.w3.org/2000/svg" class="icon icon-tabler icon-tabler-user" width="24" height="24" viewBox="0 0 24 24" stroke-width="2" stroke="currentcolor" fill="none" stroke-linecap="round" stroke-linejoin="round"><path stroke="none" d="M0 0h24v24H0z"/><circle cx="12" cy="7" r="4"/><path d="M6 21v-2a4 4 0 014-4h4a4 4 0 014 4v2"/></svg><span>About Me</span></a></li><div class=menu-bottom-section><li id=dark-mode-toggle><svg xmlns="http://www.w3.org/2000/svg" class="icon icon-tabler icon-tabler-toggle-left" width="24" height="24" viewBox="0 0 24 24" stroke-width="2" stroke="currentcolor" fill="none" stroke-linecap="round" stroke-linejoin="round"><path stroke="none" d="M0 0h24v24H0z"/><circle cx="8" cy="12" r="2"/><rect x="2" y="6" width="20" height="12" rx="6"/></svg><svg xmlns="http://www.w3.org/2000/svg" class="icon icon-tabler icon-tabler-toggle-right" width="24" height="24" viewBox="0 0 24 24" stroke-width="2" stroke="currentcolor" fill="none" stroke-linecap="round" stroke-linejoin="round"><path stroke="none" d="M0 0h24v24H0z"/><circle cx="16" cy="12" r="2"/><rect x="2" y="6" width="20" height="12" rx="6"/></svg><span>Dark Mode</span></li></div></ol></aside><main class="main full-width"><article class=main-article><header class=article-header><div class=article-details><div class=article-title-wrapper><h2 class=article-title><a href=/posts/2016-09-09-cooperative-threading/>Cooperative Threading</a></h2></div><footer class=article-time><div><svg xmlns="http://www.w3.org/2000/svg" class="icon icon-tabler icon-tabler-calendar-time" width="56" height="56" viewBox="0 0 24 24" stroke-width="2" stroke="currentcolor" fill="none" stroke-linecap="round" stroke-linejoin="round"><path stroke="none" d="M0 0h24v24H0z"/><path d="M11.795 21H5a2 2 0 01-2-2V7a2 2 0 012-2h12a2 2 0 012 2v4"/><circle cx="18" cy="18" r="4"/><path d="M15 3v4"/><path d="M7 3v4"/><path d="M3 11h16"/><path d="M18 16.496V18l1 1"/></svg><time class=article-time--published>Nov 13, 2019</time></div><div><svg xmlns="http://www.w3.org/2000/svg" class="icon icon-tabler icon-tabler-clock" width="24" height="24" viewBox="0 0 24 24" stroke-width="2" stroke="currentcolor" fill="none" stroke-linecap="round" stroke-linejoin="round"><path stroke="none" d="M0 0h24v24H0z"/><circle cx="12" cy="12" r="9"/><polyline points="12 7 12 12 15 15"/></svg><time class=article-time--reading>14 minute read</time></div></footer></div></header><section class=article-content><ol><li><a class=link href=https://byuu.net target=_blank rel=noopener>byuu.net</a></li><li><a class=link href=https://byuu.net/design target=_blank rel=noopener>design</a></li><li><a class=link href=https://byuu.net/design/cooperative-threading target=_blank rel=noopener>cooperative-threading</a></li></ol><h1 id=cooperative-threading---overview2019-10-05-updated-2019-10-17>Cooperative Threading - Overview2019-10-05 (updated 2019-10-17)</h1><p>Perhaps the largest challenge to writing an emulator is grappling with programming code being inherently serial, whereas the emulated systems often contain many, sometimes even dozens, of parallel processes all running at the same time.</p><p>In fact, most of the overhead of writing a retro video game system emulator is managing the synchronization between components in this environment.</p><p>In this article, I&rsquo;ll go into the approach I chose for my emulators: cooperative multi-threading. Please note that this is but one of several approaches, and I am not here to advocate for one or the other. I will do my best to present an unbiased look at its pros and cons.</p><h3 id=synchronization-overview>Synchronization Overview</h3><p>What we are trying to accomplish here is synchronization between emulated components.</p><p>Let&rsquo;s say we are emulating the base Super Nintendo, which has four major components: the general-purpose CPU, the audio APU, the video PPU, and the sound output DSP. Note that I&rsquo;m simplifying things a bit here (eg there are two PPU processors) for the sake of example.</p><p>Each of these chips run at clock rates of around ~20MHz.</p><p>Our emulator needs to run each chip in serial, or one at a time; whereas the real SNES would run all of these chips in parallel, or all at the same time.</p><p>After running one chip for a certain amount of time, we synchronize the other chips by running them for a period of time in order to catch up to the first chip.</p><p>The more accurate you want your emulation to be, the more fine-grained you want this synchronization to be.</p><p>You could execute one CPU opcode at a time, render one scanline at a time, and produce one audio sample at a time. And you would have a very fast SNES emulator which was about as accurate as those released in the late 1990s.</p><p>Or you could execute one CPU opcode cycle at a time, render one pixel at a time, and split audio sample generation into 32 distinct cycle stages. And you would likely have an emulator as demanding as bsnes.</p><p>The more fine-grained your emulation becomes, the more complex it gets to switch from one emulated component to another. Imagine you are emulating a CPU instruction, and after every cycle you need to switch to the video renderer to draw just one pixel and then return back to the CPU. That&rsquo;s a massive increase in technical complexity, which is what cooperative threading can help eliminate.</p><h3 id=preemptive-threading>Preemptive Threading</h3><p>First we must address the obvious: modern processors have many cores, and even in the rare event of single-core systems, modern operating systems provide threading facilities to allow multiple tasks to run in parallel in the form of threads. Can&rsquo;t we use these to write emulators?</p><p>The answer is, unfortunately, no.</p><p>Emulating the SNES accurately involves tens of millions of synchronizations, or context switches, per emulated second.</p><p>If your CPU only has one actual core, all of your threads will technically be running in serial anyway, and the OS will be performing context switches in kernel mode. Userland &lt;> kernel mode transitions are <em>extremely</em> painful for CPUs, and in the best case scenario, you might manage to do this a few hundred thousand times per second. Presuming you weren&rsquo;t emulating anything at all.</p><p>In the case of multi-core CPUs, things do get a bit better, but whether using a single core or multiple cores, you have to grapple with the fact that your PC is running at a different clock rate (let&rsquo;s say 3GHz) than the SNES (let&rsquo;s say 20MHz.) The way you&rsquo;d keep components in sync here would be to add locks, or mutexes, or semaphores, in order to make one thread wait for other threads after each slice of time has passed in each emulated thread.</p><p>These locks are immensely painful as well. Even if you bring them down to atomic instructions such as intelocked increment and interlocked decrement, you&rsquo;re still contending with modern CPUs being massively parallelized with huge pipelines and slow communication links between each CPU core.</p><p>Not to mention in a real, non-hypothetical scenario, it&rsquo;s trivial to end up with 16+ emulated threads (such as emulating a Sega CD 32X machine), and demanding that many cores to run an emulator would be a tough sell in 2019.</p><h3 id=state-machines>State Machines</h3><p>The traditional approach to retro emulator development is the use of state machines. As an example, here&rsquo;s a greatly simplified example of how one might split up a cycle-based CPU interpreter using state machines:</p><div class=highlight><div class=chroma><table class=lntable><tr><td class=lntd><pre tabindex=0 class=chroma><code><span class=lnt>1
</span></code></pre></td><td class=lntd><pre tabindex=0 class=chroma><code class=language-fallback data-lang=fallback><span class=line><span class=cl>void CPU::executeInstructionCycle() { cycle++; if(cycle == 1) { opcode = readMemory(PC++); return; } if(FlagM) switch(opcode) { //8-bit accumulator instructions case 0xb9: switch(cycle) { case 2: address = readMemory(PC++); return; case 3: address = readMemory(PC++) | address &lt;&lt; 8; return; case 4: //possible penalty cycle when crossing 8-bit page boundaries: if(address &gt;&gt; 8 != address + Y &gt;&gt; 8) { return; } cycle++; //cycle 4 not needed; fall through to cycle 5 case 5: A = readMemory(address + Y); cycle = 0; //end of instruction; start a new instruction next time return; } } }
</span></span></code></pre></td></tr></table></div></div><p>Splitting up a video renderer into individual pixel timeslices, or an audio generator into individual cycle timeslices, each has their own complexities as well.</p><p>Now let me add yet another horror: on the SNES, the above is not enough. Each CPU cycle consists of 6 - 12 master clock cycles, where the address bus is set to a given address, and after a certain amount of time, other chips will acknowledge the data on the bus. This is known as bus hold delays.</p><p>Basically, the CPU reading from the PPU cannot expect the PPU to respond with data instantaneously: it takes time. And if the CPU is writing to the PPU, the PPU needs some time as well.</p><p>So you would need to add a third-level state machine for every call to readMemory() above in order to split the reads into clock cycles:</p><div class=highlight><div class=chroma><table class=lntable><tr><td class=lntd><pre tabindex=0 class=chroma><code><span class=lnt>1
</span></code></pre></td><td class=lntd><pre tabindex=0 class=chroma><code class=language-fallback data-lang=fallback><span class=line><span class=cl>void CPU::executeInstructionCycle() { if(cycle == 1) { opcode = readMemory(PC++); cycle = 2; return; } if(FlagM) switch(opcode) { //8-bit accumulator instructions case 0xb9: switch(cycle) { case 2: switch(subcycle) { case 1: subcycle = 2; return; case 2: address = readMemory(PC++); subcycle = 1; return; } case 3: switch(subcycle) { case 1: subcycle = 2; return; case 2: address = readMemory(PC++) | address &lt;&lt; 8; subcycle = 1; return; } case 4: //possible penalty cycle when crossing 8-bit page boundaries: if(address &gt;&gt; 8 != address + Y &gt;&gt; 8) { return; } cycle++; //cycle 4 not needed; fall through to cycle 5 case 5: switch(subcycle) { case 1: subcycle = 2; return; case 2: A = readMemory(address + Y); subcycle = 1; cycle = 0; //end of instruction; start a new instruction next time return; } } } }
</span></span></code></pre></td></tr></table></div></div><p>And keep in mind, I&rsquo;m still greatly simplifying things here for the purpose of illustration.</p><h3 id=cooperative-threading>Cooperative Threading</h3><p>Enter cooperative threading: the idea is similar to preemptive threading, but instead of the kernel handling the threads, the userland process handles them instead.</p><p>Cooperative threads all run in serial: that is, only one runs at a time. When needed, a context switch will swap to another thread, which will resume exactly where it last left off.</p><p>Unlike coroutines, each cooperative thread has its own stack frame. This means that one thread can be several layers of function calls deep inside of a stack frame.</p><p>The state machine code you saw above is now <em>implicit</em>: it&rsquo;s part of the stack frame.</p><p>Thus, if we were to rewrite the code above, it would now look like this:</p><div class=highlight><div class=chroma><table class=lntable><tr><td class=lntd><pre tabindex=0 class=chroma><code><span class=lnt>1
</span></code></pre></td><td class=lntd><pre tabindex=0 class=chroma><code class=language-fallback data-lang=fallback><span class=line><span class=cl>void CPU::executeInstruction() { opcode = readMemory(PC++); if(FlagM) switch(opcode) { //8-bit accumulator instructions case 0xb9: address = readMemory(PC++); address = readMemory(PC++) | address &lt;&lt; 8; if(address &gt;&gt; 8 != address + Y &gt;&gt; 8) wait(6); A = readMemory(address + Y); } }
</span></span></code></pre></td></tr></table></div></div><p>Yes, it&rsquo;s really that dramatic of a difference.</p><p>How does it work? For that we look inside the readMemory() function:</p><div class=highlight><div class=chroma><table class=lntable><tr><td class=lntd><pre tabindex=0 class=chroma><code><span class=lnt>1
</span></code></pre></td><td class=lntd><pre tabindex=0 class=chroma><code class=language-fallback data-lang=fallback><span class=line><span class=cl>uint8\_t CPU::readMemory(uint16\_t address) { wait(2); uint8\_t data = memory\[address\]; wait(4); return data; }
</span></span></code></pre></td></tr></table></div></div><p>The trick is the call to wait():</p><div class=highlight><div class=chroma><table class=lntable><tr><td class=lntd><pre tabindex=0 class=chroma><code><span class=lnt>1
</span></code></pre></td><td class=lntd><pre tabindex=0 class=chroma><code class=language-fallback data-lang=fallback><span class=line><span class=cl>void CPU::wait(uint clockCycles) { apuCounter += clockCycles; while(apuCounter &gt; 0) yield(apu); }
</span></span></code></pre></td></tr></table></div></div><p>wait() just consumes clock cycles, and every time it does, if the CPU gets into a state where it&rsquo;s ahead of the APU, it yields execution to the APU, which then causes the APU to resume where it last left off. Once the APU has caught up and is now ahead of the CPU, it yields back to the CPU, and now the CPU resumes immediately after its last yield() point.</p><p>If you think of traditional programming functions as calls, and return statements as returning back to the call site, then you can think of cooperative threading as jumps. Instead of returning back, the APU jumps back to the CPU.</p><p>Technically, this is referred to as symmetric cooperative threads. There are also asymmetric cooperative threads that function like the typical call and return style, but they&rsquo;re not suitable in my opinion to emulation where many threads all run in parallel: the CPU may jump to the APU, and then the APU may jump to the PPU rather than back to the CPU.</p><h3 id=cooperative-threading-optimizations>Cooperative Threading Optimizations</h3><p>One thing you might have noticed: why are we always synchronizing the CPU to eg the APU after every clock cycle? What if the CPU was just reading from its own internal memory, which the APU has no access to?</p><p>When it comes to a state machine, it&rsquo;s an all-or-nothing approach: you have to split your emulated processors into the smallest timeslices possible and step by them.</p><p>Your main emulator scheduler could see that the CPU is not trying to access the APU on a given cycle, and run it more cycles before starting to run the APU a bit, but in practice you&rsquo;re already paying the overhead through the state machine anyway.</p><p>But cooperative threads present an opportunity for an optimization that I call just-in-time synchronization. Let&rsquo;s flesh out the earlier example more:</p><div class=highlight><div class=chroma><table class=lntable><tr><td class=lntd><pre tabindex=0 class=chroma><code><span class=lnt>1
</span></code></pre></td><td class=lntd><pre tabindex=0 class=chroma><code class=language-fallback data-lang=fallback><span class=line><span class=cl>uint8\_t CPU::readMemory(uint16\_t address) { wait(2); if(address &gt;= 0x2140 &amp;&amp; address &lt;= 0x2143) { //this read is going to access shared memory with the APU. //in other words, the APU might change this value before we read it. //as such, we \*must\* catch up the APU to the CPU here: while(apuCounter &gt; 0) yield(apu); } uint8\_t data = memory\[address\]; wait(4); return data; }
</span></span></code></pre></td></tr></table></div></div><p>void CPU::wait(uint clockCycles) { apuCounter += clockCycles; }</p><div class=highlight><div class=chroma><table class=lntable><tr><td class=lntd><pre tabindex=0 class=chroma><code><span class=lnt> 1
</span><span class=lnt> 2
</span><span class=lnt> 3
</span><span class=lnt> 4
</span><span class=lnt> 5
</span><span class=lnt> 6
</span><span class=lnt> 7
</span><span class=lnt> 8
</span><span class=lnt> 9
</span><span class=lnt>10
</span><span class=lnt>11
</span><span class=lnt>12
</span><span class=lnt>13
</span><span class=lnt>14
</span><span class=lnt>15
</span><span class=lnt>16
</span><span class=lnt>17
</span><span class=lnt>18
</span><span class=lnt>19
</span><span class=lnt>20
</span><span class=lnt>21
</span><span class=lnt>22
</span><span class=lnt>23
</span><span class=lnt>24
</span><span class=lnt>25
</span><span class=lnt>26
</span><span class=lnt>27
</span><span class=lnt>28
</span><span class=lnt>29
</span><span class=lnt>30
</span><span class=lnt>31
</span><span class=lnt>32
</span><span class=lnt>33
</span><span class=lnt>34
</span><span class=lnt>35
</span><span class=lnt>36
</span><span class=lnt>37
</span><span class=lnt>38
</span><span class=lnt>39
</span><span class=lnt>40
</span><span class=lnt>41
</span><span class=lnt>42
</span><span class=lnt>43
</span><span class=lnt>44
</span><span class=lnt>45
</span><span class=lnt>46
</span><span class=lnt>47
</span><span class=lnt>48
</span><span class=lnt>49
</span><span class=lnt>50
</span><span class=lnt>51
</span><span class=lnt>52
</span><span class=lnt>53
</span><span class=lnt>54
</span><span class=lnt>55
</span><span class=lnt>56
</span><span class=lnt>57
</span><span class=lnt>58
</span><span class=lnt>59
</span><span class=lnt>60
</span><span class=lnt>61
</span><span class=lnt>62
</span><span class=lnt>63
</span><span class=lnt>64
</span><span class=lnt>65
</span><span class=lnt>66
</span><span class=lnt>67
</span><span class=lnt>68
</span><span class=lnt>69
</span><span class=lnt>70
</span><span class=lnt>71
</span><span class=lnt>72
</span><span class=lnt>73
</span><span class=lnt>74
</span><span class=lnt>75
</span><span class=lnt>76
</span><span class=lnt>77
</span><span class=lnt>78
</span><span class=lnt>79
</span><span class=lnt>80
</span><span class=lnt>81
</span><span class=lnt>82
</span><span class=lnt>83
</span><span class=lnt>84
</span><span class=lnt>85
</span><span class=lnt>86
</span><span class=lnt>87
</span><span class=lnt>88
</span><span class=lnt>89
</span><span class=lnt>90
</span></code></pre></td><td class=lntd><pre tabindex=0 class=chroma><code class=language-fallback data-lang=fallback><span class=line><span class=cl>
</span></span><span class=line><span class=cl>What we&#39;ve done here is made it so the CPU will keep on running until it tries to read from a shared memory region with the APU. Only then will it catch up the APU to the CPU before performing the read.
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>If either the CPU doesn&#39;t need to synchronize to the APU often, _or vice versa_, then the number of context switches per second drops _dramatically_.
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>In the case of the SNES, my emulator [bsnes](https://bsnes.byuu.org) only has to synchronize the CPU and APU a few thousand times a second, instead of millions of times a second.
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>### Cooperative Threading Limitations
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>But what happens if you have two CPUs that share all of the memory address space with one another? Then unfortunately you wouldn&#39;t be able to know if the APU might modify the memory before the CPU (that&#39;s currently ahead) tries to read from it, and so you&#39;d always have to synchronize.
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>There are tricks to this, such as implementing a rollback mechanism as the Exodus emulator by Nemesis does, but you begin to lose the code simplification benefits of this approach in that case.
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>In the worst case, cooperative threading reverts to the same level of overhead as a state machine would.
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>### Pipeline Stalls
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>In fact, in the worst case, cooperative threading can be somewhat slower than the state machine approach.
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>Modern CPUs love to do many operations out-of-order, and build deep pipelines of queued instructions.
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>When cooperative threading changes the stack pointer, it tends to throw them into a panic.
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>As an analogy, say you have an assembly line. Along the line, the product is slowly assembled. Now say that in the middle of assembling a steady stream of products, an order comes in for a different products, and it cannot wait, but you only had one assembly line! You would have to stop the assembly line, take all the items off, and start on the new products. Now imagine you had to constantly shift back and forth between several products, tens of millions of times a second, and you start to understand why cycle-accurate retro gaming emulation is so demanding.
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>Now to be fair, state machines also suffer from this very same problem: it is still like grinding gears to a halt during operation to switch between one area of instructions (CPU emulation) and another (APU emulation): you&#39;re burning out your L1 instruction cache, etc.
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>But the need for cooperative threading to modify the stack pointer is a real penalty. In my admittedly non-scientific personal observations, if you can emulate a processor using a single-level state machine, it tends to run faster than a cooperative-threaded approach by a good margin. Whereas when you get into two (or even three or more) levels of state machines, such as the CPU opcode example abaove, the cooperative threading model comes out as the clear winner on performance. And if you are able to perform the just-in-time synchronization I previously discussed, there&#39;s no contest.
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>### Optimal Design
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>Thus, if your goal is maximum efficiency, an emulator should probably be designed to use cooperative threading where it works best, and state machines where it does not.
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>Me, I&#39;m really big on consistency, and so for my [higan emulator](https://higan.byuu.org), I chose to emulate every processor using cooperative threads. I knowingly pay a performance penalty to ensure consistency and code simplicity throughout the codebase. But of course, you don&#39;t _have_ to do that.
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>### Serialization
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>And now the elephant in the room: probably the biggest concern to emulator developers with cooperative threading is serialization: the act of saving and loading save states.
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>Save states are not only useful to save and restore your progress at any point (great for cheating a bit at those unforgiving games, too), they are also the building blocks for features like real-time rewind and run-ahead input latency reduction (each of which warrants its own separate article.)
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>When using state machines, serialization just means saving and restoring all of the variables used for it: opcode, cycle, and subcycle in the earlier example.
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>But when using cooperative threading, that information is, once again, implicit: it&#39;s stored on the individual stacks in the form of call frames and CPU context registers.
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>Implementing serialization with cooperative threading is possible, but it&#39;s a complicated topic, for which I have penned a separate article devoted to the topic. You can read that article here:
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>[Cooperative Threading - Synchronization](https://byuu.net/design/cooperative-synchronization)
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>### Code
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>So if you&#39;ve read this far and you&#39;re still considering giving cooperative threading a go, you&#39;ll need an implementation of it for your chosen language.
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>C++20 has proposed a coroutines extension, but since as of the time of writing this article, I don&#39;t have a working, mature implementation of it, I cannot say if it will be useful in emulation or not. Probably the biggest limitation for it is that C++20 coroutines are stackless, which vastly limits their potential for anything of even moderate complexity.
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>Alternatively, I&#39;ve developed my own cooperative threading library in C89 code, which can thusly be used in C++ code as well, which I call [libco](https://code.byuu.org/libco). You can browse its [source code here](https://github.com/byuu/bsnes/tree/master/libco).
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>Basically it requires dropping down to the assembler-level for each supported architecture to implement the context switching, as modifying the CPU context registers and stack frame directly is not permitted by most sane programming languages. But it&#39;s really not hard: an x86 context switch is only about twelve lines of assembler code. 32-bit ARM is a mere three.
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>libco is ISC licensed and supports a very large amount of CPU architectures, so you can skip having to do all of this work yourself if you want.
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>There are of course as many cooperative threading libraries as there are people talking about cooperative threading, so feel free to use another, or make your own.
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>For most languages there&#39;s probably an implementation of it somewhere. But perhaps not always.
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>### Closing
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>In any case, I really believe cooperative threading gives my emulators a unique advantage in having easier to read code by removing the burden of managing state machines, and it makes it trivial to be clock-cycle accurate as well.
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>Again, I&#39;m not saying this is the best way to write a retro emulator, but it&#39;s served me very well.
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>* * *
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>### Share
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>If you enjoyed the article, please help spread the word about it, if you don&#39;t mind. Thank you!
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>[![Twitter](https://byuu.net/images/social/twitter.svg)](https://twitter.com/home?status=https://byuu.net/design/cooperative-threading) [![Pinterest](https://byuu.net/images/social/pinterest.svg)](https://pinterest.com/pin/create/button/?url=https://byuu.net/design/cooperative-threading&amp;media=https://byuu.net/images/about/byuu.png&amp;description=Cooperative%20Threading%20-%20Overview) [![Facebook](https://byuu.net/images/social/facebook.svg)](https://facebook.com/sharer/sharer.php?u=https://byuu.net/design/cooperative-threading)
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>* * *
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>3 Comments
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>* * *
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>### Post Comment
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>  
</span></span><span class=line><span class=cl>  
</span></span><span class=line><span class=cl>from Hacker News https://ift.tt/2ALZL4A
</span></span></code></pre></td></tr></table></div></div></section><footer class=article-footer><section class=article-copyright><svg xmlns="http://www.w3.org/2000/svg" class="icon icon-tabler icon-tabler-copyright" width="24" height="24" viewBox="0 0 24 24" stroke-width="2" stroke="currentcolor" fill="none" stroke-linecap="round" stroke-linejoin="round"><path stroke="none" d="M0 0h24v24H0z"/><circle cx="12" cy="12" r="9"/><path d="M14.5 9a3.5 4 0 100 6"/></svg><span>Licensed under CC BY-NC-SA 4.0</span></section></footer></article><footer class=site-footer><section class=copyright>&copy;
2020 -
2022 ZYChimne</section><section class=powerby>Built with <a href=https://gohugo.io/ target=_blank rel=noopener>Hugo</a><br>Theme <b><a href=https://github.com/CaiJimmy/hugo-theme-stack target=_blank rel=noopener data-version=3.11.0>Stack</a></b> designed by <a href=https://jimmycai.com target=_blank rel=noopener>Jimmy</a></section></footer><div class=pswp tabindex=-1 role=dialog aria-hidden=true><div class=pswp__bg></div><div class=pswp__scroll-wrap><div class=pswp__container><div class=pswp__item></div><div class=pswp__item></div><div class=pswp__item></div></div><div class="pswp__ui pswp__ui--hidden"><div class=pswp__top-bar><div class=pswp__counter></div><button class="pswp__button pswp__button--close" title="Close (Esc)"></button>
<button class="pswp__button pswp__button--share" title=Share></button>
<button class="pswp__button pswp__button--fs" title="Toggle fullscreen"></button>
<button class="pswp__button pswp__button--zoom" title="Zoom in/out"></button><div class=pswp__preloader><div class=pswp__preloader__icn><div class=pswp__preloader__cut><div class=pswp__preloader__donut></div></div></div></div></div><div class="pswp__share-modal pswp__share-modal--hidden pswp__single-tap"><div class=pswp__share-tooltip></div></div><button class="pswp__button pswp__button--arrow--left" title="Previous (arrow left)"></button>
<button class="pswp__button pswp__button--arrow--right" title="Next (arrow right)"></button><div class=pswp__caption><div class=pswp__caption__center></div></div></div></div></div><script src=https://cdn.jsdelivr.net/npm/photoswipe@4.1.3/dist/photoswipe.min.js integrity="sha256-ePwmChbbvXbsO02lbM3HoHbSHTHFAeChekF1xKJdleo=" crossorigin=anonymous defer></script><script src=https://cdn.jsdelivr.net/npm/photoswipe@4.1.3/dist/photoswipe-ui-default.min.js integrity="sha256-UKkzOn/w1mBxRmLLGrSeyB4e1xbrp4xylgAWb3M42pU=" crossorigin=anonymous defer></script><link rel=stylesheet href=https://cdn.jsdelivr.net/npm/photoswipe@4.1.3/dist/default-skin/default-skin.css integrity="sha256-c0uckgykQ9v5k+IqViZOZKc47Jn7KQil4/MP3ySA3F8=" crossorigin=anonymous><link rel=stylesheet href=https://cdn.jsdelivr.net/npm/photoswipe@4.1.3/dist/photoswipe.css integrity="sha256-SBLU4vv6CA6lHsZ1XyTdhyjJxCjPif/TRkjnsyGAGnE=" crossorigin=anonymous></main><aside class="sidebar right-sidebar sticky"><section class="widget archives"><div class=widget-icon><svg xmlns="http://www.w3.org/2000/svg" class="icon icon-tabler icon-tabler-hash" width="24" height="24" viewBox="0 0 24 24" stroke-width="2" stroke="currentcolor" fill="none" stroke-linecap="round" stroke-linejoin="round"><path stroke="none" d="M0 0h24v24H0z"/><line x1="5" y1="9" x2="19" y2="9"/><line x1="5" y1="15" x2="19" y2="15"/><line x1="11" y1="4" x2="7" y2="20"/><line x1="17" y1="4" x2="13" y2="20"/></svg></div><h2 class="widget-title section-title">Table of contents</h2><div class=widget--toc><nav id=TableOfContents><ol><li><ol><li><a href=#synchronization-overview>Synchronization Overview</a></li><li><a href=#preemptive-threading>Preemptive Threading</a></li><li><a href=#state-machines>State Machines</a></li><li><a href=#cooperative-threading>Cooperative Threading</a></li><li><a href=#cooperative-threading-optimizations>Cooperative Threading Optimizations</a></li></ol></li></ol></nav></div></section></aside></div><script src=https://cdn.jsdelivr.net/npm/node-vibrant@3.1.5/dist/vibrant.min.js integrity="sha256-5NovOZc4iwiAWTYIFiIM7DxKUXKWvpVEuMEPLzcm5/g=" crossorigin=anonymous></script><script type=text/javascript src=/ts/main.js defer></script>
<script>(function(){const e=document.createElement("link");e.href="https://fonts.googleapis.com/css2?family=Lato:wght@300;400;700&display=swap",e.type="text/css",e.rel="stylesheet",document.head.appendChild(e)})()</script></body></html>