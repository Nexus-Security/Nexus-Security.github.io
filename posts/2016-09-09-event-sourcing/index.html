<!doctype html><html lang=en-us dir=ltr><head><meta charset=utf-8><meta name=viewport content="width=device-width,initial-scale=1"><meta name=description content="Event Sourcing May the source be with you. I recently became intrigued by the concept of event sourcing as applied to back-end architecture, specifically a microservice-oriented approach. I have spent the last few years working predominantly on the front-end, and became enamored by the simplicity and elegance of this pattern as the backbone of front-end architecture, popularized by Redux.
To better understand the trade-offs between a traditional, monolithic back-end, and a microservice-oriented, event sourced approach, I began sketching a toy architecture for the initial user flow of seemingly every web application: signing up for a user account, and receiving an activation email."><title>Event Sourcing</title><link rel=canonical href=https://Nexus-Security.github.io/posts/2016-09-09-event-sourcing/><link rel=stylesheet href=/scss/style.min.450926226e724574a6b936335ea06111f8aeb253d932c86cb2cc807341cd2889.css><meta property="og:title" content="Event Sourcing"><meta property="og:description" content="Event Sourcing May the source be with you. I recently became intrigued by the concept of event sourcing as applied to back-end architecture, specifically a microservice-oriented approach. I have spent the last few years working predominantly on the front-end, and became enamored by the simplicity and elegance of this pattern as the backbone of front-end architecture, popularized by Redux.
To better understand the trade-offs between a traditional, monolithic back-end, and a microservice-oriented, event sourced approach, I began sketching a toy architecture for the initial user flow of seemingly every web application: signing up for a user account, and receiving an activation email."><meta property="og:url" content="https://Nexus-Security.github.io/posts/2016-09-09-event-sourcing/"><meta property="og:site_name" content="ZYChimne"><meta property="og:type" content="article"><meta property="article:section" content="Posts"><meta property="article:published_time" content="2020-01-02T01:36:00+01:00"><meta property="article:modified_time" content="2020-01-02T01:36:00+01:00"><meta name=twitter:title content="Event Sourcing"><meta name=twitter:description content="Event Sourcing May the source be with you. I recently became intrigued by the concept of event sourcing as applied to back-end architecture, specifically a microservice-oriented approach. I have spent the last few years working predominantly on the front-end, and became enamored by the simplicity and elegance of this pattern as the backbone of front-end architecture, popularized by Redux.
To better understand the trade-offs between a traditional, monolithic back-end, and a microservice-oriented, event sourced approach, I began sketching a toy architecture for the initial user flow of seemingly every web application: signing up for a user account, and receiving an activation email."></head><body class=article-page><script>(function(){const e="StackColorScheme";localStorage.getItem(e)||localStorage.setItem(e,"auto")})()</script><script>(function(){const t="StackColorScheme",e=localStorage.getItem(t),n=window.matchMedia("(prefers-color-scheme: dark)").matches===!0;e=="dark"||e==="auto"&&n?document.documentElement.dataset.scheme="dark":document.documentElement.dataset.scheme="light"})()</script><div class="container main-container flex on-phone--column extended"><aside class="sidebar left-sidebar sticky"><button class="hamburger hamburger--spin" type=button id=toggle-menu aria-label="Toggle Menu">
<span class=hamburger-box><span class=hamburger-inner></span></span></button><header><figure class=site-avatar><a href=/><img src=/img/avatar_hu8b78332b6420dc9affabe23720d11e63_1937019_300x0_resize_q75_box.jpg width=300 height=300 class=site-logo loading=lazy alt=Avatar></a>
<span class=emoji>üçá</span></figure><div class=site-meta><h1 class=site-name><a href=/>ZYChimne</a></h1><h2 class=site-description>Computer Science, Wuhan University</h2></div></header><ol class=social-menu><li><a href=https://github.com/ZYChimne target=_blank title=GitHub><svg xmlns="http://www.w3.org/2000/svg" class="icon icon-tabler icon-tabler-brand-github" width="24" height="24" viewBox="0 0 24 24" stroke-width="2" stroke="currentcolor" fill="none" stroke-linecap="round" stroke-linejoin="round"><path stroke="none" d="M0 0h24v24H0z" fill="none"/><path d="M9 19c-4.3 1.4-4.3-2.5-6-3m12 5v-3.5c0-1 .1-1.4-.5-2 2.8-.3 5.5-1.4 5.5-6a4.6 4.6.0 00-1.3-3.2 4.2 4.2.0 00-.1-3.2s-1.1-.3-3.5 1.3a12.3 12.3.0 00-6.2.0C6.5 2.8 5.4 3.1 5.4 3.1a4.2 4.2.0 00-.1 3.2A4.6 4.6.0 004 9.5c0 4.6 2.7 5.7 5.5 6-.6.6-.6 1.2-.5 2V21"/></svg></a></li><li><a href=https://twitter.com/ZChimne target=_blank title=Twitter><svg xmlns="http://www.w3.org/2000/svg" class="icon icon-tabler icon-tabler-brand-twitter" width="24" height="24" viewBox="0 0 24 24" stroke-width="2" stroke="currentcolor" fill="none" stroke-linecap="round" stroke-linejoin="round"><path stroke="none" d="M0 0h24v24H0z" fill="none"/><path d="M22 4.01c-1 .49-1.98.689-3 .99-1.121-1.265-2.783-1.335-4.38-.737S11.977 6.323 12 8v1c-3.245.083-6.135-1.395-8-4 0 0-4.182 7.433 4 11-1.872 1.247-3.739 2.088-6 2 3.308 1.803 6.913 2.423 10.034 1.517 3.58-1.04 6.522-3.723 7.651-7.742a13.84 13.84.0 00.497-3.753C20.18 7.773 21.692 5.25 22 4.009z"/></svg></a></li></ol><ol class=menu id=main-menu><li><a href=/><svg xmlns="http://www.w3.org/2000/svg" class="icon icon-tabler icon-tabler-home" width="24" height="24" viewBox="0 0 24 24" stroke-width="2" stroke="currentcolor" fill="none" stroke-linecap="round" stroke-linejoin="round"><path stroke="none" d="M0 0h24v24H0z"/><polyline points="5 12 3 12 12 3 21 12 19 12"/><path d="M5 12v7a2 2 0 002 2h10a2 2 0 002-2v-7"/><path d="M9 21v-6a2 2 0 012-2h2a2 2 0 012 2v6"/></svg><span>Home</span></a></li><li><a href=/about-me/><svg xmlns="http://www.w3.org/2000/svg" class="icon icon-tabler icon-tabler-user" width="24" height="24" viewBox="0 0 24 24" stroke-width="2" stroke="currentcolor" fill="none" stroke-linecap="round" stroke-linejoin="round"><path stroke="none" d="M0 0h24v24H0z"/><circle cx="12" cy="7" r="4"/><path d="M6 21v-2a4 4 0 014-4h4a4 4 0 014 4v2"/></svg><span>About Me</span></a></li><li><a href=/archives/><svg xmlns="http://www.w3.org/2000/svg" class="icon icon-tabler icon-tabler-archive" width="24" height="24" viewBox="0 0 24 24" stroke-width="2" stroke="currentcolor" fill="none" stroke-linecap="round" stroke-linejoin="round"><path stroke="none" d="M0 0h24v24H0z"/><rect x="3" y="4" width="18" height="4" rx="2"/><path d="M5 8v10a2 2 0 002 2h10a2 2 0 002-2V8"/><line x1="10" y1="12" x2="14" y2="12"/></svg><span>Archives</span></a></li><div class=menu-bottom-section><li id=dark-mode-toggle><svg xmlns="http://www.w3.org/2000/svg" class="icon icon-tabler icon-tabler-toggle-left" width="24" height="24" viewBox="0 0 24 24" stroke-width="2" stroke="currentcolor" fill="none" stroke-linecap="round" stroke-linejoin="round"><path stroke="none" d="M0 0h24v24H0z"/><circle cx="8" cy="12" r="2"/><rect x="2" y="6" width="20" height="12" rx="6"/></svg><svg xmlns="http://www.w3.org/2000/svg" class="icon icon-tabler icon-tabler-toggle-right" width="24" height="24" viewBox="0 0 24 24" stroke-width="2" stroke="currentcolor" fill="none" stroke-linecap="round" stroke-linejoin="round"><path stroke="none" d="M0 0h24v24H0z"/><circle cx="16" cy="12" r="2"/><rect x="2" y="6" width="20" height="12" rx="6"/></svg><span>Dark Mode</span></li></div></ol></aside><main class="main full-width"><article class=main-article><header class=article-header><div class=article-details><div class=article-title-wrapper><h2 class=article-title><a href=/posts/2016-09-09-event-sourcing/>Event Sourcing</a></h2></div><footer class=article-time><div><svg xmlns="http://www.w3.org/2000/svg" class="icon icon-tabler icon-tabler-calendar-time" width="56" height="56" viewBox="0 0 24 24" stroke-width="2" stroke="currentcolor" fill="none" stroke-linecap="round" stroke-linejoin="round"><path stroke="none" d="M0 0h24v24H0z"/><path d="M11.795 21H5a2 2 0 01-2-2V7a2 2 0 012-2h12a2 2 0 012 2v4"/><circle cx="18" cy="18" r="4"/><path d="M15 3v4"/><path d="M7 3v4"/><path d="M3 11h16"/><path d="M18 16.496V18l1 1"/></svg><time class=article-time--published>Jan 02, 2020</time></div><div><svg xmlns="http://www.w3.org/2000/svg" class="icon icon-tabler icon-tabler-clock" width="24" height="24" viewBox="0 0 24 24" stroke-width="2" stroke="currentcolor" fill="none" stroke-linecap="round" stroke-linejoin="round"><path stroke="none" d="M0 0h24v24H0z"/><circle cx="12" cy="12" r="9"/><polyline points="12 7 12 12 15 15"/></svg><time class=article-time--reading>33 minute read</time></div></footer></div></header><section class=article-content><p><img src=https://arkwright.github.io/images/event-sourcing/bugs.svg loading=lazy alt="A marching line of various types of insects, including ants, flies, ladybugs, etc., intended as an analogy to the stream of diverse events in an event sourced system. The irony of using a stream of bugs is not lost on me."></p><h1 id=event-sourcing>Event Sourcing</h1><h2 id=may-the-source-be-with-you>May the source be with you.</h2><p>I recently became intrigued by the concept of <em><a class=link href=https://martinfowler.com/eaaDev/EventSourcing.html target=_blank rel=noopener>event sourcing</a></em> as applied to back-end architecture, specifically a microservice-oriented approach. I have spent the last few years working predominantly on the front-end, and became enamored by the simplicity and elegance of this pattern as the backbone of front-end architecture, popularized by <a class=link href=http://redux.js.org/ target=_blank rel=noopener>Redux</a>.</p><p>To better understand the trade-offs between a traditional, monolithic back-end, and a microservice-oriented, event sourced approach, I began sketching a toy architecture for the initial user flow of seemingly every web application: signing up for a user account, and receiving an activation email. Easy enough, right?</p><p><img src=https://arkwright.github.io/images/event-sourcing/alien.svg loading=lazy alt="Alien from outer space, saying &lsquo;Sup.&rsquo;, as aliens do."></p><p>Little did I realize just how alien the event sourcing pattern would feel. I quickly developed more questions than answers. I spent the next several days reading everything I could about the subject, desperately begging Google to show me the way. I learned an incredible amount during that time, and in the spirit of the great <a class=link href=https://jvns.ca/ target=_blank rel=noopener>Julia Evans</a> I felt compelled to distill and summarize what I have learned for you, my fellow traveler.</p><p>This is by no means a guide indicating the ‚Äúcorrect‚Äù way to do anything. My hope is that, if you‚Äôre new to event sourcing, this summary might help you to start reasoning about how such a system could work.</p><h2 id=things-we-will-talk-about>Things We Will Talk About</h2><p><img src=https://arkwright.github.io/images/event-sourcing/devil.svg loading=lazy alt="Lousy stick figure drawing of the devil holding a pitchfork. &lsquo;What the hell is event sourcing?&rsquo; Get it? Get it?!"></p><h2 id=what-the-hell-is-event-sourcing>What the hell is event sourcing?</h2><p>Good question! Martin Fowler <a class=link href=https://martinfowler.com/eaaDev/EventSourcing.html target=_blank rel=noopener>can tell you</a>:</p><blockquote><p>Event Sourcing ensures that all changes to application state are stored as a sequence of events. Not just can we query these events, we can also use the event log to reconstruct past states, and as a foundation to automatically adjust the state to cope with retroactive changes.</p></blockquote><p>But ‚Äî¬†and this is true of most definitions of most things ‚Äî this will just leave you with even more questions. So I‚Äôll try to explain event sourcing instead of defining it.</p><ul><li>Your application produces a <em>log of events</em>. For example, you might log a <code>UserAccountCreated</code> event for each user account that is created. The log might be split into smaller, independent logs called <em>topics</em>, to help organize your events.</li><li>The events are the <em>source of truth</em> or <a class=link href=https://en.wikipedia.org/wiki/System_of_record target=_blank rel=noopener>system of record</a> for your application. It is common for applications to write to a database and treat it as the source of truth, but when event sourcing we write to the event log instead.</li><li>Other parts of the application can read from the event log. This allows for a pub/sub style of communication, where multiple listeners can react to events they are interested in.</li><li>Listeners can reconstruct their own application state by reading from the event log and applying the events to their own, private data store, such as a database. They might apply some of the events, or all of them, depending on their use case. Events are always applied in the total order that they appear in the log.</li></ul><p><img src=https://arkwright.github.io/images/event-sourcing/overview.svg loading=lazy alt="Diagram of application synchronously writing (appending) an event to the event log. A service named Some Service asynchronously reads a prior event from the log, processes it, and writes to its own, private database."></p><h2 id=what-can-event-sourcing-do-for-me>What can event sourcing do for me?</h2><p>I can recommend two really good sales pitches for event sourced architectures (sometimes called <em>log-oriented</em> architectures), and a more pragmatic overview. I recommend that you read and watch these in the following order:</p><ol><li>Martin Kleppmann has an <a class=link href=https://www.confluent.io/blog/using-logs-to-build-a-solid-data-infrastructure-or-why-dual-writes-are-a-bad-idea/ target=_blank rel=noopener>excellent write-up</a> to whet your appetite.</li><li>Greg Young gave <a class=link href="https://www.youtube.com/watch?v=8JKjvY4etTY" target=_blank rel=noopener>a great talk</a> which really helped me to understand how event sourcing can be useful <em>even at small traffic scales.</em></li><li>And as always, Martin Fowler will try to talk some sense into us as a part of <a class=link href="https://www.youtube.com/watch?v=aweV9FLTZkU" target=_blank rel=noopener>his fantastic overview</a>.</li></ol><p><img src=https://arkwright.github.io/images/event-sourcing/fowler-kleppmann-young.svg loading=lazy alt="Poorly drawn stick figure versions of Martin Fowler (beard), Martin Kleppmann (long hair), and Greg Young (beard and short hair). It is surprisingly hard to capture someone&rsquo;s essence using only hair."></p><p>But it‚Äôs not fair for me to dump two hours of educational materials into your lap, so I‚Äôll do my best to summarize the observations of the great masters.</p><h3 id=historical-queries>Historical Queries</h3><p>A typical database can answer questions about your data as it exists right now, but it struggles to answer time-series queries about the historical context and evolution of your data.</p><p>For example, you can query your database to determine the number of user accounts that exist. But what if your business stakeholder wanted to know how many users create an account, delete it, and then change their mind and create it again? Your database typically will not capture this data, since it only stores the <em>current</em> state ‚Äî it only stores the user account, and not the <em>steps</em> that were taken to create that account. Writing events to a <a class=link href="https://www.youtube.com/watch?v=-fQGPZTECYs" target=_blank rel=noopener>log</a> naturally makes these kinds of queries possible, because the historical data is never deleted. The ability to be able to answer <em>any</em> question that the business asks about the history of the application is incredibly valuable.</p><p><img src=https://arkwright.github.io/images/event-sourcing/history-state.svg loading=lazy alt="Diagram of event log representing time series data. The values comprising the history are: 0, +3, -5, +9, +1. When added, they compute the value 8, which is the application&rsquo;s state, depicted as being stored in a database."></p><p>Historical queries can ask, ‚ÄúHow did we arrive at this state?‚Äù, instead of, ‚ÄúWhat does the current state look like?‚Äù</p><h3 id=immutable-data>Immutable Data</h3><p>Modeling your data as an immutable, append-only log of events greatly simplifies reasoning about how the application works. It is harder to get yourself into a confusing situation by accidentally mutating state. This is easier to understand when we consider the utility of time-traveling debugging.</p><p><img src=https://arkwright.github.io/images/event-sourcing/append.svg loading=lazy alt="Diagram of an event being appended to the end of an event log, with sweet explosion effect to emphasize that we always append, never mutate."></p><h3 id=time-traveling-debugging>Time-Traveling Debugging</h3><p>Dan Abramov (creator of Redux) has <a class=link href="https://youtu.be/xsSnOQynTHs?t=18m2s" target=_blank rel=noopener>sung the praises</a> of time-traveling debugging from a front-end perspective, and the same principle applies from a back-end one.</p><p>Given that the event log is immutable, all changes to the application‚Äôs state must be driven by <em>appending</em> to the event log instead of changing it. This means that when our application behaves in a confusing way, we can simply start from the ‚Äúbeginning of time‚Äù and replay events one by one until we isolate the event that is triggering the confusing behavior. This is a powerful and incredibly simple tool for debugging our application.</p><p><img src=https://arkwright.github.io/images/event-sourcing/time-travel.svg loading=lazy alt="Diagram of event log containing 8 events. A second log pulls 5 events from the first log, in original order, to reconstruct a previous application state."></p><p><em>But that‚Äôs not all!</em> Just as our version control system can ‚Äúcheck out‚Äù code at a particular point in the project‚Äôs history, our event log can ‚Äúcheck out‚Äù a particular point in time so that we can inspect how the state looked at that moment.</p><p>As Martin Fowler <a class=link href=https://martinfowler.com/eaaDev/EventSourcing.html target=_blank rel=noopener>pointed out</a>, instead of exclusively writing end-to-end tests we can explore a complementary approach: store and replay a sequence of events into the log, and then inspect the application‚Äôs state to ensure that it matches what we expect.</p><p><img src=https://arkwright.github.io/images/event-sourcing/testing.svg loading=lazy alt="Diagram of an event log with Mr. Potato Head pieces representing the content of events. The production environment correctly assembles the events into a very poorly drawn Mr. Potato Head, and the QA environment assembles them into a horrifying mutation. Stick figure Martin Fowler comments that &lsquo;It&rsquo;s broke.&rsquo;"></p><p>These are just some examples. Retaining the time-series data in our event log opens up numerous opportunities for building <a class=link href=http://firstround.com/review/forget-technical-debt-heres-how-to-build-technical-wealth/ target=_blank rel=noopener>technical wealth</a>.</p><p><img src=https://arkwright.github.io/images/event-sourcing/crown.svg loading=lazy alt="Drawing of a crown with jewels. I put some extra lines around it to indicate fanciness. This is a metaphor for technical wealth. The exact type of crown is left to the reader&rsquo;s imagination."></p><h3 id=easily-connect-data-consumers>Easily Connect Data Consumers</h3><p>An event sourced architecture features an event log as the central hub to which data producers write, and from which data consumers read. This pub/sub architecture minimizes or eliminates the need to write custom adaptors to get data out of one system and into another. All data is published in a standardized message format (JSON, or whatever you enjoy). Writing a new consumer becomes easier and more predictable, since systems share data in a consistent way. Multiple listeners can subscribe to an event log without a problem.</p><p><img src=https://arkwright.github.io/images/event-sourcing/central-log.svg loading=lazy alt="Diagram of an event log at the center of many services, which connect to it to consume each other&rsquo;s data. A sharpy dressed smiley face (with bow tie) connects to one of the services, as users do."></p><p>Systems often mutate into Frankenstein architectures as new features and use cases are bolted on accommodated. Martin Kleppmann does a great job of <a class=link href=https://www.confluent.io/blog/using-logs-to-build-a-solid-data-infrastructure-or-why-dual-writes-are-a-bad-idea/ target=_blank rel=noopener>describing this phenomenon</a>. Modeling data consumption as a log of events can mitigate this unsatisfactory result.</p><h3 id=reasonable-scaling-defaults>Reasonable Scaling Defaults</h3><p>An event sourced architecture provides reasonable defaults for common scalability challenges that applications face as load increases, and after exhausting vertical scaling strategies. It isn‚Äôt a silver bullet (nothing is), but we can take comfort in the fact that we are probably not painting ourselves into a corner.</p><p>If writing to the event log is the bottleneck, we can split a single log into <em>partitions</em> spread over multiple servers, each responsible for handling writes to its fair share of the partitions. This is how Apache Kafka works.</p><p><img src=https://arkwright.github.io/images/event-sourcing/partitions.svg loading=lazy alt="Diagram of two services, Service and Moar Service, each synchronously writing to two different event log partitions. Service produces events 1 through 5, and Moar Service produces events A through E. Both partitions are asynchronously read by two different consumers."></p><p>If reading from the event log is the bottleneck, we can introduce log replication and have consumers read from the replicas.</p><p><img src=https://arkwright.github.io/images/event-sourcing/replicas.svg loading=lazy alt="Diagram of a service synchronously writing events 1 through 5 to an event log. The log replicates those events to two replicas, from which two consumers are able to asynchronously read."></p><p>If consumers cannot keep up with the volume of events, we can add more consumers and parallelize the work of processing the events.</p><p><img src=https://arkwright.github.io/images/event-sourcing/parallel-consumers.svg loading=lazy alt="Diagram of a service synchronously writing events 1 through 9 to an event log. The log is asynchronously split into three partitions, with events being spread over the partitions. The partitions receive events {1, 4, 7}, {2, 5, 8}, and {3, 6, 9}, respectively. Three consumers are each dedicated to consuming events from one of the partitions."></p><p>If we run out of disk space to store the log, we can explore options for long-term storage. We could write a service to read older log messages and push them into a some kind of data warehouse. Consumers which only need to keep up with processing new-ish messages read directly from the primary log. Consumers which wish to rebuild their local state by processing all log messages from the beginning of time may do so by reading from the data warehouse until they reach the most recent warehoused message, and then switch to reading from the primary log.</p><p><img src=https://arkwright.github.io/images/event-sourcing/storage.svg loading=lazy alt="Diagram of a service synchronously writing event 12 to the event log. A Long Term Storage service asynchronously pulls event 7 from the log and synchronously appends it to its own log, which now contains events 1 through 7. A cool consumer wearing (probably expensive) sunglasses asynchronously reads events 1 through 7 from the Long Term Storage log, and events 8 through latest from the main log."></p><h3 id=fault-tolerance-and-resiliency>Fault Tolerance and Resiliency</h3><p>This is my favourite feature of a log-oriented architecture, and the one that attracted me to event sourcing.</p><p>Often, one portion of an application will need to react to a change in a different subsystem. For example, when a user account is created, we might want to send an account activation email to that user.</p><p>In a traditional monolithic system, the controller which handles this logic might look something like this pseudocode:</p><div class=highlight><div class=chroma><table class=lntable><tr><td class=lntd><pre tabindex=0 class=chroma><code><span class=lnt>1
</span></code></pre></td><td class=lntd><pre tabindex=0 class=chroma><code class=language-fallback data-lang=fallback><span class=line><span class=cl>user = new User(&#39;dvader@empire.gov&#39;) user.save() MailService.sendAccountActivationEmail(user) 
</span></span></code></pre></td></tr></table></div></div><p>The above logic will work 99% of the time. But every now and then the <code>MailService</code> will go offline. The new account will be created but the user will not receive their activation email. The user cannot activate their account!</p><p><img src=https://arkwright.github.io/images/event-sourcing/dual-writes.svg loading=lazy alt="Diagram of a Controller performing a dual write, first to a database, and then to a Mail Service. A cartoon-style bomb, with the silly sparkling fuse hanging out, demonstrates how the second write could fail at any time."></p><p>This is a tricky situation to recover from, and an example of the problem of <a class=link href=https://www.confluent.io/blog/using-logs-to-build-a-solid-data-infrastructure-or-why-dual-writes-are-a-bad-idea/ target=_blank rel=noopener>dual writes</a>. It would be much better if we could build an application which simply <em>pauses</em> when a subsystem goes down, and resumes from where it left off when that subsystem comes back online. This would provide tremendous peace of mind, save countless users from headache, and prevent us from wasting many days recovering from, debugging, and prematurely optimizing the availability problems of our <code>MailService</code>.</p><p><img src=https://arkwright.github.io/images/event-sourcing/hard-drive.svg loading=lazy alt="Completely ridiculous drawing of a muscled hard drive, with the glorious caption, &lsquo;World&rsquo;s Most Reliable!!!!!!1&rsquo; The hard drive is Super Duper brand, if that makes any difference."></p><p>Remember: we can often substitute rapid recovery for high availability. Instead of investing significant sums to achieve high availability, we can <a class=link href=https://en.wikipedia.org/wiki/Pareto_principle target=_blank rel=noopener>Pareto-optimize</a> by investing a smaller amount into rapid recovery. For example, instead of buying the world‚Äôs most reliable hard drive, we could simply make frequent backups. Our system can go down frequently, but the user will never notice as long as we can recover in a reasonable amount of time. As Gary Bernhardt <a class=link href=https://www.destroyallsoftware.com/compendium/network-protocols/97d3ba4c24d21147 target=_blank rel=noopener>astutely points out</a>, TCP is so good at this that we take it for granted!</p><blockquote><p>TCP is so successful at its job [packet retransmission, rapid recovery] that we don‚Äôt even think of networks as being unreliable in our daily use, even though they fail routinely under normal conditions.</p></blockquote><p>This is a great example of the unreasonable effectiveness of <a class=link href=https://en.wikipedia.org/wiki/Defense_in_depth_%28computing%29 target=_blank rel=noopener>defense in depth</a> strategies. The first layer of defense is designing for availability, and the second layer is designing for recovery.</p><p>A log-oriented architecture can give us these benefits! Let‚Äôs rewrite our pseudocode controller:</p><div class=highlight><div class=chroma><table class=lntable><tr><td class=lntd><pre tabindex=0 class=chroma><code><span class=lnt>1
</span></code></pre></td><td class=lntd><pre tabindex=0 class=chroma><code class=language-fallback data-lang=fallback><span class=line><span class=cl>user = new User(&#39;dvader@empire.gov&#39;) event = new AccountCreatedEvent(user) EventLog.append(event) 
</span></span></code></pre></td></tr></table></div></div><p>Notice how we are no longer performing dual writes. Instead, we perform a single append to the event log. This is equivalent to saving the user account in the first example. If we model our writes as single log appends, they become inherently atomic.</p><p><img src=https://arkwright.github.io/images/event-sourcing/single-write.svg loading=lazy alt="Diagram of a User Service performing a single, synchronous write to an event log. A Mail Service then asynchronously reads from the event log. The world-famous &lsquo;shrug emoji&rsquo; person adds much needed comic relief."></p><p>The email service would be monitoring the log for new account creation events, and would send emails in response to those events. If the email service were to go offline, it could simply pick up from where it left off. In fact, the email service could go offline for <em>days</em>, and catch up on unsent emails when it comes back online. It could also contain a memory leak which causes the system to crash every hour, but as long as the email service restarts automatically, your users will not likely perceive a service interruption.</p><h3 id=mitigation-of-data-inconsistencies>Mitigation of Data Inconsistencies</h3><p>Kleppmann <a class=link href=https://www.confluent.io/blog/using-logs-to-build-a-solid-data-infrastructure-or-why-dual-writes-are-a-bad-idea/ target=_blank rel=noopener>points out</a> that systems which employ dual writes pretty much guarantee data consistency problems.</p><p>For example, let‚Äôs say you update a user account record in the database, and then update a cache containing the now stale data. Let‚Äôs further say that the cache update operation fails. Your cache is now out of sync with your database. Have fun debugging the consequences!</p><p><img src=https://arkwright.github.io/images/event-sourcing/cache-out-of-sync.svg loading=lazy alt="Diagram of Your App attempting dual writes again, because you never learn. First to write name=jane to the Database, and then to write name=jane to the Cache. The latter write fails, due to nuclear explosion, which is surprisingly difficult to draw, because it can easily end up looking like mashed potatoes. Whatever. The Cache contains name=barb, because name=jane was not written."></p><p>A read-through cache can exhibit a similar problem. Updates to a user account in the database will not be immediately reflected in its corresponding cache entry until that entry expires. Stale cache data can be very confusing to both users and developers.</p><p>But what if we perform all writes to an event log? The cache can read and apply the events in order. The cache is always in sync with its source of truth, with the standard disclaimers about eventual consistency applying. But under normal circumstances, your cache could be quite consistent with its data source. Should anything go wrong, the cache can be rebuilt by simply starting from scratch and re-consuming the event log.</p><p><img src=https://arkwright.github.io/images/event-sourcing/cache-sync.svg loading=lazy alt="Diagram of Your App synchronously writing name=jane to an event log, which already contains one event, name=barb. Database and Cache asynchronously read events from the log, and are eventually rendered consistent, with both containing name=jane."></p><h3 id=simplicity>Simplicity</h3><p>Nothing about software architecture is truly simple. But anyone who has been burned by the legacy of a bad decision will intuitively understand that simpler solutions are generally preferable. Simple solutions reduce cognitive overload, maximizing the chances that you will correctly predict the system‚Äôs behavior.</p><p>An event sourced architecture really shines as a simplifying abstraction when compared to the Frankenstein architecture which tends to evolve from modest monolithic beginnings. Producers write to a log, consumers read from the log. This simple, unifying principle allows us to reason about data flow between subsystems without becoming bogged down in their idiosyncrasies.</p><p>Kleppmann <a class=link href=https://www.confluent.io/blog/apache-kafka-samza-and-the-unix-philosophy-of-distributed-data/ target=_blank rel=noopener>described</a> the event sourcing approach as Unix philosophy (specifically pipes) for distributed systems. The simplicity of Unix pipes is precisely what makes them so composable and powerful.</p><h3 id=forgiving-of-mistakes>Forgiving Of Mistakes</h3><p>We all love that feeling when we write a piece of code and it works on the first try. That feeling is so wonderful because it is so rare. It is more common to spend as much time debugging our code as we did writing it. Mistakes are by far the normal mode of software development. Anything our architecture can do to help us recover from mistakes will have a dramatic impact on our iteration speed.</p><p>The traditional, stateful model of data persistence is very unforgiving in this regard. A bug in your code which mutates state in the wrong way will often require a one-off, compensating transaction to correct. And it‚Äôs a race against time to make the correction, since subsequent operations based on bad data will only <a class=link href=https://en.wikipedia.org/wiki/Garbage_in,_garbage_out target=_blank rel=noopener>compound the error</a>.</p><p>But what if we can fix the bug and simply rebuild the state by re-consuming events via the patched system? We wouldn‚Äôt need to duct tape our state. When the application is corrected, so is the state. We can reduce instances of fixing the application and then <em>also</em> fixing the state.</p><p>Of course, there will always be exceptions. <a class=link href=https://www.joelonsoftware.com/2002/11/11/the-law-of-leaky-abstractions/ target=_blank rel=noopener>All abstractions leak.</a> But in general we prefer boats with fewer leaks.</p><p><img src=https://arkwright.github.io/images/event-sourcing/leaks.svg loading=lazy alt="Cartoon of a stick figure in a sinking boat, saying &lsquo;We&rsquo;ll fix it in the next sprint.&rsquo; Haven&rsquo;t we all been there?"></p><h3 id=ends-normalization-debate>Ends Normalization Debate</h3><p>Kleppmann makes an <a class=link href=https://www.confluent.io/blog/turning-the-database-inside-out-with-apache-samza/ target=_blank rel=noopener>excellent observation</a> regarding the best practice of database normalization. There is a tension between read- and write-optimized schemas. At a certain point, in order to boost read performance, we are tempted to denormalize our database. We might also attempt to cache query results from the normalized database, usually employing some kind of error-prone, dual write strategy.</p><p><img src=https://arkwright.github.io/images/event-sourcing/normalization.svg loading=lazy alt="Stick figure cartoon re-enactment of that Pulp Fiction scene where Vincent and Jules point their guns at a guy, his hands in the air, and Jules yelling, &lsquo;Say normalization again!&rsquo;"></p><p>A log-oriented system breaks the tension by <em>deriving</em> one or more read models from the log. We accept from the outset that one model cannot be great at everything. The log is write-optimized, and the derived read models can be denormalized to suit their specific usage pattern.</p><h3 id=audit-trail>Audit Trail</h3><p>Greg Young <a class=link href="https://youtu.be/8JKjvY4etTY?t=1m1s" target=_blank rel=noopener>recalls</a> that he was initially attracted to event sourcing because he needed to implement auditing. Storing a log of every event that has occurred in the history of the application provides a natural audit trail.</p><p>If we aren‚Äôt working on, say, a financial application, we tend to think that auditing will not be an important use case for our software. Then an incident occurs in production, and what do we do? We check the logs!</p><p><img src=https://arkwright.github.io/images/event-sourcing/batcomputer.svg loading=lazy alt="Stick figure batman asks the Batcomputer, &lsquo;Where were the Joker&rsquo;s last three known locations?&rsquo; The caption underneath reads, &lsquo;Tragically, the Batcomputer was stateful.&rsquo;"></p><h3 id=better-business-agility>Better Business Agility</h3><p>Kleppmann sees <a class=link href=https://www.confluent.io/blog/turning-the-database-inside-out-with-apache-samza/ target=_blank rel=noopener>agility enhancing</a> benefits in this approach to building software, and I think he‚Äôs on to something.</p><p>Monolithic, stateful systems are optimized for consistency, not for change. At a certain point it becomes difficult to make changes, because those changes must render the system consistent when completed. Within a large system, that is no small feat! The result is that the rate of change decreases, because it becomes a huge pain to run even a small experiment.</p><p>The ability to connect new consumers to the log stream opens up the possibility of <em>bypassing</em> existing systems to build one-off experiments. There is no need to run a migration to modify a database schema ‚Äî¬†simply deploy a new service with a different database, and store the additional data there for the duration of the experiment. The same goes for new read models, which can provide denormalized views for experimental new queries.</p><p>Have you ever noticed that <em>changing</em> an existing system tends to trigger a bikeshedding process? <em>Adding</em> a new system, in my experience, does not produce the same strong political reaction. My hunch is that this is because changing an existing system might break something which a colleague considers incredibly valuable, even sacrosanct. So by architecting our application to allow for the easy introduction of new subsystems, it seems reasonable to expect that we could actually reduce the amount of political debate associated with the running of experiments.</p><p><img src=https://arkwright.github.io/images/event-sourcing/change.svg loading=lazy alt="Two stick figure people, one with a moneybag emoji for a head. Person: &lsquo;I&rsquo;d like to change two pixels on the homepage.&rsquo; Moneybag: &lsquo;Don&rsquo;t, we might lose revenue!&rsquo; Person: &lsquo;I&rsquo;d like to launch a new product line.&rsquo; Moneybag: &lsquo;That&rsquo;s fine.&rsquo;"></p><h2 id=what-might-event-sourcing-look-like-in-practice>What might event sourcing look like in practice?</h2><p>Recall the initial user flow I described earlier: the user signs up for an account, and receives an activation email. Thinking about how to implement these features in an event sourced architecture provides a surprising amount of insight into the pattern and its subtleties. Let‚Äôs work through it!</p><h3 id=signing-up-for-an-account>Signing Up For An Account</h3><p>Our user lands on the account sign up page and fills out the form, providing their <code>username</code>, <code>email</code>, and <code>password</code>.</p><p><img src=https://arkwright.github.io/images/event-sourcing/api-gateway.svg loading=lazy alt="Diagram of smiley face connecting to an API Gateway service over HTTP. I don&rsquo;t know why a smiley face would do that, but I also don&rsquo;t want to judge."></p><p>The user submits the form and an HTTP request is sent to our API Gateway service, which is the public-facing portion of our system. It might implement server-side rendered views, or it might expose an API or <a class=link href=http://samnewman.io/patterns/architectural/bff/ target=_blank rel=noopener>Backend For Frontend (BFF)</a> for a single-page or mobile application to consume.</p><p>We want to build this application in a microservice style, and so we have decided to delegate ownership of all write-related user logic to a User Command service.</p><p><img src=https://arkwright.github.io/images/event-sourcing/signup-fail.svg loading=lazy alt="Diagram of a neutral face emoji sending a request to an API Gateway, which optimistically sends an AccountSignUp event to the event log, both synchronously. The event log is asynchronously read by the User Command service, but validation is unsuccessful, as indicated by a smiling poop emoji who exclaims, &lsquo;Fail!&rsquo; The neutral face emoji is returned a &lsquo;success&rsquo; response, but there are no successes here. Only nightmares. I think my emoji game is improving."></p><p>We might be tempted to have the API Gateway publish an <code>AccountSignUp</code> event, which our User Command service would listen for and process. After all, this is how a lot of event-driven architectures behave ‚Äî¬†the user <em>did a thing</em> that the system can react to. Unfortunately this creates a huge UX problem: we lose the ability to provide immediate feedback to the user. There is no guarantee that the User Command service is currently available ‚Äî it could be overloaded, or it might have crashed. If we publish an <code>AccountSignUp</code> event and some of the form data is invalid, we have no way of informing the user. The best we can do would be to display an optimistic ‚Äúsuccess‚Äù message, <em>hope</em> that the form data is valid, <em>hope</em> that the user account is persisted, and <em>hope</em> that, should any errors occur, the user would return to our website to try again.</p><p>The breakthrough approach here, for me, was when I understood that in an event sourced system, all of the <em>writes</em> must occur to the event log. It could be interesting or useful to log some of the antecedent details (such as requests), but the only thing we really <em>must</em> do is ensure that all writes are modeled as log appends.</p><p><img src=https://arkwright.github.io/images/event-sourcing/signup-success.svg loading=lazy alt="Diagram of smiling face emoji connecting synchronously to the API gateway, which POSTs to a /users endpoint on the User Command service, which in turn sends a synchronous AccountCreated event to the Event Log. Because the User Command service performed pessimistic validation, the success message returned to the smiling face emoji is an honest-to-goodness success."></p><p>If the user interface requires a synchronous response for immediate feedback, so be it. We can achieve this in the traditional, RESTful way, by having the API Gateway issue an HTTP request to the User Command service. Perhaps this would be modeled as a <code>POST /users</code> endpoint. The User Command service would perform any validations, write an <code>AccountCreated</code> event to the event log, and return a <code>201 Created</code> response to the API Gateway. The API Gateway would then render a success message for the user. The user account creation is considered to be a success ‚Äî¬†a historical fact ‚Äî at the moment the log append occurs. (Greg Young <a class=link href="https://youtu.be/8JKjvY4etTY?t=3m3s" target=_blank rel=noopener>emphasizes the importance</a> of storing only facts in our event log.)</p><h3 id=failure-modes>Failure Modes</h3><p>Let‚Äôs think about how this part of the system would handle various failures:</p><ul><li>If the API Gateway‚Äôs HTTP request to the User Command service fails, the API Gateway can immediately render an error message for the user. The user is then able to retry their request.</li><li>If any of the form validations fail, the User Command service can return a <code>400 Bad Request</code> error to the API Gateway, which in turn can render field errors for the user.</li><li>If the event log is unavailable and the User Command service cannot write to it, the User Command service can return a <code>500 Internal Server Error</code> to the API Gateway. The API Gateway can then render an error message for the user, who may retry their request.</li><li>If the User Command service successfully writes to the event log and then dies, or its HTTP response is not delivered to the API Gateway, then the API Gateway will render an error message for the user, believing the User Command service to be unavailable. The user might then retry their request, if they don‚Äôt notice their account activation email first! This could result in a second <code>AccountCreated</code> event being published to the log. It is therefore important that consumers of the event log implement their consumption in an idempotent way.</li></ul><p><img src=https://arkwright.github.io/images/event-sourcing/poops.svg loading=lazy alt="Cartoon of two poop emojis, one bigger than another. I actually drew these as practice for other drawings, but they turned out alright and I thought, &lsquo;Hey, why not decorate with poop?&rsquo;"></p><h3 id=validation>Validation</h3><p>Whenever we are working with user generated data, there is always some validation that must occur. We can think of a few common constraints for our account sign up form:</p><ol><li>The <code>username</code> cannot be blank.</li><li>The <code>email</code> cannot be blank.</li><li>The <code>password</code> cannot be blank.</li><li>No two user accounts should have the same <code>username</code>.</li><li>No two user accounts should have the same <code>email</code>.</li></ol><p>To accommodate some of these rules, we will need to think a bit differently than we are used to.</p><p>Ensuring that fields are not blank can be accomplished in the obvious way: the User Command service checks for the existence and length of these values and returns the appropriate error code to the API Gateway in the event of an invalid submission.</p><p>But how can we enforce the constraints that no two accounts should have the same <code>username</code> or <code>email</code>? If we were using an ACID-compliant relational database, this would be easily achievable by adding a <code>UNIQUE</code> constraint to the <code>username</code> and <code>email</code> columns ‚Äî¬†the database would thereafter refuse to insert duplicates. Since our event log is not a relational database, we will have to devise another way.</p><p>Naturally, the mind will wonder if the User Command service could first read from the database used to store its corresponding read model, searching for duplicate values ‚Äî if a duplicate is found, do not write to the event log. And this approach would <em>appear</em> to work at first, but due to the eventually consistent property of our system, writes to the event log are not immediately reflected in the various read models that our services maintain. A race condition has been introduced: it is possible to create two user accounts with duplicate data in rapid succession, because we cannot guarantee that the read model will be up to date with the first write at the time that the second write occurs.</p><p><img src=https://arkwright.github.io/images/event-sourcing/race-condition.svg loading=lazy alt="Diagram of Darth Vader and a Smiley emoji sending concurrent requests for the username &lsquo;dvader&rsquo; to the User Command service. User Command first checks the Database to see if that username is taken, but in both cases it is not! User Command then writes two AccountCreated events to the Event Log. The Database asynchronously reads those events from the Event Log, accidentally creating duplicate &lsquo;dvader&rsquo; accounts. A poop emoji beams a gigantic smile from within the database, because our data is now&mldr; inconsistent."></p><p>I can‚Äôt remember where I first encountered the following solution, but it struck me as a novel and contrarian approach with a lot of utility: why not simply embrace the fact that the system no longer provides an immediate consistency guarantee? We could design our system to gracefully handle some the uniqueness constraints in a different way:</p><ol><li>We could allow duplicate accounts to be created with the same email address, and simply ignore all but the first creation event. If a user accidentally signs up twice, only one account will be created. The total order of our log messages ensures that these two events will always be processed in the same order. This approach has the undesirable effect of including two account creation events in the log, which might be confusing.</li><li>We could allow more than one user to enjoy the same username. Why not? Social networks allow users to change their names at will. A surrogate key (e.g. universally unique identifier) can be used for internal purposes. The user‚Äôs email address can be used for login purposes.</li><li>We could shamelessly violate <a class=link href=https://martinfowler.com/bliki/CQRS.html target=_blank rel=noopener>CQRS</a> and perform a kind of optimistic concurrency control by allowing the read model to detect when a duplicate username is about to be inserted, and then <em>modify the username</em> to preserve its uniqueness. For example, <code>dvader</code> might be renamed to <code>dvader_1</code>. Finally, the read model would emit another event to notify the user that they should change their username. This seems like a contrived and impractical solution, but consider what happens in macOS when a file is copied and pasted on top of itself: instead of throwing an error, the operating system allows the paste, and automatically renames the second version to be <code>file 2</code>. Still, I don‚Äôt like the conflation of read/write concerns.</li></ol><p>For our user sign up flow, I think we can eliminate the uniqueness constraint for usernames. But what about email addresses? I would prefer not to have duplicate account creation events in the log if we can avoid it.</p><p><img src=https://arkwright.github.io/images/event-sourcing/lock-tag.svg loading=lazy alt="Cartoon of a &lsquo;dvader@empire.gov&rsquo; name tag with a small lock (perhaps one of those airport suitcase locks) shackled through it. The lock isn&rsquo;t attached to anything else. This is a metaphor, okay. Visual metaphors aren&rsquo;t supposed to take the complexities of life into account. Whatever."></p><h3 id=locks>Locks</h3><p>We could make judicious use of locks to enforce a uniqueness constraint for email addresses.</p><p><img src=https://arkwright.github.io/images/event-sourcing/lock-service.svg loading=lazy alt="Diagram of Darth Vader, who clearly has too much time on his hands, requesting a &lsquo;dvader&rsquo; account from the User Command service. User Command requests a lock on the username &lsquo;dvader&rsquo; from the Lock service, which replies with &lsquo;Cool.&rsquo;, as services do. 200 OK, Cool. Anyways, User Command next sends an AccountCreated event for &lsquo;dvader&rsquo; to the event log."></p><p>We would add a new Lock service to our ecosystem. The Lock service does what it says on the tin: other services can use it to obtain a lock on a resource before writing to it. This could be as simple as an HTTP service wrapping a transactional data store, but probably we would want to reach for an off-the-shelf solution.</p><p>When requesting a lock, services would specify a key which uniquely identifies the resource. For example, the key might be <code>dvader@empire.gov</code>. The corresponding value would be a unique identifier representing the service instance requesting the lock.</p><p>After successfully acquiring a lock on an email address, the User Command service can safely publish an event to create an account, or change a user‚Äôs email address. Since only the service instance which holds the lock has permission to perform writes which involve that email address, duplicate account creation events are thereby prevented.</p><p>When the write is successful, the User Command service can release the lock by sending another request to the Lock service. If this is not done, no further writes which involve that email address could be made! The lock would be stored with a time-to-live so that, in the event that the User Command service dies before it can release its lock, the lock is automatically released, preventing a deadlock.</p><p>Unfortunately, there are problems with this approach.</p><h3 id=temporal-anomalies>Temporal Anomalies</h3><p>Kleppmann does a <a class=link href=https://martin.kleppmann.com/2016/02/08/how-to-do-distributed-locking.html target=_blank rel=noopener>great job</a> of explaining why timing-based lock algorithms cannot prevent errors. As it turns out, as soon as we apply a time-to-live to the lock, it becomes unreliable. For example, a long GC pause in a service could actually exceed the time-to-live on our lock, allowing the same lock to be acquired twice! And even if only a small number of locking errors occur during the lifetime of the application, allowing a few duplicate writes to leak through, we will need to modify <em>all</em> event log consumers to handle that exceptional case. If the event stream contains even <em>one</em> duplicate, from the consumer‚Äôs perspective it might as well contain a million of them.</p><p>If we remove the time-to-live from the lock, we will be okay until the User Command service dies immediately after acquiring the lock, but before writing the new user account. When the service restarts after this error, we will be deadlocked. The user will be unable to retry their account creation, because their email address is now permanently locked.</p><p><img src=https://arkwright.github.io/images/event-sourcing/deadlock.svg loading=lazy alt="Diagram of Darth Vader, whom I can now draw from memory, requesting user name &lsquo;dvader&rsquo; from the User Command service. User Command requests a lock on &lsquo;dvader&rsquo; from the Lock service, but explodes in a horrifying nuclear blast after acquiring the lock. User Command fails to send an event to the Event Log, and we are now deadlocked. Now would be a good time to find somebody to blame."></p><p>Really, what we need to do is <a class=link href=https://engineering.linkedin.com/distributed-systems/log-what-every-software-engineer-should-know-about-real-time-datas-unifying target=_blank rel=noopener>‚Äúsqueeze all the non-determinism out of the input stream‚Äù</a>. Kleppmann provides two strategies for achieving this.</p><h3 id=fencing-tokens>Fencing Tokens</h3><p>The first strategy is to have the Lock service implment a <a class=link href=https://martin.kleppmann.com/2016/02/08/how-to-do-distributed-locking.html target=_blank rel=noopener>fencing token</a>. Basically, each time a lock is acquired, the Lock service assigns a monotonically increasing integer to the lock. If the same lock is accidentally acquired twice due to temporal anomalies, each version of the lock will have a different integer associated with it. Requests to write must then supply the integer, and the service which handles the writes is responsible for ignoring writes whose integer is not larger than that of the previous write.</p><p><img src=https://arkwright.github.io/images/event-sourcing/fencing-token.svg loading=lazy alt="Diagram of Darth Vader and, later, a Stormtrooper, attempting to create account &lsquo;dvader&rsquo;. User Command service acquires a lock with fencing token #1 for Darth Vader&rsquo;s attempt, and publishes an AccountCreated event, which somehow becomes very delayed in transit. Meanwhile, User Command does the same thing for Stormtrooper&rsquo;s request, acquiring fencing token #2, and promptly delivering an event to the Event Log. The effect is that Stormtrooper&rsquo;s event was sent after, but arrives before, Darth Vader&rsquo;s event. The Database, when asynchronously reading these events, reads Stormtrooper&rsquo;s request first, which contains fencing token #2. Next, when Database reads Darth Vader&rsquo;s request, complete with fencing token #1, it rejects it, because fencing tokens are invalid if received in decreasing order."></p><p>Notice something about this strategy? It looks awfully similar to a totally ordered log! This implies that a log-oriented solution might be possible. It also requires a heck of a lot of plumbing, in my opinion. Each service responsible for writing to a locked resource must understand and correctly implement the monotonically increasing integer check.</p><h3 id=filtering-duplicates>Filtering Duplicates</h3><p>Kleppmann‚Äôs <a class=link href=https://www.infoq.com/presentations/event-streams-kafka target=_blank rel=noopener>second strategy</a> is a log-oriented one. Basically, services wishing to acquire a lock publish a request event to a topic within the event log. A consumer service (similar to our Lock service) reads those events and essentially filters out duplicates, finally publishing a different event (the actual write) to a different topic within the event log. If the consumer service dies, it can simply reconstruct its state by replaying log events.</p><p><img src=https://arkwright.github.io/images/event-sourcing/deduplicator.svg loading=lazy alt="Diagram of Darth Vader and Boba Fett both requesting account &lsquo;dvader&rsquo; from the User Command service, which publishes two events to the Request Log. A Deduplicator service asynchronously reads from this log and writes a single event to the Write Log, representing the winning request. A database finally asynchronously reads the winning request from the Write Log, and creates a user account for real. Both Darth Vader and Boba Fett receive optimistic &lsquo;success&rsquo; messages. They are gonna be pissed later. To be honest, I&rsquo;m choosing Star Wars characters entirely based on how easy it is to draw their heads, which in practice means I&rsquo;m using only people who wear helmets. I tried drawing Yoda but honestly that was a bad idea. Same for Chewbacca. Same for R2D2. Same for C3P0. I&rsquo;m trying, okay?"></p><p>This is a very clever solution, built out of simple components, and it relieves the services handling writes from the responsibility of implementing a fencing token. Unfortunately the cost of this simplicity is losing the ability to provide a synchronous response to the user ‚Äî¬†we can‚Äôt tell them if their write was successful, because consumption of the event log might be delayed. I wonder if we can come up with a reusable solution which allows us to provide immediate feedback to the user?</p><h3 id=uniq-service>Uniq Service</h3><p>Recall the distributed-systems-as-Unix-pipes philosophy. It might be possible to create a composable Uniq service ‚Äî similar to the Unix <code>uniq</code> command ‚Äî which could be reused across multiple services for their event deduplication and locking needs.</p><p><img src=https://arkwright.github.io/images/event-sourcing/pipes.svg loading=lazy alt="Handwritten version of the following, for emphasis: uniq_service | uniq | event log"></p><p>How would this Uniq service work? All write events subject to uniqueness constraints would be sent to Uniq for deduplication and constraint checking purposes, before being forwarded on to the event log. Uniq could expose a RESTful API for create, update, and delete operations, . An <code>/event</code> endpoint would do nicely.</p><p><img src=https://arkwright.github.io/images/event-sourcing/uniq.svg loading=lazy alt="Diagram of Darth Vader and X-Wing Pilot Luke Skywalker (with helmet!) sending simultaneous requests for account &lsquo;dvader&rsquo; to the User Command service. User Command attempts to publish two AccountCreated events through the stateful Uniq service, which rejects the second request because &lsquo;dvader&rsquo; is already taken. Darth Vader&rsquo;s request receives a 200 OK, and Luke Skywalker&rsquo;s receives a 409 Conflict. But don&rsquo;t worry, in the movie Luke totally wins, or at least they end up on the same team. Finally, the successful AccountCreated event is synchronously written by the Uniq service to the event log. Uniq is acting in a write-through capacity, here."></p><p>When performing user account creation, the User Command service would construct its desired log message and <code>POST</code> it to the Uniq service. Uniq would support a simple configuration file which maps fields of log messages to CRUD operations. For example, when Uniq receives our <code>AccountCreated</code> event, it would extract the <code>email</code> field from that event and add that email to a set it maintains in memory. If the email does not already exist in the set, Uniq writes that event to the log, and returns a <code>200 OK</code> response. In this way, Uniq can provide a synchronous response to our User Command service, which facilitates immediate feedback for the user. Uniq acts as a proxy for events ‚Äî¬†just another piece of the pipeline.</p><p>If an email address already exists in the set, Uniq would return a <code>409 Conflict</code> response, and would <em>not</em> forward the event to the log. Use cases involving changing or deleting an existing email address are easily supported by the HTTP <code>PATCH</code> and <code>DELETE</code> semantics. Unlike the Unix <code>uniq</code>, our Uniq requires these semantics because it is stateful. It isn‚Äôt simply counting unique items, but rather allowing that set of unique items to be maintained in the face of change.</p><p>Of course, given that Uniq will be maintaining a set in memory, we must consider what will happen in the event that it crashes. If this were to occur, Uniq can rebuild its set by re-consuming log messages. When Uniq has caught up with where it left off, it can accept write traffic again. Because our log is totally ordered, and because Uniq processes write requests serially, it should never commit a duplicate write to the log, even when recovering from an outage.</p><p>If availability is a concern, a second instance of Uniq can operate in follower mode, consuming from the event log to maintain a replica of the leader‚Äôs state. When the leader dies, the follower can be promoted to leader.</p><h3 id=relational-database-envy>Relational Database Envy</h3><p>So we have our strategy for handling concurrent requests for user accounts with the same email address: we will pipe all writes through a Uniq service, which enforces the uniqueness constraint.</p><p>Couldn‚Äôt we have used a relational database to enforce this constraint instead? Well, yes, we could. Michael Ploed makes the <a class=link href="https://youtu.be/A0goyZ9F4bg?t=50m14s" target=_blank rel=noopener>wise recommendation</a> that we implement the level of consistency that our business domain requires. The ideal level of consistency for one subsystem may not be needed across the entire system. For example, it might be considered unacceptable for two user accounts to <em>ever</em> accidentally share the same email address, since this would prevent users from logging in. Therefore, we can enforce a uniqueness constraint only for email addresses, paying the complexity cost because we see it as justifiable. But for other subsystems where we might traditionally enforce a uniqueness constraint, we could employ more creative solutions. We are dialing in the amount of consistency that each part of the system requires.</p><p>So it would make sense for the User Command service to write to a relational database implementing a uniqueness constraint. But this approach creates one very unfortunate side-effect: we lose atomicity. Writing a new user to the database, and <em>then</em> writing an event to the log, opens the possibility that the first write will succeed but the second one will fail. This situation would render our data inconsistent, and a compensating transaction would be required to reconcile the two. Dual writes strike again!</p><p><img src=https://arkwright.github.io/images/event-sourcing/atomicity.svg loading=lazy alt="Diagram of user wearing fashionable top hat sending a request to the User Command service, which performs an ill-advised dual write, first to a database, and then to an Event Log. Skull and crossbones indicates that the second write is headed for a grim fate."></p><p>It might be possible to create a second table in our relational database, and write our events to that table. We could then wrap both our user write and the event write in a transaction, regaining atomicity. But now we need a way to get those events <em>out</em> of the database and into the event log, and ideally without implementing a polling strategy, which would increase replication lag. The amount of plumbing required to make this happen is excessive, in my opinion. And the solution wouldn‚Äôt be reusable across services.</p><h3 id=the-read-model>The Read Model</h3><p>So our User Command service is able to write an <code>AccountCreated</code> event to the log via the Uniq service. But how would we handle reads? One cannot simply perform queries against an event log, since query performance would decrease as the log grows in size! To support reads, we will need to implement a <em>read model</em>.</p><p>The read model consists a service wrapping a persistence mechanism. Most likely we would choose a database which provides a good fit for the types of queries we will be performing. For our User Query service we will assume a document database which stores JSON-like documents.</p><p><img src=https://arkwright.github.io/images/event-sourcing/read-model.svg loading=lazy alt="Diagram of Event Log containing names of Star Wars characters, being asynchronously read by a User Query service, which synchronously writes those characters to its own private database, in a denormalized schema, for later querying."></p><p>The read model will consume relevant events from the event log, and update its database accordingly. In the case of the User Query service, every <code>AccountCreated</code> event consumed would trigger the insertion of a new user document into the database.</p><p>Where this pattern can become very powerful is in maintaining highly optimized, incrementally computed query results. One could imagine introducing a Friends service which maintains a list of frequently contacted friends for each user, entirely derived from log messages. Batch computing this contact frequency information could take a long time, and the results would quickly become stale. Incrementally computing with each new piece of information can provide a more consistent view, while maintaining fast query response times.</p><p>Another interesting property is the potential for the elimination of schema migrations. Denormalizing the read model brings the possibility of introducing NoSQL stores. The addition of a new field would be handled in application code, and a schema ‚Äúrollback‚Äù, if one were required, could be accomplished by reverting the application code and replaying events from the affected period.</p><h3 id=sending-mail>Sending Mail</h3><p>After our user has signed up, we want to send them an account activation email. To accomplish this, we will create a Mail service which monitors the event log for <code>AccountCreated</code> events, and sends activation emails to those users, probably by calling the RESTful API of a third-party email provider.</p><p><img src=https://arkwright.github.io/images/event-sourcing/mail.svg loading=lazy alt="Diagram of Event Log being asynchronously read by Mail service, which sends requests to a Fancy Cloud Email Co., represented by a huge factory puffing clouds of smoke. Cloud, get it? After each email is sent, Mail service writes a checkpoint corresponding to the event processed to a Checkpoint log. I had way too much fun trying to figure out how to draw the Fancy Cloud Email Co."></p><p>But what happens if the Mail service crashes after reading a message from the log? As it turns out, this is not a problem, provided that the mail service persists the ID of the last message it consumed ‚Äî¬†a <em>checkpoint</em>. And where better to persist this ID than to a topic within the event log!</p><p>How frequently should we store checkpoints? If we store a checkpoint for every message consumed, log consumption speed will be limited by the need to write a checkpoint in between every read. If this was inadequate for our purposes, we could have the Mail service store a checkpoint every hundred writes, or every thousand, or every 60 seconds. But then wouldn‚Äôt we be at risk of sending a hundred, or a thousand, or 60 seconds worth of duplicate emails if the Mail service crashes?</p><p>As it turns out, the Mail service cannot guarantee exactly-once delivery of emails to users. Let‚Äôs say that the Mail service has already consumed event 1, and is now consuming event 2 from the log. If an email is sent and the service crashes before checkpoint 2 can be written, when the service restarts it will begin working from the next event after its last checkpoint. The last checkpoint was 1, so event 2 will be processed again. A duplicate email will be sent!</p><p><img src=https://arkwright.github.io/images/event-sourcing/duplicates.svg loading=lazy alt="Diagram of Event Log being asynchronously consumed by Mail service, which is on fire. Drawing fire is tricky, let me tell you. I added some smoke lines to really make the effect. Anyways, Mail service sends an email to Fancy Cloud Email Co. and then suddenly dies before it can write to the Checkpoint log. Another instance Mail service, which has restarted, reads the checkpoint, reads from the event log, and sends the same email again! Finally, restarted Mail service writes a checkpoint successfully, absolutely not dying in the process. All of this to send an email. But also to prove that it&rsquo;s impossible to avoid sending duplicate emails."></p><p>Writing the checkpoint before sending the email will only make things worse. If we store checkpoint 2 and then crash before sending email 2, when the service restarts it will begin working from the next event after its last checkpoint. The last checkpoint was 2, so event 3 will be processed. In this case, event 2 will be skipped!</p><p>Since we cannot prevent the Mail service from sending duplicate emails, and since it would be a Very Bad Thing‚Ñ¢ to fail to send any account activation emails, we can feel a bit better about setting a less frequent checkpoint rate.</p><h3 id=dependency-woes>Dependency Woes</h3><p>It is a prudent exercise to think about what would happen if our third-party email provider were to suffer various failures.</p><p>If for any reason our Mail service does not receive a response from the provider, we can retry the request. In fact, we <em>need</em> to retry the request, because the log-oriented nature of our system can only guarantee that all events are processed if they are handled sequentially. If we start skipping events, we would need to enqueue those skipped events into ‚Äî you guessed it ‚Äî another log for later processing. It‚Äôs logs all the way down.</p><p><img src=https://arkwright.github.io/images/event-sourcing/retries.svg loading=lazy alt="Diagram of Mail service asynchronously reading from the Event Log, and retrying attempts to send email to Fancy Cloud Email Co., which is on fire right now. I mean like, really on fire. So much so that a stick figure person wearing a top hat is holding their hand up and saying, &lsquo;Now is not a good time.&rsquo; That&rsquo;s a lot of fire. So the retries keep coming and the top hat person keeps saying no, and we never find out how this episode ends."></p><p>So the system will guarantee delivery by <em>pausing</em> when an error occurs, polling for success, and resuming when the error condition has passed. This entails installing a <a class=link href=https://martinfowler.com/bliki/CircuitBreaker.html target=_blank rel=noopener>circuit breaker</a> to gate calls to the email provider. If the provider becomes unresponsive, the Mail service will retry the request repeatedly until the circuit breaker trips, at which point it will retry the request at a slower rate. When the provider comes back online, a request will eventually be successful and the Mail service can catch up with its backlog.</p><h3 id=thats-all-folks>That‚Äôs All, Folks</h3><p>Our toy system is now complete. Obviously this represents merely the user signup flow for what would be a much larger application. I would go on, but as you can see, even this slice of architecture requires a lengthy description. Nevertheless, I hope this has been a useful dive into the finer details how we might go implementing such a system. I know I‚Äôve learned a lot while writing it!</p><p><img src=https://arkwright.github.io/images/event-sourcing/architecture.svg loading=lazy alt="Diagram of the final architecture, just so we&rsquo;re on the same page. Darth Vader (naturally) sends a request to API Gateway, which forwards it to User Command, which writes an event through Uniq (for deduplication), which lands in the Event Log. User Query asynchronously reads from Event Log to update its own User Database. Mail also asynchronously reads from Event Log in order to send requests to Fancy Cloud Email Co. Really, it&rsquo;s not as complicated as it looks. It&rsquo;s kind of like modern art. Some people look at it and are all like, &lsquo;I could totally do that.&rsquo; Well, yeah, you probably could. In fact, I think that&rsquo;s kind of the point."></p><h2 id=where-do-we-go-from-here>Where do we go from here?</h2><p>Event sourcing is a radically different way of looking at software architecture. Predictably, this new approach is not without its learning curve. It also comes with tantalizing potential benefits. The question is how to sensibly proceed.</p><p>I am reminded of Spolsky‚Äôs <a class=link href=https://www.joelonsoftware.com/2002/11/11/the-law-of-leaky-abstractions/ target=_blank rel=noopener>Law of Leaky Abstractions</a>. Event sourcing is just another abstraction which seeks to simplify the complexity of the software we write. Naturally, this abstraction will leak, creating problems for us. But it is important to keep in mind that the monolithic, relational, strongly consistent style of architecture is <em>also</em> an abstraction. Our comfort with the abstraction we know too often spares us the terrifying advantages of new ways of doing things!</p><p>One thing I have learned over the years is that I can never predict the practical consequences of introducing an unfamiliar technique into an organization. Because something always goes wrong, we actually <em>need</em> to implement the technique to discover where it breaks down. Only after introduction can we identify the concrete problems, and begin to devise solutions.</p><p>The knowledge that something <em>will</em> go wrong is often used as a justification for not experimenting with new techniques at all. ‚ÄúWe‚Äôll revisit this discussion <em>later</em>.‚Äù Later effectively means never. Interestingly, this seems to be the wrong conclusion to draw from the mere possibility of risk. Problems may be unavoidable, but we have the power to control the scope of the introduction, and thereby shape the size of the problems encountered. Given that we possess a ‚Äúrisk knob‚Äù which we can dial down to comfortable levels, what justification remains for failing to experiment with new techniques?</p><p>I think the most sensible course of action is to treat event sourcing as an evolutionary pattern. Incorporate this pattern into a portion of your project ‚Äî get your feet wet. But don‚Äôt dive in, because you‚Äôll probably drown. But don‚Äôt stay out of the pool, because then you‚Äôll never learn to swim! Learning takes time, so the sooner you can get started, the sooner you can determine how to incorporate the benefits while mitigating the pitfalls. As the <a class=link href=http://nicholaskuechler.com/2006/10/23/favorite-quotes-chinese-proverb-best-time-to-plant-a-tree/ target=_blank rel=noopener>famous Chinese proverb</a> says:</p><blockquote><p>‚ÄúThe best time to [begin reinforcing event sourcing patterns] was 20 [sprints] ago.‚Äù</p></blockquote><p><img src=https://arkwright.github.io/images/event-sourcing/ants.svg loading=lazy alt="Picture of a bunch of ants marching towards an anthill, and walking inside. I was considering using this as the title graphic, but I went with the insects instead. It&rsquo;s all about artistic choices."></p><p>from Hacker News <a class=link href=https://ift.tt/2ZBtKHW target=_blank rel=noopener>https://ift.tt/2ZBtKHW</a></p></section><footer class=article-footer><section class=article-copyright><svg xmlns="http://www.w3.org/2000/svg" class="icon icon-tabler icon-tabler-copyright" width="24" height="24" viewBox="0 0 24 24" stroke-width="2" stroke="currentcolor" fill="none" stroke-linecap="round" stroke-linejoin="round"><path stroke="none" d="M0 0h24v24H0z"/><circle cx="12" cy="12" r="9"/><path d="M14.5 9a3.5 4 0 100 6"/></svg><span>Licensed under CC BY-NC-SA 4.0</span></section></footer></article><footer class=site-footer><section class=copyright>&copy;
2020 -
2022 ZYChimne</section><section class=powerby>Built with <a href=https://gohugo.io/ target=_blank rel=noopener>Hugo</a><br>Theme <b><a href=https://github.com/CaiJimmy/hugo-theme-stack target=_blank rel=noopener data-version=3.11.0>Stack</a></b> designed by <a href=https://jimmycai.com target=_blank rel=noopener>Jimmy</a></section></footer><div class=pswp tabindex=-1 role=dialog aria-hidden=true><div class=pswp__bg></div><div class=pswp__scroll-wrap><div class=pswp__container><div class=pswp__item></div><div class=pswp__item></div><div class=pswp__item></div></div><div class="pswp__ui pswp__ui--hidden"><div class=pswp__top-bar><div class=pswp__counter></div><button class="pswp__button pswp__button--close" title="Close (Esc)"></button>
<button class="pswp__button pswp__button--share" title=Share></button>
<button class="pswp__button pswp__button--fs" title="Toggle fullscreen"></button>
<button class="pswp__button pswp__button--zoom" title="Zoom in/out"></button><div class=pswp__preloader><div class=pswp__preloader__icn><div class=pswp__preloader__cut><div class=pswp__preloader__donut></div></div></div></div></div><div class="pswp__share-modal pswp__share-modal--hidden pswp__single-tap"><div class=pswp__share-tooltip></div></div><button class="pswp__button pswp__button--arrow--left" title="Previous (arrow left)"></button>
<button class="pswp__button pswp__button--arrow--right" title="Next (arrow right)"></button><div class=pswp__caption><div class=pswp__caption__center></div></div></div></div></div><script src=https://cdn.jsdelivr.net/npm/photoswipe@4.1.3/dist/photoswipe.min.js integrity="sha256-ePwmChbbvXbsO02lbM3HoHbSHTHFAeChekF1xKJdleo=" crossorigin=anonymous defer></script><script src=https://cdn.jsdelivr.net/npm/photoswipe@4.1.3/dist/photoswipe-ui-default.min.js integrity="sha256-UKkzOn/w1mBxRmLLGrSeyB4e1xbrp4xylgAWb3M42pU=" crossorigin=anonymous defer></script><link rel=stylesheet href=https://cdn.jsdelivr.net/npm/photoswipe@4.1.3/dist/default-skin/default-skin.css integrity="sha256-c0uckgykQ9v5k+IqViZOZKc47Jn7KQil4/MP3ySA3F8=" crossorigin=anonymous><link rel=stylesheet href=https://cdn.jsdelivr.net/npm/photoswipe@4.1.3/dist/photoswipe.css integrity="sha256-SBLU4vv6CA6lHsZ1XyTdhyjJxCjPif/TRkjnsyGAGnE=" crossorigin=anonymous></main><aside class="sidebar right-sidebar sticky"><section class="widget archives"><div class=widget-icon><svg xmlns="http://www.w3.org/2000/svg" class="icon icon-tabler icon-tabler-hash" width="24" height="24" viewBox="0 0 24 24" stroke-width="2" stroke="currentcolor" fill="none" stroke-linecap="round" stroke-linejoin="round"><path stroke="none" d="M0 0h24v24H0z"/><line x1="5" y1="9" x2="19" y2="9"/><line x1="5" y1="15" x2="19" y2="15"/><line x1="11" y1="4" x2="7" y2="20"/><line x1="17" y1="4" x2="13" y2="20"/></svg></div><h2 class="widget-title section-title">Table of contents</h2><div class=widget--toc><nav id=TableOfContents><ol><li><a href=#may-the-source-be-with-you>May the source be with you.</a></li><li><a href=#things-we-will-talk-about>Things We Will Talk About</a></li><li><a href=#what-the-hell-is-event-sourcing>What the hell is event sourcing?</a></li><li><a href=#what-can-event-sourcing-do-for-me>What can event sourcing do for me?</a><ol><li><a href=#historical-queries>Historical Queries</a></li><li><a href=#immutable-data>Immutable Data</a></li><li><a href=#time-traveling-debugging>Time-Traveling Debugging</a></li><li><a href=#easily-connect-data-consumers>Easily Connect Data Consumers</a></li><li><a href=#reasonable-scaling-defaults>Reasonable Scaling Defaults</a></li><li><a href=#fault-tolerance-and-resiliency>Fault Tolerance and Resiliency</a></li><li><a href=#mitigation-of-data-inconsistencies>Mitigation of Data Inconsistencies</a></li><li><a href=#simplicity>Simplicity</a></li><li><a href=#forgiving-of-mistakes>Forgiving Of Mistakes</a></li><li><a href=#ends-normalization-debate>Ends Normalization Debate</a></li><li><a href=#audit-trail>Audit Trail</a></li><li><a href=#better-business-agility>Better Business Agility</a></li></ol></li><li><a href=#what-might-event-sourcing-look-like-in-practice>What might event sourcing look like in practice?</a><ol><li><a href=#signing-up-for-an-account>Signing Up For An Account</a></li><li><a href=#failure-modes>Failure Modes</a></li><li><a href=#validation>Validation</a></li><li><a href=#locks>Locks</a></li><li><a href=#temporal-anomalies>Temporal Anomalies</a></li><li><a href=#fencing-tokens>Fencing Tokens</a></li><li><a href=#filtering-duplicates>Filtering Duplicates</a></li><li><a href=#uniq-service>Uniq Service</a></li><li><a href=#relational-database-envy>Relational Database Envy</a></li><li><a href=#the-read-model>The Read Model</a></li><li><a href=#sending-mail>Sending Mail</a></li><li><a href=#dependency-woes>Dependency Woes</a></li><li><a href=#thats-all-folks>That‚Äôs All, Folks</a></li></ol></li><li><a href=#where-do-we-go-from-here>Where do we go from here?</a></li></ol></nav></div></section></aside></div><script src=https://cdn.jsdelivr.net/npm/node-vibrant@3.1.5/dist/vibrant.min.js integrity="sha256-5NovOZc4iwiAWTYIFiIM7DxKUXKWvpVEuMEPLzcm5/g=" crossorigin=anonymous></script><script type=text/javascript src=/ts/main.js defer></script>
<script>(function(){const e=document.createElement("link");e.href="https://fonts.googleapis.com/css2?family=Lato:wght@300;400;700&display=swap",e.type="text/css",e.rel="stylesheet",document.head.appendChild(e)})()</script></body></html>