<!doctype html><html lang=en-us dir=ltr><head><meta charset=utf-8><meta name=viewport content="width=device-width,initial-scale=1"><meta name=description content="BodyPix - Person Segmentation in the Browser Note: We&amp;rsquo;ve just released Version 2.0 with multi-person support, a new ResNet model and API. Check out the new documentation below. This package contains a standalone model called BodyPix, as well as some demos, for running real-time person and body part segmentation in the browser using TensorFlow.js.
Try the demo here!
This model can be used to segment an image into pixels that are and are not part of a person, and into pixels that belong to each of twenty-four body parts."><title>BodyPix: Person Segmentation in the Browser</title><link rel=canonical href=https://Nexus-Security.github.io/posts/2016-09-09-bodypix-person-segmentation-in-browser/><link rel=stylesheet href=/scss/style.min.7dbfdd4b0c439bdacf631096fda79b869b5850a9d35a3f67b9a557f3010f3972.css><meta property="og:title" content="BodyPix: Person Segmentation in the Browser"><meta property="og:description" content="BodyPix - Person Segmentation in the Browser Note: We&amp;rsquo;ve just released Version 2.0 with multi-person support, a new ResNet model and API. Check out the new documentation below. This package contains a standalone model called BodyPix, as well as some demos, for running real-time person and body part segmentation in the browser using TensorFlow.js.
Try the demo here!
This model can be used to segment an image into pixels that are and are not part of a person, and into pixels that belong to each of twenty-four body parts."><meta property="og:url" content="https://Nexus-Security.github.io/posts/2016-09-09-bodypix-person-segmentation-in-browser/"><meta property="og:site_name" content="0x000216"><meta property="og:type" content="article"><meta property="article:section" content="Posts"><meta property="article:published_time" content="2019-10-11T02:48:00+01:00"><meta property="article:modified_time" content="2019-10-11T02:48:00+01:00"><meta name=twitter:title content="BodyPix: Person Segmentation in the Browser"><meta name=twitter:description content="BodyPix - Person Segmentation in the Browser Note: We&amp;rsquo;ve just released Version 2.0 with multi-person support, a new ResNet model and API. Check out the new documentation below. This package contains a standalone model called BodyPix, as well as some demos, for running real-time person and body part segmentation in the browser using TensorFlow.js.
Try the demo here!
This model can be used to segment an image into pixels that are and are not part of a person, and into pixels that belong to each of twenty-four body parts."></head><body class=article-page><script>(function(){const e="StackColorScheme";localStorage.getItem(e)||localStorage.setItem(e,"auto")})()</script><script>(function(){const t="StackColorScheme",e=localStorage.getItem(t),n=window.matchMedia("(prefers-color-scheme: dark)").matches===!0;e=="dark"||e==="auto"&&n?document.documentElement.dataset.scheme="dark":document.documentElement.dataset.scheme="light"})()</script><div class="container main-container flex on-phone--column extended"><aside class="sidebar left-sidebar sticky"><button class="hamburger hamburger--spin" type=button id=toggle-menu aria-label="Toggle Menu">
<span class=hamburger-box><span class=hamburger-inner></span></span></button><header><figure class=site-avatar><a href=/><img src=/img/avatar_hue825486955cd7c56d95e38b4bd2a8e3c_229979_300x0_resize_box_3.png width=300 height=300 class=site-logo loading=lazy alt=Avatar></a></figure><div class=site-meta><h1 class=site-name><a href=/>0x000216</a></h1><h2 class=site-description>Where Do Russian Hackers Store Their Exploits? ðŸ¤“ /ussr/bin/ ðŸ˜‹</h2></div></header><ol class=social-menu><li><a href=https://github.com/Nexus-Security target=_blank title=GitHub><svg width="72" height="72" viewBox="0 0 72 72" xmlns="http://www.w3.org/2000/svg"><g fill="none" fill-rule="evenodd"><path d="M36 72c19.882251.0 36-16.117749 36-36 0-19.882251-16.117749-36-36-36-19.882251 365231026e-23-36 16.117749-36 36C24348735e-22 55.882251 16.117749 72 36 72z" fill="#3e75c3"/><path d="M35.9985 12C22.746 12 12 22.7870921 12 36.096644c0 10.6440272 6.876 19.6751861 16.4145 22.8617681C29.6145 59.1797862 30.0525 58.4358488 30.0525 57.7973276 30.0525 57.2250681 30.0315 55.7100863 30.0195 53.6996482c-6.6765 1.4562499-8.085-3.2302544-8.085-3.2302544-1.0905-2.7829884-2.664-3.5239139-2.664-3.5239139C17.091 45.4500754 19.4355 45.4801943 19.4355 45.4801943c2.4075.1701719 3.675 2.4833051 3.675 2.4833051 2.142 3.6820383 5.6175 2.6188404 6.9855 2.0014024C30.3135 48.4077535 30.9345 47.3460615 31.62 46.7436831 26.2905 46.1352808 20.688 44.0691228 20.688 34.8361671c0-2.6308879.9345-4.781379 2.4705-6.4665327C22.911 27.7597262 22.0875 25.3110578 23.3925 21.9934585c0 0 2.016-.6475568 6.6 2.4697516C31.908 23.9285993 33.96 23.6620468 36.0015 23.6515052 38.04 23.6620468 40.0935 23.9285993 42.0105 24.4632101c4.581-3.1173084 6.5925-2.4697516 6.5925-2.4697516C49.9125 25.3110578 49.089 27.7597262 48.8415 28.3696344 50.3805 30.0547881 51.309 32.2052792 51.309 34.8361671c0 9.2555448-5.6115 11.29309-10.9575 11.8894446.860999999999997.7439374 1.629 2.2137408 1.629 4.4621184C41.9805 54.4089489 41.9505 57.0067059 41.9505 57.7973276 41.9505 58.4418726 42.3825 59.1918338 43.6005 58.9554002 53.13 55.7627944 60 46.7376593 60 36.096644 60 22.7870921 49.254 12 35.9985 12" fill="#fff"/></g></svg></a></li><li><a href=mailto:0x000216@gmail.com target=_blank title=Email><svg id="Layer_1" xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink" viewBox="0 0 512 512" style="enable-background:new 0 0 512 512"><path style="fill:#e6f3ff" d="M512 105.739v300.522c0 27.715-22.372 50.087-50.087 50.087H50.087C22.372 456.348.0 433.976.0 406.261V105.739c0-.89.0-1.781.111-2.671 1.336-25.6 21.704-45.969 47.304-47.304.89-.111 1.781-.111 2.671-.111h411.826c.89.0 1.892.0 2.783.111 25.489 1.336 45.857 21.704 47.193 47.193C512 103.847 512 104.849 512 105.739z"/><path style="fill:#cfdbe6" d="M464.696 55.763c-.892-.111-1.891-.111-2.783-.111H256v400.696h205.913c27.715.0 50.087-22.372 50.087-50.087V105.739c0-.89.0-1.892-.111-2.783C510.553 77.468 490.184 57.099 464.696 55.763z"/><path style="fill:#ff4b26" d="M511.889 102.957c-1.336-25.489-21.704-45.857-47.193-47.193C382.89 137.569 336.951 183.509 256 264.459 225.291 233.732 77.61 85.958 47.416 55.763c-25.6 1.336-45.969 21.704-47.304 47.304C0 103.958.0 104.849.0 105.739v300.522c0 27.715 22.372 50.087 50.087 50.087h16.696V169.739l165.621 165.51c6.456 6.567 15.026 9.795 23.597 9.795 8.57.0 17.141-3.228 23.597-9.795l165.621-165.621v286.72h16.696c27.715.0 50.087-22.372 50.087-50.087V105.739C512 104.849 512 103.847 511.889 102.957z"/><path style="fill:#d93f21" d="M279.596 335.249l165.621-165.621v286.72h16.696c27.715.0 50.087-22.372 50.087-50.087V105.739c0-.89.0-1.892-.111-2.783-1.336-25.489-21.704-45.857-47.193-47.193C382.891 137.569 336.951 183.509 256 264.459v80.584C264.57 345.043 273.141 341.816 279.596 335.249z"/><g/><g/><g/><g/><g/><g/><g/><g/><g/><g/><g/><g/><g/><g/><g/></svg></a></li><li><a href=https://www.buymeacoffee.com/0x000216 target=_blank title=RSS><svg id="Layer_1" xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink" viewBox="0 0 512 512" style="enable-background:new 0 0 512 512"><path style="fill:#f78c20" d="M78.333 355.334C35.14 355.334.0 390.474.0 433.667S35.14 512 78.333 512s78.333-35.14 78.333-78.333-35.14-78.333-78.333-78.333z"/><g><path style="fill:#ffa929" d="M78.333 381.445c-28.795.0-52.222 23.427-52.222 52.222s23.427 52.222 52.222 52.222 52.222-23.427 52.222-52.222-23.427-52.222-52.222-52.222z"/><path style="fill:#ffa929" d="M477.918 264.861c-21.843-51.641-53.111-98.019-92.936-137.842-39.824-39.824-86.201-71.093-137.843-92.935C193.669 11.468 136.874.0 78.333.0c-4.807.0-8.704 3.897-8.704 8.704v85.519c0 4.807 3.897 8.704 8.704 8.704 182.37.0 330.74 148.369 330.74 330.74.0 4.807 3.897 8.704 8.704 8.704h85.52c4.807.0 8.704-3.897 8.704-8.704C512 375.126 500.533 318.331 477.918 264.861z"/><path style="fill:#ffa929" d="M78.333 163.853c-4.807.0-8.704 3.897-8.704 8.704v95.74c0 4.807 3.897 8.704 8.704 8.704 86.386.0 156.666 70.281 156.666 156.666.0 4.807 3.897 8.704 8.704 8.704h95.74c4.807.0 8.704-3.897 8.704-8.704.0-72.07-28.065-139.826-79.027-190.787-50.961-50.961-118.717-79.027-190.787-79.027z"/></g><g><path style="fill:#f78c20" d="M78.333 242.186c-2.918.0-5.817.076-8.704.206v25.905c0 4.807 3.897 8.704 8.704 8.704 86.386.0 156.666 70.281 156.666 156.666.0 4.807 3.897 8.704 8.704 8.704h25.905c.129-2.886.206-5.786.206-8.704.0-105.752-85.729-191.481-191.481-191.481z"/><path style="fill:#f78c20" d="M78.333 68.113c-2.91.0-5.81.042-8.704.11v26.001c0 4.807 3.897 8.704 8.704 8.704 182.37.0 330.74 148.369 330.74 330.74.0 4.807 3.897 8.704 8.704 8.704h26.001c.067-2.894.11-5.793.11-8.704C443.887 231.777 280.223 68.113 78.333 68.113z"/></g><g/><g/><g/><g/><g/><g/><g/><g/><g/><g/><g/><g/><g/><g/><g/></svg></a></li><li><a href target=_blank title=Slack><svg width="256" height="256" viewBox="0 0 256 256" xmlns="http://www.w3.org/2000/svg" preserveAspectRatio="xMidYMid"><path d="M165.964 15.838c-3.89-11.975-16.752-18.528-28.725-14.636-11.975 3.89-18.528 16.752-14.636 28.725l58.947 181.365c4.048 11.187 16.132 17.473 27.732 14.135 12.1-3.483 19.475-16.334 15.614-28.217L165.964 15.838" fill="#dfa22f"/><path d="M74.626 45.516C70.734 33.542 57.873 26.989 45.9 30.879 33.924 34.77 27.37 47.631 31.263 59.606l58.948 181.366c4.047 11.186 16.132 17.473 27.732 14.132 12.099-3.481 19.474-16.332 15.613-28.217L74.626 45.516" fill="#3cb187"/><path d="M240.162 166.045c11.975-3.89 18.526-16.75 14.636-28.726-3.89-11.973-16.752-18.527-28.725-14.636L44.708 181.632c-11.187 4.046-17.473 16.13-14.135 27.73 3.483 12.099 16.334 19.475 28.217 15.614l181.372-58.93" fill="#ce1e5b"/><path d="M82.508 217.27l43.347-14.084-14.086-43.352-43.35 14.09 14.089 43.347" fill="#392538"/><path d="M173.847 187.591c16.388-5.323 31.62-10.273 43.348-14.084l-14.088-43.36-43.35 14.09 14.09 43.354" fill="#bb242a"/><path d="M210.484 74.706c11.974-3.89 18.527-16.751 14.637-28.727-3.89-11.973-16.752-18.526-28.727-14.636L15.028 90.293C3.842 94.337-2.445 106.422.896 118.022c3.481 12.098 16.332 19.474 28.217 15.613l181.371-58.93" fill="#72c5cd"/><path d="M52.822 125.933c11.805-3.836 27.025-8.782 43.354-14.086-5.323-16.39-10.273-31.622-14.084-43.352l-43.36 14.092 14.09 43.346" fill="#248c73"/><path d="M144.16 96.256l43.356-14.088a546179.21 546179.21.0 00-14.089-43.36L130.07 52.9l14.09 43.356" fill="#62803a"/></svg></a></li><li><a href=https://www.buymeacoffee.com/0x000216 target=_blank title=Minds><svg id="Capa_1" xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink" viewBox="0 0 512 512" style="enable-background:new 0 0 512 512"><g><g><path style="fill:#ffe1b2" d="M256 33.085C245.078 13.38 224.079.0 2e2.0c-23.781.0-45.57 13.293-56.594 34.184C115.711 41.602 96 66.977 96 96c0 .059.0.113.0.172-9.977 7.512-16 19.301-16 31.828.0 1.316.078 2.637.234 3.992C60.211 145.266 48 167.758 48 192c0 14.07 4.039 27.543 11.719 39.262C57.273 236.512 56 242.207 56 248c0 2.738.281 5.445.828 8.098C36.672 267.308 24 288.539 24 312c0 27.973 18.305 52.34 44.109 60.785C65.398 378.828 64 385.324 64 392c0 21.098 13.805 39.508 33.539 45.727C103.891 466.746 129.828 488 160 488c4.617.0 9.227-.512 13.766-1.527C181.992 502 198.141 512 216 512c16.687.0 31.396-8.567 40-21.523V33.085z"/></g><g><g><path style="fill:#ffb980" d="M264 256c-4.422.0-8-3.582-8-8 0-22.055-17.945-40-40-40-8.008.0-15.734 2.355-22.336 6.812-3.023 2.043-7.055 1.781-9.797-.652-3.156-2.809-8.477-6.16-15.867-6.16-4.422.0-8-3.582-8-8s3.578-8 8-8c7.711.0 15.234 2.293 21.719 6.539C197.773 194.246 206.758 192 216 192c30.875.0 56 25.121 56 56C272 252.418 268.422 256 264 256z"/></g></g><g><g><path style="fill:#ffb980" d="M120 120c18.977.0 36.875 7.312 50.414 20.594 3.141 3.09 8.203 3.047 11.312-.109 3.094-3.152 3.047-8.219-.109-11.312C165.07 112.941 143.187 104 120 104c-13.046.0-25.395 2.93-36.542 8.046C81.253 117.019 80 122.423 80 128c0 1.316.078 2.637.234 3.992-.094.062-.173.139-.267.202C91.423 124.501 105.193 120 120 120z"/></g></g><g><g><path style="fill:#ffb980" d="M216 360c0-4.418-3.578-8-8-8s-8 3.582-8 8c0 17.645-14.352 32-32 32-14.211.0-26.82-9.648-30.664-23.465-.703-2.512-2.578-4.523-5.039-5.395-2.453-.871-5.188-.492-7.305 1.02C114.094 371.906 101.305 376 88 376c-6.948.0-13.625-1.149-19.894-3.207-2.214 4.939-3.501 10.19-3.916 15.586C71.714 390.73 79.711 392 88 392c13.297.0 26.187-3.266 37.773-9.52C133.969 397.894 150.141 408 168 408c26.469.0 48-21.531 48-48z"/></g></g><g><path style="fill:#fdc88e" d="M488 312c0-23.461-12.672-44.691-32.828-55.902.547-2.652.828-5.359.828-8.098.0-5.793-1.273-11.488-3.719-16.738C459.961 219.543 464 206.07 464 192c0-24.242-12.211-46.734-32.234-60.008.156-1.355.234-2.676.234-3.992.0-12.527-6.023-24.316-16-31.828.0-.059.0-.113.0-.172.0-29.023-19.711-54.398-47.406-61.816C357.57 13.293 335.781.0 312 0c-24.08.0-45.078 13.38-56 33.085v457.391C264.604 503.433 279.313 512 296 512c17.859.0 34.008-10 42.234-25.527C342.773 487.488 347.383 488 352 488c30.172.0 56.109-21.254 62.461-50.273C434.195 431.508 448 413.097 448 392c0-6.676-1.398-13.172-4.109-19.215C469.695 364.34 488 339.973 488 312z"/></g><g><path style="fill:#f8ab6b" d="M272.008 151.199C272 151.465 272 151.734 272 152c0 26.469 21.531 48 48 48s48-21.531 48-48c0-4.418-3.578-8-8-8s-8 3.582-8 8c0 17.645-14.352 32-32 32s-32-14.355-32-32c0-2.184.219-4.359.656-6.465.492-2.395-.133-4.883-1.703-6.754-1.57-1.871-4.016-3.066-6.352-2.859-.453.012-.891.059-.602.078-13.234.0-24-10.766-24-24v31.813C260.673 147.348 266.061 149.988 272.008 151.199z"/></g><g><path style="fill:#f8ab6b" d="M296 328c9.242.0 18.219-2.246 26.281-6.539C328.765 325.707 336.289 328 344 328c4.422.0 8-3.582 8-8s-3.578-8-8-8c-7.391.0-12.711-3.352-15.867-6.16-2.742-2.434-6.766-2.695-9.797-.656C311.726 309.644 304 312 296 312c-22.055.0-40-17.945-40-40v39.116C266.174 321.517 280.337 328 296 328z"/></g><g><g><path style="fill:#f8ab6b" d="M431.765 131.992c.156-1.355.234-2.676.234-3.992.0-5.577-1.253-10.981-3.458-15.954C417.395 106.93 405.046 104 392 104c-4.422.0-8 3.582-8 8s3.578 8 8 8c14.807.0 28.577 4.501 40.032 12.194C431.939 132.131 431.859 132.054 431.765 131.992z"/></g></g><g><g><path style="fill:#f8ab6b" d="M447.81 388.38c-.415-5.396-1.702-10.647-3.916-15.586C437.624 374.85 430.948 376 424 376c-13.578.0-26.594-4.266-37.641-12.332-2.07-1.5-4.719-1.93-7.133-1.168-2.43.77-4.344 2.648-5.164 5.059C369.101 382.176 355.414 392 340 392c-4.422.0-8 3.582-8 8s3.578 8 8 8c18.875.0 35.961-10.191 45.094-26.156C396.976 388.512 410.258 392 424 392 432.288 392 440.285 390.73 447.81 388.38z"/></g></g></g><g/><g/><g/><g/><g/><g/><g/><g/><g/><g/><g/><g/><g/><g/><g/></svg></a></li><li><a href=https://www.buymeacoffee.com/0x000216 target=_blank title=Coffee><svg id="Capa_1" xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink" viewBox="0 0 340 340" style="enable-background:new 0 0 340 340"><g id="XMLID_18_"><polygon id="XMLID_138_" style="fill:#dedde0" points="76.429,290 80,340 170,340 170,290"/><polygon id="XMLID_169_" style="fill:#dedde0" points="170,80 61.429,80 65,130 170,130"/><polygon id="XMLID_197_" style="fill:#acabb1" points="170,290 170,340 260,340 263.571,290"/><polygon id="XMLID_221_" style="fill:#acabb1" points="170,80 170,130 275,130 278.571,80"/><path id="XMLID_222_" style="fill:#ffda44" d="M170 260c-22.091.0-40-22.386-40-50s17.909-50 40-50v-30H65 50l10 160h16.429H170V260z"/><path id="XMLID_33_" style="fill:#ff9811" d="M170 130v30c22.091.0 40 22.386 40 50s-17.909 50-40 50v30h93.571H280l10-160h-15H170z"/><path id="XMLID_223_" style="fill:#50412e" d="M210 210c0-27.614-17.909-50-40-50v1e2c22.091.0 40-22.386 40-50z"/><path id="XMLID_224_" style="fill:#786145" d="M130 210c0 27.614 17.909 50 40 50V160c-22.091.0-40 22.386-40 50z"/><polygon id="XMLID_225_" style="fill:#50412e" points="278.571,80 300,80 300,40 260,40 260,0 80,0 80,40 40,40 40,80 61.429,80 170,80"/></g><g/><g/><g/><g/><g/><g/><g/><g/><g/><g/><g/><g/><g/><g/><g/></svg></a></li></ol><ol class=menu id=main-menu><li><a href=/><svg xmlns="http://www.w3.org/2000/svg" class="icon icon-tabler icon-tabler-home" width="24" height="24" viewBox="0 0 24 24" stroke-width="2" stroke="currentcolor" fill="none" stroke-linecap="round" stroke-linejoin="round"><path stroke="none" d="M0 0h24v24H0z"/><polyline points="5 12 3 12 12 3 21 12 19 12"/><path d="M5 12v7a2 2 0 002 2h10a2 2 0 002-2v-7"/><path d="M9 21v-6a2 2 0 012-2h2a2 2 0 012 2v6"/></svg><span>Home</span></a></li><li><a href=/about-me/><svg xmlns="http://www.w3.org/2000/svg" class="icon icon-tabler icon-tabler-user" width="24" height="24" viewBox="0 0 24 24" stroke-width="2" stroke="currentcolor" fill="none" stroke-linecap="round" stroke-linejoin="round"><path stroke="none" d="M0 0h24v24H0z"/><circle cx="12" cy="7" r="4"/><path d="M6 21v-2a4 4 0 014-4h4a4 4 0 014 4v2"/></svg><span>About Me</span></a></li><div class=menu-bottom-section><li id=dark-mode-toggle><svg xmlns="http://www.w3.org/2000/svg" class="icon icon-tabler icon-tabler-toggle-left" width="24" height="24" viewBox="0 0 24 24" stroke-width="2" stroke="currentcolor" fill="none" stroke-linecap="round" stroke-linejoin="round"><path stroke="none" d="M0 0h24v24H0z"/><circle cx="8" cy="12" r="2"/><rect x="2" y="6" width="20" height="12" rx="6"/></svg><svg xmlns="http://www.w3.org/2000/svg" class="icon icon-tabler icon-tabler-toggle-right" width="24" height="24" viewBox="0 0 24 24" stroke-width="2" stroke="currentcolor" fill="none" stroke-linecap="round" stroke-linejoin="round"><path stroke="none" d="M0 0h24v24H0z"/><circle cx="16" cy="12" r="2"/><rect x="2" y="6" width="20" height="12" rx="6"/></svg><span>Dark Mode</span></li></div></ol></aside><main class="main full-width"><article class=main-article><header class=article-header><div class=article-details><div class=article-title-wrapper><h2 class=article-title><a href=/posts/2016-09-09-bodypix-person-segmentation-in-browser/>BodyPix: Person Segmentation in the Browser</a></h2></div><footer class=article-time><div><svg xmlns="http://www.w3.org/2000/svg" class="icon icon-tabler icon-tabler-calendar-time" width="56" height="56" viewBox="0 0 24 24" stroke-width="2" stroke="currentcolor" fill="none" stroke-linecap="round" stroke-linejoin="round"><path stroke="none" d="M0 0h24v24H0z"/><path d="M11.795 21H5a2 2 0 01-2-2V7a2 2 0 012-2h12a2 2 0 012 2v4"/><circle cx="18" cy="18" r="4"/><path d="M15 3v4"/><path d="M7 3v4"/><path d="M3 11h16"/><path d="M18 16.496V18l1 1"/></svg><time class=article-time--published>Oct 11, 2019</time></div><div><svg xmlns="http://www.w3.org/2000/svg" class="icon icon-tabler icon-tabler-clock" width="24" height="24" viewBox="0 0 24 24" stroke-width="2" stroke="currentcolor" fill="none" stroke-linecap="round" stroke-linejoin="round"><path stroke="none" d="M0 0h24v24H0z"/><circle cx="12" cy="12" r="9"/><polyline points="12 7 12 12 15 15"/></svg><time class=article-time--reading>21 minute read</time></div></footer></div></header><section class=article-content><h1 id=httpsgithubcomtensorflowtfjs-modelstreemasterbody-pixbodypix---person-segmentation-in-the-browserbodypix---person-segmentation-in-the-browser><a class=link href=https://github.com/tensorflow/tfjs-models/tree/master/body-pix#bodypix---person-segmentation-in-the-browser target=_blank rel=noopener></a>BodyPix - Person Segmentation in the Browser</h1><h2 id=httpsgithubcomtensorflowtfjs-modelstreemasterbody-pixnote-weve-just-released-version-20-with-multi-person-support-a-new-resnet-model-and-api-check-out-the-new-documentation-belownote-weve-just-released-version-20-with-multi-person-support-a-new-resnet-model-and-api-check-out-the-new-documentation-below><a class=link href=https://github.com/tensorflow/tfjs-models/tree/master/body-pix#note-weve-just-released-version-20-with-multi-person-support-a-new-resnet-model-and-api-check-out-the-new-documentation-below target=_blank rel=noopener></a>Note: We&rsquo;ve just released Version 2.0 with <strong>multi-person</strong> support, a <strong>new ResNet</strong> model and API. Check out the new documentation below.</h2><p>This package contains a standalone model called BodyPix, as well as some demos, for running real-time person and body part segmentation in the browser using TensorFlow.js.</p><p><a class=link href=https://storage.googleapis.com/tfjs-models/demos/body-pix/index.html target=_blank rel=noopener>Try the demo here!</a></p><p><a class=link href=https://github.com/tensorflow/tfjs-models/blob/master/body-pix/images/body-pix-2.0.gif target=_blank rel=noopener><img src=https://github.com/tensorflow/tfjs-models/raw/master/body-pix/images/body-pix-2.0.gif loading=lazy alt=BodyPix></a></p><p>This model can be used to segment an image into pixels that are and are not part of a person, and into pixels that belong to each of twenty-four body parts. It works for a single person, and its ideal use case is for when there is only one person centered in an input image or video. It can be combined with a person detector to segment multiple people in an image by first cropping boxes for each detected person then estimating segmentation in each of those crops, but that responsibility is currently outside of the scope of this model.</p><p>To keep track of issues we use the <a class=link href=https://github.com/tensorflow/tfjs target=_blank rel=noopener>tensorflow/tfjs</a> Github repo.</p><h2 id=httpsgithubcomtensorflowtfjs-modelstreemasterbody-pixcontributorscontributors><a class=link href=https://github.com/tensorflow/tfjs-models/tree/master/body-pix#contributors target=_blank rel=noopener></a>Contributors</h2><h3 id=httpsgithubcomtensorflowtfjs-modelstreemasterbody-pixmain-contributorsmain-contributors><a class=link href=https://github.com/tensorflow/tfjs-models/tree/master/body-pix#main-contributors target=_blank rel=noopener></a>Main Contributors</h3><h3 id=httpsgithubcomtensorflowtfjs-modelstreemasterbody-pixacknowledgementacknowledgement><a class=link href=https://github.com/tensorflow/tfjs-models/tree/master/body-pix#acknowledgement target=_blank rel=noopener></a>Acknowledgement</h3><h3 id=httpsgithubcomtensorflowtfjs-modelstreemasterbody-pixspecial-thanks-for-participants-in-the-demo-videospecial-thanks-for-participants-in-the-demo-video><a class=link href=https://github.com/tensorflow/tfjs-models/tree/master/body-pix#special-thanks-for-participants-in-the-demo-video target=_blank rel=noopener></a>Special thanks for participants in the demo video</h3><h2 id=httpsgithubcomtensorflowtfjs-modelstreemasterbody-pixtables-of-contentstables-of-contents><a class=link href=https://github.com/tensorflow/tfjs-models/tree/master/body-pix#tables-of-contents target=_blank rel=noopener></a>Tables of Contents</h2><h2 id=httpsgithubcomtensorflowtfjs-modelstreemasterbody-pixinstallationinstallation><a class=link href=https://github.com/tensorflow/tfjs-models/tree/master/body-pix#installation target=_blank rel=noopener></a>Installation</h2><p>You can use this as standalone es5 bundle like this:</p><div class=highlight><div class=chroma><table class=lntable><tr><td class=lntd><pre tabindex=0 class=chroma><code><span class=lnt>1
</span></code></pre></td><td class=lntd><pre tabindex=0 class=chroma><code class=language-fallback data-lang=fallback><span class=line><span class=cl> &lt;script src\=&#34;https://cdn.jsdelivr.net/npm/@tensorflow/tfjs@1.0.0&#34;\&gt;script&gt; &lt;script src\=&#34;https://cdn.jsdelivr.net/npm/@tensorflow-models/body-pix@2.0.0&#34;\&gt;script&gt;
</span></span></code></pre></td></tr></table></div></div><p>Or you can install it via npm for use in a TypeScript / ES6 project.</p><div class=highlight><div class=chroma><table class=lntable><tr><td class=lntd><pre tabindex=0 class=chroma><code><span class=lnt>1
</span></code></pre></td><td class=lntd><pre tabindex=0 class=chroma><code class=language-fallback data-lang=fallback><span class=line><span class=cl>npm install @tensorflow-models/body-pix
</span></span></code></pre></td></tr></table></div></div><h2 id=httpsgithubcomtensorflowtfjs-modelstreemasterbody-pixusageusage><a class=link href=https://github.com/tensorflow/tfjs-models/tree/master/body-pix#usage target=_blank rel=noopener></a>Usage</h2><p>Either a person or part of the body can be segmented in an image. Each methodology has similar input parameters with different outputs.</p><h3 id=httpsgithubcomtensorflowtfjs-modelstreemasterbody-pixloading-a-pre-trained-bodypix-modelloading-a-pre-trained-bodypix-model><a class=link href=https://github.com/tensorflow/tfjs-models/tree/master/body-pix#loading-a-pre-trained-bodypix-model target=_blank rel=noopener></a>Loading a pre-trained BodyPix Model</h3><p>In the first step of person segmentation and body part segmentation, an image is fed through a pre-trained model. BodyPix <strong>comes with a few different versions of the model,</strong> corresponding to variances of MobileNetV1 architecture and ResNet50 architecture. To get started, a model must be loaded from a checkpoint:</p><div class=highlight><div class=chroma><table class=lntable><tr><td class=lntd><pre tabindex=0 class=chroma><code><span class=lnt>1
</span></code></pre></td><td class=lntd><pre tabindex=0 class=chroma><code class=language-fallback data-lang=fallback><span class=line><span class=cl>const net \= await bodyPix.load();
</span></span></code></pre></td></tr></table></div></div><p>By default, <code>bodyPix.load()</code> loads a faster and smaller model that is based on MobileNetV1 architecture and has a lower accuracy. If you want to load the larger and more accurate model, specify the architecture explicitly in <code>bodyPix.load()</code> using a <code>ModelConfig</code> dictionary:</p><h4 id=httpsgithubcomtensorflowtfjs-modelstreemasterbody-pixmobilenet-smaller-faster-less-accuratemobilenet-smaller-faster-less-accurate><a class=link href=https://github.com/tensorflow/tfjs-models/tree/master/body-pix#mobilenet-smaller-faster-less-accurate target=_blank rel=noopener></a>MobileNet (smaller, faster, less accurate)</h4><div class=highlight><div class=chroma><table class=lntable><tr><td class=lntd><pre tabindex=0 class=chroma><code><span class=lnt>1
</span></code></pre></td><td class=lntd><pre tabindex=0 class=chroma><code class=language-fallback data-lang=fallback><span class=line><span class=cl>const net \= await bodyPix.load({ architecture: &#39;MobileNetV1&#39;, outputStride: 16, inputResolution: 513, multiplier: 0.75 });
</span></span></code></pre></td></tr></table></div></div><h4 id=httpsgithubcomtensorflowtfjs-modelstreemasterbody-pixresnet-larger-slower-more-accurate-newresnet-larger-slower-more-accurate-new><a class=link href=https://github.com/tensorflow/tfjs-models/tree/master/body-pix#resnet-larger-slower-more-accurate-new target=_blank rel=noopener></a>ResNet (larger, slower, more accurate) **new!**</h4><div class=highlight><div class=chroma><table class=lntable><tr><td class=lntd><pre tabindex=0 class=chroma><code><span class=lnt>1
</span></code></pre></td><td class=lntd><pre tabindex=0 class=chroma><code class=language-fallback data-lang=fallback><span class=line><span class=cl>const net \= await bodyPix.load({ architecture: &#39;ResNet50&#39;, outputStride: 32, inputResolution: 257, quantBytes: 2 });
</span></span></code></pre></td></tr></table></div></div><h4 id=httpsgithubcomtensorflowtfjs-modelstreemasterbody-pixconfig-params-in-bodypixloadconfig-params-in-bodypixload><a class=link href=https://github.com/tensorflow/tfjs-models/tree/master/body-pix#config-params-in-bodypixload target=_blank rel=noopener></a>Config params in bodyPix.load()</h4><ul><li><p><strong>architecture</strong> - Can be either <code>MobileNetV1</code> or <code>ResNet50</code>. It determines which BodyPix architecture to load.</p></li><li><p><strong>outputStride</strong> - Can be one of <code>8</code>, <code>16</code>, <code>32</code> (Stride <code>16</code>, <code>32</code> are supported for the ResNet architecture and stride <code>8</code>, <code>16</code>, <code>32</code> are supported for the MobileNetV1 architecture). It specifies the output stride of the BodyPix model. The smaller the value, the larger the output resolution, and more accurate the model at the cost of speed. Set this to a larger value to increase speed at the cost of accuracy.</p></li><li><p><strong>inputResolution</strong> - Can be one of <code>161</code>, <code>193</code>, <code>257</code>, <code>289</code>, <code>321</code>, <code>353</code>, <code>385</code>, <code>417</code>, <code>449</code>, <code>481</code>, <code>513</code>, and <code>801</code>. Defaults to <code>257.</code> It specifies the size the image is resized to before it is fed into the BodyPix model. The larger the value, the more accurate the model at the cost of speed. Set this to a smaller value to increase speed at the cost of accuracy.</p></li><li><p><strong>multiplier</strong> - Can be one of <code>1.0</code>, <code>0.75</code>, or <code>0.50</code> (The value is used <em>only</em> by the MobileNetV1 architecture and not by the ResNet architecture). It is the float multiplier for the depth (number of channels) for all convolution ops. The larger the value, the larger the size of the layers, and more accurate the model at the cost of speed. Set this to a smaller value to increase speed at the cost of accuracy.</p></li><li><p><strong>quantBytes</strong> - This argument controls the bytes used for weight quantization. The available options are:</p><ul><li><code>4</code>. 4 bytes per float (no quantization). Leads to highest accuracy and original model size.</li><li><code>2</code>. 2 bytes per float. Leads to slightly lower accuracy and 2x model size reduction.</li><li><code>1</code>. 1 byte per float. Leads to lower accuracy and 4x model size reduction.</li></ul><p>The following table contains the corresponding BodyPix 2.0 model checkpoint sizes (widthout gzip) when using different quantization bytes:</p><p>Architecture</p><p>quantBytes=4</p><p>quantBytes=2</p><p>quantBytes=1</p><p>ResNet50</p><p>~90MB</p><p>~45MB</p><p>~22MB</p><p>MobileNetV1 (1.00)</p><p>~13MB</p><p>~6MB</p><p>~3MB</p><p>MobileNetV1 (0.75)</p><p>~5MB</p><p>~2MB</p><p>~1MB</p><p>MobileNetV1 (0.50)</p><p>~2MB</p><p>~1MB</p><p>~0.6MB</p></li><li><p><strong>modelUrl</strong> - An optional string that specifies custom url of the model. This is useful for local development or countries that don&rsquo;t have access to the models hosted on GCP.</p></li></ul><p><strong>By default,</strong> BodyPix loads a MobileNetV1 architecture with a <strong><code>0.75</code></strong> multiplier. This is recommended for computers with <strong>mid-range/lower-end GPUs.</strong> A model with a <strong><code>0.50</code></strong> multiplier is recommended for <strong>mobile.</strong> The ResNet architecture is recommended for computers with <strong>even more powerful GPUs</strong>.</p><h3 id=httpsgithubcomtensorflowtfjs-modelstreemasterbody-pixperson-segmentationperson-segmentation><a class=link href=https://github.com/tensorflow/tfjs-models/tree/master/body-pix#person-segmentation target=_blank rel=noopener></a>Person segmentation</h3><p>Given an image with multiple people, multi-person segmentation model predicts segmentation for <em>each</em> person. It returns <em>an array</em> of <code>PersonSegmentation</code> and each corresponding to one person. Each element is a binary array for one person with 1 for the pixels that are part of the person, and 0 otherwise. The array size corresponds to the number of pixels in the image.</p><p><a class=link href=https://github.com/tensorflow/tfjs-models/blob/master/body-pix/images/two_people_segmentation.jpg target=_blank rel=noopener><img src=https://github.com/tensorflow/tfjs-models/raw/master/body-pix/images/two_people_segmentation.jpg loading=lazy alt="Multi-person Segmentation"></a></p><div class=highlight><div class=chroma><table class=lntable><tr><td class=lntd><pre tabindex=0 class=chroma><code><span class=lnt>1
</span></code></pre></td><td class=lntd><pre tabindex=0 class=chroma><code class=language-fallback data-lang=fallback><span class=line><span class=cl>const net \= await bodyPix.load(); const segmentation \= await net.estimateMultiPersonInstanceSegmentation(image, { flipHorizontal: false, segmentationThreshold: 0.7, maxDetections: 10, scoreThreshold: 0.2, nmsRadius: 20, minKeypointScore: 0.3, refineSteps: 10 });
</span></span></code></pre></td></tr></table></div></div><h4 id=httpsgithubcomtensorflowtfjs-modelstreemasterbody-pixparams-in-estimatemultipersoninstancesegmentationparams-in-estimatemultipersoninstancesegmentation><a class=link href=https://github.com/tensorflow/tfjs-models/tree/master/body-pix#params-in-estimatemultipersoninstancesegmentation target=_blank rel=noopener></a>Params in estimateMultiPersonInstanceSegmentation()</h4><ul><li><strong>image</strong> - ImageData|HTMLImageElement|HTMLCanvasElement|HTMLVideoElement The input image to feed through the network.</li><li><strong>config</strong> - an optional dictionary containing:<ul><li><strong>flipHorizontal</strong> - Defaults to false. If the segmentation & pose should be flipped/mirrored horizontally. This should be set to true for videos where the video is by default flipped horizontally (i.e. a webcam), and you want the segmentation & pose to be returned in the proper orientation.</li><li><strong>segmentationThreshold</strong> - Defaults to 0.7. Must be between 0 and 1. For each pixel, the model estimates a score between 0 and 1 that indicates how confident it is that part of a person is displayed in that pixel. This <em>segmentationThreshold</em> is used to convert these values to binary 0 or 1s by determining the minimum value a pixel&rsquo;s score must have to be considered part of a person. In essence, a higher value will create a tighter crop around a person but may result in some pixels being that are part of a person being excluded from the returned segmentation mask.</li><li><strong>maxDetections</strong> - Defaults to 10. Maximum number of returned instance detections per image.</li><li><strong>scoreThreshold</strong> - Only return instance detections that have root part score greater or equal to this value. Defaults to 0.5</li><li><strong>nmsRadius</strong> - Defaults to 20. Non-maximum suppression part distance in pixels. It needs to be strictly positive. Two parts suppress each other if they are less than <code>nmsRadius</code> pixels away.</li><li><strong>minKeypointScore</strong> - Default to 0.3. Keypoints above the score are used for matching and assigning segmentation mask to each person..</li><li><strong>refineSteps</strong> - Default to 10. The number of refinement steps used when assigning the instance segmentation. It needs to be strictly positive. The larger the higher the accuracy and slower the inference.</li></ul></li></ul><h4 id=httpsgithubcomtensorflowtfjs-modelstreemasterbody-pixreturnsreturns><a class=link href=https://github.com/tensorflow/tfjs-models/tree/master/body-pix#returns target=_blank rel=noopener></a>Returns</h4><p>It returns a <code>Promise</code> that resolves with <strong>an array</strong> of <code>PersonSegmentation</code>s. When there are multiple people in the image, each <code>PersonSegmentation</code> object in the array represents one person. More details about the <code>PersonSegmentation</code> object can be found in the documentation of the <code>estimateSinglePersonSegmentation</code> method.</p><div class=highlight><div class=chroma><table class=lntable><tr><td class=lntd><pre tabindex=0 class=chroma><code><span class=lnt>1
</span></code></pre></td><td class=lntd><pre tabindex=0 class=chroma><code class=language-fallback data-lang=fallback><span class=line><span class=cl>\[{ width: 640, height: 480, data: Uint8Array(307200) \[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1 â€¦\] }, ... // the data array for the 1st person containing 307200 values, one for each pixel of the 640x480 image. { width: 640, height: 480, data: Uint8Array(307200) \[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, â€¦\] }\] // the data array for the n-th person containing 307200 values, one for each pixel of the 640x480 image.
</span></span></code></pre></td></tr></table></div></div><h4 id=httpsgithubcomtensorflowtfjs-modelstreemasterbody-pixexample-usageexample-usage><a class=link href=https://github.com/tensorflow/tfjs-models/tree/master/body-pix#example-usage target=_blank rel=noopener></a>Example Usage</h4><h5 id=httpsgithubcomtensorflowtfjs-modelstreemasterbody-pixvia-script-tagvia-script-tag><a class=link href=https://github.com/tensorflow/tfjs-models/tree/master/body-pix#via-script-tag target=_blank rel=noopener></a>via Script Tag</h5><div class=highlight><div class=chroma><table class=lntable><tr><td class=lntd><pre tabindex=0 class=chroma><code><span class=lnt>1
</span></code></pre></td><td class=lntd><pre tabindex=0 class=chroma><code class=language-fallback data-lang=fallback><span class=line><span class=cl>&lt;html\&gt; &lt;head\&gt;  &lt;script src\=&#34;https://cdn.jsdelivr.net/npm/@tensorflow/tfjs@1.0.0&#34;\&gt;script&gt;  &lt;script src\=&#34;https://cdn.jsdelivr.net/npm/@tensorflow-models/body-pix@2.0.0&#34;\&gt;script&gt; head&gt; &lt;body\&gt; &lt;img id\=&#39;person&#39; src\=&#39;/images/person.jpg &#39;/&gt; body&gt;  &lt;script\&gt;  var imageElement \= document.getElementById(&#39;image&#39;);   bodyPix.load().then(function(net){  return net.estimateMultiPersonSegmentation(imageElement, {  flipHorizontal: false,  segmentationThreshold: 0.7,  maxDetections: 10,  scoreThreshold: 0.2,  nmsRadius: 20,  minKeypointScore: 0.3,  refineSteps: 10  });  }).then(function(allSegmentations){  console.log(allSegmentations);  })  script&gt; html&gt;
</span></span></code></pre></td></tr></table></div></div><h6 id=httpsgithubcomtensorflowtfjs-modelstreemasterbody-pixvia-npmvia-npm><a class=link href=https://github.com/tensorflow/tfjs-models/tree/master/body-pix#via-npm target=_blank rel=noopener></a>via NPM</h6><div class=highlight><div class=chroma><table class=lntable><tr><td class=lntd><pre tabindex=0 class=chroma><code><span class=lnt>1
</span></code></pre></td><td class=lntd><pre tabindex=0 class=chroma><code class=language-fallback data-lang=fallback><span class=line><span class=cl>import \* as bodyPix from &#39;@tensorflow-models/body-pix&#39;; const imageElement \= document.getElementById(&#39;image&#39;); // load the BodyPix model from a checkpoint const net \= await bodyPix.load(); const allSegmentations \= await net.estimateMultiPersonInstanceSegmentation(imageElement, { flipHorizontal: false, segmentationThreshold: 0.7, maxDetections: 10, scoreThreshold: 0.2, nmsRadius: 20, minKeypointScore: 0.3, refineSteps: 10 }); console.log(allSegmentations);
</span></span></code></pre></td></tr></table></div></div><h3 id=httpsgithubcomtensorflowtfjs-modelstreemasterbody-pixmulti-person-body-part-segmentationmulti-person-body-part-segmentation><a class=link href=https://github.com/tensorflow/tfjs-models/tree/master/body-pix#multi-person-body-part-segmentation target=_blank rel=noopener></a>Multi-person body part segmentation</h3><p>Given an image with multiple people. BodyPix&rsquo;s <code>estimateMultiPersonInstancePartSegmentation</code> method predicts the 24 body part segmentations for <em>each</em> person. It returns <em>an array</em> of <code>PartSegmentation</code>s, each corresponding to one of the people. The <code>PartSegmentation</code> object contains a width, height, <code>Pose</code> and an Int32 array with a part id from 0-24 for the pixels that are part of a corresponding body part, and -1 otherwise.</p><p><a class=link href=https://github.com/tensorflow/tfjs-models/blob/master/body-pix/images/two_people_parts.jpg target=_blank rel=noopener><img src=https://github.com/tensorflow/tfjs-models/raw/master/body-pix/images/two_people_parts.jpg loading=lazy alt="Multi-person Segmentation"></a></p><h4 id=httpsgithubcomtensorflowtfjs-modelstreemasterbody-pixthe-body-partsthe-body-parts><a class=link href=https://github.com/tensorflow/tfjs-models/tree/master/body-pix#the-body-parts target=_blank rel=noopener></a>The Body Parts</h4><p>As stated above, the result contains an array with ids for one of 24 body parts, or -1 if there is no body part:</p><p>Part Id</p><p>Part Name</p><p>Part Id</p><p>Part Name</p><p>0</p><p>left_face</p><p>12</p><p>torso_front</p><p>1</p><p>right_face</p><p>13</p><p>torso_back</p><p>2</p><p>left_upper_arm_front</p><p>14</p><p>left_upper_leg_front</p><p>3</p><p>left_upper_arm_back</p><p>15</p><p>left_upper_leg_back</p><p>4</p><p>right_upper_arm_front</p><p>16</p><p>right_upper_leg_front</p><p>5</p><p>right_upper_arm_back</p><p>17</p><p>right_upper_leg_back</p><p>6</p><p>left_lower_arm_front</p><p>18</p><p>left_lower_leg_front</p><p>7</p><p>left_lower_arm_back</p><p>19</p><p>left_lower_leg_back</p><p>8</p><p>right_lower_arm_front</p><p>20</p><p>right_lower_leg_front</p><p>9</p><p>right_lower_arm_back</p><p>21</p><p>right_lower_leg_back</p><p>10</p><p>left_hand</p><p>22</p><p>left_feet</p><p>11</p><p>right_hand</p><p>23</p><p>right_feet</p><p>(Note: Part Id value -1 represents the non-person background)</p><div class=highlight><div class=chroma><table class=lntable><tr><td class=lntd><pre tabindex=0 class=chroma><code><span class=lnt>1
</span></code></pre></td><td class=lntd><pre tabindex=0 class=chroma><code class=language-fallback data-lang=fallback><span class=line><span class=cl>const net \= await bodyPix.load(); const segmentation \= await net.estimateMultiPersonInstancePartSegmentation(image, { flipHorizontal: false, segmentationThreshold: 0.7, maxDetections: 10, scoreThreshold: 0.2, nmsRadius: 20, minKeypointScore: 0.3, refineSteps: 10 });
</span></span></code></pre></td></tr></table></div></div><h4 id=httpsgithubcomtensorflowtfjs-modelstreemasterbody-pixparams-in-estimatemultipersoninstancesegmentation-1params-in-estimatemultipersoninstancesegmentation><a class=link href=https://github.com/tensorflow/tfjs-models/tree/master/body-pix#params-in-estimatemultipersoninstancesegmentation-1 target=_blank rel=noopener></a>Params in estimateMultiPersonInstanceSegmentation</h4><ul><li><strong>image</strong> - ImageData|HTMLImageElement|HTMLCanvasElement|HTMLVideoElement The input image to feed through the network.</li><li><strong>inferenceConfig</strong> - an object containing:<ul><li><strong>flipHorizontal</strong> - Defaults to false. If the segmentation & pose should be flipped/mirrored horizontally. This should be set to true for videos where the video is by default flipped horizontally (i.e. a webcam), and you want the segmentation & pose to be returned in the proper orientation.</li><li><strong>segmentationThreshold</strong> - Must be between 0 and 1. For each pixel, the model estimates a score between 0 and 1 that indicates how confident it is that part of a person is displayed in that pixel. This <em>segmentationThreshold</em> is used to convert these values to binary 0 or 1s by determining the minimum value a pixel&rsquo;s score must have to be considered part of a person. In essence, a higher value will create a tighter crop around a person but may result in some pixels being that are part of a person being excluded from the returned segmentation mask.</li><li><strong>maxDetections</strong> - Maximum number of returned instance detections per image. Defaults to 10</li><li><strong>scoreThreshold</strong> - Only return instance detections that have root part score greater or equal to this value. Defaults to 0.5</li><li><strong>nmsRadius</strong> - Non-maximum suppression part distance in pixels. It needs to be strictly positive. Two parts suppress each other if they are less than <code>nmsRadius</code> pixels away. Defaults to 20.</li><li><strong>minKeypointScore</strong> - Default to 0.3. Keypoints above the score are used for matching and assigning segmentation mask to each person..</li><li><strong>refineSteps</strong> - The number of refinement steps used when assigning the instance segmentation. It needs to be strictly positive. The larger the higher the accuracy and slower the inference.</li></ul></li></ul><h4 id=httpsgithubcomtensorflowtfjs-modelstreemasterbody-pixreturns-1returns><a class=link href=https://github.com/tensorflow/tfjs-models/tree/master/body-pix#returns-1 target=_blank rel=noopener></a>Returns</h4><p>It returns a <code>Promise</code> that resolves with <strong>an array</strong> of <code>PartSegmentation</code>s. When there are multiple people in the image, each <code>PartSegmentation</code> object in the array represents one person. More details about the <code>PartSegmentation</code> object can be found in the documentation of the <code>estimateSinglePersonPartSegmentation</code> method.</p><div class=highlight><div class=chroma><table class=lntable><tr><td class=lntd><pre tabindex=0 class=chroma><code><span class=lnt>1
</span></code></pre></td><td class=lntd><pre tabindex=0 class=chroma><code class=language-fallback data-lang=fallback><span class=line><span class=cl>\[ { width: 680, height: 480, data: Int32Array(307200) \[\-1, \-1, \-1, \-1, \-1, \-1, \-1, \-1, \-1, \-1, \-1, \-1, \-1, \-1, \-1, \-1, \-1, \-1, \-1, \-1, 3, 3, 3, 3, \-1, \-1, \-1, \-1, \-1, \-1, \-1, \-1, \-1, \-1, \-1, \-1, \-1, \-1, \-1, 0, 0, 0, 0, 0, 1, 1, 2, \-1, \-1, \-1, \-1, \-1, \-1, \-1, \-1, \-1, \-1, \-1, \-1, \-1, \-1, \-1, \-1, \-1, \-1, 15, 15, 15, 15, 16, 16, \-1, \-1, \-1, \-1, \-1, \-1, \-1, \-1, \-1, \-1, \-1, \-1, \-1, \-1, \-1, \-1, \-1, \-1, 23, 23, 23, 22, 22, \-1, \-1, \-1, \-1,  â€¦\] }, { width: 680, height: 480, data: Int32Array(307200) \[\-1, \-1, \-1, \-1, \-1, \-1, \-1, \-1, \-1, \-1, \-1, \-1, \-1, \-1, \-1, \-1, \-1, \-1, \-1, \-1, 3, 3, 3, 3, \-1, \-1, \-1, \-1, \-1, \-1, \-1, \-1, \-1, \-1, \-1, \-1, \-1, \-1, \-1, 0, 0, 0, 0, 0, 1, 1, 2, \-1, \-1, \-1, \-1, \-1, \-1, \-1, \-1, \-1, \-1, \-1, \-1, \-1, \-1, \-1, \-1, \-1, \-1, 15, 15, 15, 15, 16, 16, \-1, \-1, \-1, \-1, \-1, \-1, \-1, \-1, \-1, \-1, \-1, \-1, \-1, \-1, \-1, \-1, \-1, \-1, 23, 23, 23, 22, 22, \-1, \-1, \-1, \-1,  â€¦\] } \] // the array contains 307200 values, one for each pixel of the 640x480 image that was passed to the function.
</span></span></code></pre></td></tr></table></div></div><h4 id=httpsgithubcomtensorflowtfjs-modelstreemasterbody-pixexample-usage-1example-usage><a class=link href=https://github.com/tensorflow/tfjs-models/tree/master/body-pix#example-usage-1 target=_blank rel=noopener></a>Example Usage</h4><h5 id=httpsgithubcomtensorflowtfjs-modelstreemasterbody-pixvia-script-tag-1via-script-tag><a class=link href=https://github.com/tensorflow/tfjs-models/tree/master/body-pix#via-script-tag-1 target=_blank rel=noopener></a>via Script Tag</h5><div class=highlight><div class=chroma><table class=lntable><tr><td class=lntd><pre tabindex=0 class=chroma><code><span class=lnt>1
</span></code></pre></td><td class=lntd><pre tabindex=0 class=chroma><code class=language-fallback data-lang=fallback><span class=line><span class=cl>&lt;html\&gt; &lt;head\&gt;  &lt;script src\=&#34;https://cdn.jsdelivr.net/npm/@tensorflow/tfjs@1.0.0&#34;\&gt;script&gt;  &lt;script src\=&#34;https://cdn.jsdelivr.net/npm/@tensorflow-models/body-pix@2.0.0&#34;\&gt;script&gt; head&gt; &lt;body\&gt; &lt;img id\=&#39;person&#39; src\=&#39;/images/person.jpg &#39;/&gt; body&gt;  &lt;script\&gt;  var imageElement \= document.getElementById(&#39;image&#39;);   bodyPix.load().then(function(net){  return net.estimateMultiPersonPartSegmentation(imageElement, {  flipHorizontal: false,  segmentationThreshold: 0.7,  maxDetections: 10,  scoreThreshold: 0.2,  nmsRadius: 20,  minKeypointScore: 0.3,  refineSteps: 10  });  }).then(function(multiPersonPartSegmentations){  console.log(multiPersonPartSegmentations);  })  script&gt; html&gt;
</span></span></code></pre></td></tr></table></div></div><h6 id=httpsgithubcomtensorflowtfjs-modelstreemasterbody-pixvia-npm-1via-npm><a class=link href=https://github.com/tensorflow/tfjs-models/tree/master/body-pix#via-npm-1 target=_blank rel=noopener></a>via NPM</h6><div class=highlight><div class=chroma><table class=lntable><tr><td class=lntd><pre tabindex=0 class=chroma><code><span class=lnt>1
</span></code></pre></td><td class=lntd><pre tabindex=0 class=chroma><code class=language-fallback data-lang=fallback><span class=line><span class=cl>import \* as bodyPix from &#39;@tensorflow-models/body-pix&#39;; const imageElement \= document.getElementById(&#39;image&#39;); // load the BodyPix model from a checkpoint const net \= await bodyPix.load(); const multiPersonPartSegmentations \= await net.estimateMultiPersonPartSegmentation(imageElement, { flipHorizontal: false, segmentationThreshold: 0.7, maxDetections: 10, scoreThreshold: 0.2, nmsRadius: 20, minKeypointScore: 0.3, refineSteps: 10 }); console.log(multiPersonPartSegmentations);
</span></span></code></pre></td></tr></table></div></div><p>which would produce the output:</p><div class=highlight><div class=chroma><table class=lntable><tr><td class=lntd><pre tabindex=0 class=chroma><code><span class=lnt>1
</span></code></pre></td><td class=lntd><pre tabindex=0 class=chroma><code class=language-fallback data-lang=fallback><span class=line><span class=cl>\[{ width: 640, height: 480, data: Uint8Array(307200) \[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, â€¦\] }, // the data array contains 307200 values, one for each pixel of the 640x480 image that was passed to the function. { width: 640, height: 480, data: Uint8Array(307200) \[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, â€¦\] }\] // the data array contains 307200 values, one for each pixel of the 640x480 image that was passed to the function.
</span></span></code></pre></td></tr></table></div></div><h3 id=httpsgithubcomtensorflowtfjs-modelstreemasterbody-pixoutput-visualization-utility-functionsoutput-visualization-utility-functions><a class=link href=https://github.com/tensorflow/tfjs-models/tree/master/body-pix#output-visualization-utility-functions target=_blank rel=noopener></a>Output Visualization Utility Functions</h3><p>BodyPix contains utility functions to help with drawing and compositing using the outputs. <strong>These API methods are experimental and subject to change.</strong></p><h4 id=httpsgithubcomtensorflowtfjs-modelstreemasterbody-pixtomaskimagedatatomaskimagedata><a class=link href=https://github.com/tensorflow/tfjs-models/tree/master/body-pix#tomaskimagedata target=_blank rel=noopener></a><code>toMaskImageData</code></h4><p>Given the output of person segmentation (or multi-person instance segmentation), generates a visualization of each pixel determined by the corresponding binary segmentation value at the pixel from the output. In other words, pixels where there is a person will be colored by the foreground color and where there is not a person will be colored by the background color. This can be used as a mask to crop a person or the background when compositing.</p><h5 id=httpsgithubcomtensorflowtfjs-modelstreemasterbody-pixinputsinputs><a class=link href=https://github.com/tensorflow/tfjs-models/tree/master/body-pix#inputs target=_blank rel=noopener></a>Inputs</h5><ul><li><p><strong>personSegmentation</strong> The output from <a class=link href=https://github.com/tensorflow/tfjs-models/tree/master/body-pix#Single-person-segmentation target=_blank rel=noopener>estimatePersonSegmentation</a> or <a class=link href=https://github.com/tensorflow/tfjs-models/tree/master/body-pix#Multi-person-segmentation target=_blank rel=noopener>estimateMultiPersonSegmentation</a>. The former is a PersonSegmentation object and later is an <em>array</em> of PersonSegmentation object.</p></li><li><p><strong>foreground</strong> The foreground color (r,g,b,a) for visualizing pixels that belong to people.</p></li><li><p><strong>background</strong> The background color (r,g,b,a) for visualizing pixels that don&rsquo;t belong to people.</p></li><li><p><strong>drawContour</strong> Whether to draw the contour around each person&rsquo;s segmentation mask.</p></li></ul><h5 id=httpsgithubcomtensorflowtfjs-modelstreemasterbody-pixreturns-2returns><a class=link href=https://github.com/tensorflow/tfjs-models/tree/master/body-pix#returns-2 target=_blank rel=noopener></a>Returns</h5><p>An <a class=link href=https://developer.mozilla.org/en-US/docs/Web/API/ImageData target=_blank rel=noopener>ImageData</a> with the same width and height of the personSegmentation, with color and opacity at each pixel determined by the corresponding binary segmentation value at the pixel from the output.</p><p><a class=link href=https://github.com/tensorflow/tfjs-models/blob/master/body-pix/images/toMultiPersonMaskImageData.jpg target=_blank rel=noopener><img src=https://github.com/tensorflow/tfjs-models/raw/master/body-pix/images/toMultiPersonMaskImageData.jpg loading=lazy alt=MaskImageData></a></p><p><em>With the output from <code>estimateMultiPersonSegmentation</code> on the first image above, <code>toMultiPersonMaskImageData</code> will produce an <a class=link href=https://developer.mozilla.org/en-US/docs/Web/API/ImageData target=_blank rel=noopener>ImageData</a> that either looks like the second image above if setting <code>foregroundColor</code> to {r: 0, g: 0, b: 0, a: 0} and <code>backgroundColor</code> to {r: 0, g: 0, b: 0, a: 255} (by default), or the third image if if setting <code>foregroundColor</code> to {r: 0, g: 0, b: 0, a: 255} and <code>backgroundColor</code> to {r: 0, g: 0, b: 0, a: 0}. This can be used to mask either the person or the background using the method <code>drawMask</code>.</em></p><h4 id=httpsgithubcomtensorflowtfjs-modelstreemasterbody-pixtocoloredpartimagedatatocoloredpartimagedata><a class=link href=https://github.com/tensorflow/tfjs-models/tree/master/body-pix#tocoloredpartimagedata target=_blank rel=noopener></a><code>toColoredPartImageData</code></h4><p>Given the output from person body part segmentation (or multi-person instance body part segmentation) and an array of colors indexed by part id, generates an image with the corresponding color for each part at each pixel, and white pixels where there is no part.</p><h5 id=httpsgithubcomtensorflowtfjs-modelstreemasterbody-pixinputs-1inputs><a class=link href=https://github.com/tensorflow/tfjs-models/tree/master/body-pix#inputs-1 target=_blank rel=noopener></a>Inputs</h5><h5 id=httpsgithubcomtensorflowtfjs-modelstreemasterbody-pixreturns-3returns><a class=link href=https://github.com/tensorflow/tfjs-models/tree/master/body-pix#returns-3 target=_blank rel=noopener></a>Returns</h5><p>An <a class=link href=https://developer.mozilla.org/en-US/docs/Web/API/ImageData target=_blank rel=noopener>ImageData</a> with the same width and height of the estimated person part segmentation, with the corresponding color for each part at each pixel, and black pixels where there is no part.</p><h5 id=httpsgithubcomtensorflowtfjs-modelstreemasterbody-pixexample-usage-2example-usage><a class=link href=https://github.com/tensorflow/tfjs-models/tree/master/body-pix#example-usage-2 target=_blank rel=noopener></a>Example usage</h5><div class=highlight><div class=chroma><table class=lntable><tr><td class=lntd><pre tabindex=0 class=chroma><code><span class=lnt>1
</span></code></pre></td><td class=lntd><pre tabindex=0 class=chroma><code class=language-fallback data-lang=fallback><span class=line><span class=cl>const imageElement \= document.getElementById(&#39;person&#39;); const net \= await bodyPix.load(); const partSegmentation \= await net.estimateMultiPersonPartSegmentation(imageElement); // The rainbow colormap const rainbow \= \[ \[110, 64, 170\], \[143, 61, 178\], \[178, 60, 178\], \[210, 62, 167\], \[238, 67, 149\], \[255, 78, 125\], \[255, 94, 99\], \[255, 115, 75\], \[255, 140, 56\], \[239, 167, 47\], \[217, 194, 49\], \[194, 219, 64\], \[175, 240, 91\], \[135, 245, 87\], \[96, 247, 96\], \[64, 243, 115\], \[40, 234, 141\], \[28, 219, 169\], \[26, 199, 194\], \[33, 176, 213\], \[47, 150, 224\], \[65, 125, 224\], \[84, 101, 214\], \[99, 81, 195\] \]; // the colored part image is an rgb image with a corresponding color from thee rainbow colors for each part at each pixel, and black pixels where there is no part. const coloredPartImage \= bodyPix.toColoredPartImageData(partSegmentation, rainbow); const opacity \= 0.7; const flipHorizontal \= true; const maskBlurAmount \= 0; const canvas \= document.getElementById(&#39;canvas&#39;); // draw the colored part image on top of the original image onto a canvas. The colored part image will be drawn semi-transparent, with an opacity of 0.7, allowing for the original image to be visible under. bodyPix.drawMask( canvas, imageElement, coloredPartImageData, opacity, maskBlurAmount, flipHorizontal);
</span></span></code></pre></td></tr></table></div></div><p><a class=link href=https://github.com/tensorflow/tfjs-models/blob/master/body-pix/images/toMultiPersonColoredPartImage.jpg target=_blank rel=noopener><img src=https://github.com/tensorflow/tfjs-models/raw/master/body-pix/images/toMultiPersonColoredPartImage.jpg loading=lazy alt=toColoredPartImageData></a></p><p><em>With the output from <code>estimateMultiPersonPartSegmentation</code> on the first image above, a &lsquo;spectral&rsquo; or &lsquo;rainbow&rsquo; color scale in <code>toColoredPartImageData</code> will produce an <code>ImageData</code> that looks like the second image or the third image above.</em></p><h4 id=httpsgithubcomtensorflowtfjs-modelstreemasterbody-pixdrawmaskdrawmask><a class=link href=https://github.com/tensorflow/tfjs-models/tree/master/body-pix#drawmask target=_blank rel=noopener></a><code>drawMask</code></h4><p>Draws an image onto a canvas and draws an <code>ImageData</code> containing a mask on top of it with a specified opacity; The <code>ImageData</code> is typically generated using <code>toMaskImageData</code>, <code>toMultiPersonMaskImageData</code>, <code>toColoredPartImageData</code> or <code>toMultiPersonColoredPartImageData</code>.</p><h5 id=httpsgithubcomtensorflowtfjs-modelstreemasterbody-pixinputs-2inputs><a class=link href=https://github.com/tensorflow/tfjs-models/tree/master/body-pix#inputs-2 target=_blank rel=noopener></a>Inputs</h5><ul><li><strong>canvas</strong> The canvas to be drawn onto.</li><li><strong>image</strong> The original image to apply the mask to.</li><li><strong>maskImage</strong> An ImageData containing the mask. Ideally this should be generated by <code>toMaskImageData</code> or <code>toColoredPartImageData.</code></li><li><strong>maskOpacity</strong> The opacity when drawing the mask on top of the image. Defaults to 0.7. Should be a float between 0 and 1.</li><li><strong>maskBlurAmount</strong> How many pixels to blur the mask by. Defaults to 0. Should be an integer between 0 and 20.</li><li><strong>flipHorizontal</strong> If the result should be flipped horizontally. Defaults to false.</li></ul><h5 id=httpsgithubcomtensorflowtfjs-modelstreemasterbody-pixexample-usage-3example-usage><a class=link href=https://github.com/tensorflow/tfjs-models/tree/master/body-pix#example-usage-3 target=_blank rel=noopener></a>Example usage</h5><div class=highlight><div class=chroma><table class=lntable><tr><td class=lntd><pre tabindex=0 class=chroma><code><span class=lnt>1
</span></code></pre></td><td class=lntd><pre tabindex=0 class=chroma><code class=language-fallback data-lang=fallback><span class=line><span class=cl>const imageElement \= document.getElementById(&#39;image&#39;); const net \= await bodyPix.load(); const segmentation \= await net.estimateSinglePersonSegmentation(imageElement); const maskBackground \= true; // Convert the personSegmentation into a mask to darken the background. const foregroundColor \= {r: 0, g: 0, b: 0, a: 0}; const backgroundColor \= {r: 0, g: 0, b: 0, a: 255}; const backgroundDarkeningMask \= bodyPix.toMaskImageData(personSegmentation, foregroundColor, backgroundColor); const opacity \= 0.7; const maskBlurAmount \= 3; const flipHorizontal \= true; const canvas \= document.getElementById(&#39;canvas&#39;); // draw the mask onto the image on a canvas. With opacity set to 0.7 and maskBlurAmount set to 3, this will darken the background and blur the darkened background&#39;s edge. bodyPix.drawMask( canvas, imageElement, backgroundDarkeningMask, opacity, maskBlurAmount, flipHorizontal);
</span></span></code></pre></td></tr></table></div></div><p><a class=link href=https://github.com/tensorflow/tfjs-models/blob/master/body-pix/images/drawMask.jpg target=_blank rel=noopener><img src=https://github.com/tensorflow/tfjs-models/raw/master/body-pix/images/drawMask.jpg loading=lazy alt=drawMask></a></p><p><em>The above shows drawing a mask generated by <code>toMaskImageData</code> on top of an image and canvas using <code>toMask</code>. In this case, <code>segmentationThreshold</code> was set to a lower value of 0.25, making the mask include more pixels. The top two images show the mask drawn on top of the image, and the second two images show the mask blurred by setting <code>maskBlurAmount</code> to 9 before being drawn onto the image, resulting in a smoother transition between the person and the masked background.</em></p><h4 id=httpsgithubcomtensorflowtfjs-modelstreemasterbody-pixdrawpixelatedmaskdrawpixelatedmask><a class=link href=https://github.com/tensorflow/tfjs-models/tree/master/body-pix#drawpixelatedmask target=_blank rel=noopener></a><code>drawPixelatedMask</code></h4><p>Draws an image onto a canvas and draws an <code>ImageData</code> containing a mask on top of it with a specified opacity; The <code>ImageData</code> is typically generated using <code>toColoredPartImageData</code>. Different from <code>drawMask</code>, this rendering function applies the pixelation effect to the BodyPix&rsquo;s body part segmentation prediction. This allows a user to display low resolution body part segmentation and thus offers an aesthetic interpretation of the body part segmentation prediction.</p><h5 id=httpsgithubcomtensorflowtfjs-modelstreemasterbody-pixinputs-3inputs><a class=link href=https://github.com/tensorflow/tfjs-models/tree/master/body-pix#inputs-3 target=_blank rel=noopener></a>Inputs</h5><ul><li><strong>canvas</strong> The canvas to be drawn onto.</li><li><strong>image</strong> The original image to apply the mask to.</li><li><strong>maskImage</strong> An ImageData containing the mask. Ideally this should be generated by <code>toColoredPartImageData.</code></li><li><strong>maskOpacity</strong> The opacity when drawing the mask on top of the image. Defaults to 0.7. Should be a float between 0 and 1.</li><li><strong>maskBlurAmount</strong> How many pixels to blur the mask by. Defaults to 0. Should be an integer between 0 and 20.</li><li><strong>flipHorizontal</strong> If the result should be flipped horizontally. Defaults to false.</li><li><strong>pixelCellWidth</strong> The width of each pixel cell. Default to 10 px.</li></ul><h5 id=httpsgithubcomtensorflowtfjs-modelstreemasterbody-pixexample-usage-4example-usage><a class=link href=https://github.com/tensorflow/tfjs-models/tree/master/body-pix#example-usage-4 target=_blank rel=noopener></a>Example usage</h5><div class=highlight><div class=chroma><table class=lntable><tr><td class=lntd><pre tabindex=0 class=chroma><code><span class=lnt>1
</span></code></pre></td><td class=lntd><pre tabindex=0 class=chroma><code class=language-fallback data-lang=fallback><span class=line><span class=cl>const imageElement \= document.getElementById(&#39;person&#39;); const net \= await bodyPix.load(); const partSegmentation \= await net.estimateSinglePersonPartSegmentation(imageElement); const rainbow \= \[ \[110, 64, 170\], \[143, 61, 178\], \[178, 60, 178\], \[210, 62, 167\], \[238, 67, 149\], \[255, 78, 125\], \[255, 94, 99\], \[255, 115, 75\], \[255, 140, 56\], \[239, 167, 47\], \[217, 194, 49\], \[194, 219, 64\], \[175, 240, 91\], \[135, 245, 87\], \[96, 247, 96\], \[64, 243, 115\], \[40, 234, 141\], \[28, 219, 169\], \[26, 199, 194\], \[33, 176, 213\], \[47, 150, 224\], \[65, 125, 224\], \[84, 101, 214\], \[99, 81, 195\] \]; // the colored part image is an rgb image with a corresponding color from thee rainbow colors for each part at each pixel, and white pixels where there is no part. const coloredPartImage \= bodyPix.toColoredPartImageData(partSegmentation, rainbow); const opacity \= 0.7; const flipHorizontal \= true; const maskBlurAmount \= 0; const pixelCellWidth \= 10.0; const canvas \= document.getElementById(&#39;canvas&#39;); // draw the pixelated colored part image on top of the original image onto a canvas. Each pixel cell&#39;s width will be set to 10 px. The pixelated colored part image will be drawn semi-transparent, with an opacity of 0.7, allowing for the original image to be visible under. bodyPix.drawPixelatedMask( canvas, imageElement, coloredPartImageData, opacity, maskBlurAmount, flipHorizontal, pixelCellWidth);
</span></span></code></pre></td></tr></table></div></div><p><a class=link href=https://github.com/tensorflow/tfjs-models/blob/master/body-pix/images/drawPixelatedMask.png target=_blank rel=noopener><img src=https://github.com/tensorflow/tfjs-models/raw/master/body-pix/images/drawPixelatedMask.png loading=lazy alt=drawPixelatedMask></a></p><p><em>The pixelation effect is applied to part image using <code>drawPixelatedMask</code>; the result is shown in the image above.</em></p><h4 id=httpsgithubcomtensorflowtfjs-modelstreemasterbody-pixdrawbokeheffectdrawbokeheffect><a class=link href=https://github.com/tensorflow/tfjs-models/tree/master/body-pix#drawbokeheffect target=_blank rel=noopener></a><code>drawBokehEffect</code></h4><p>Given a personSegmentation and an image, draws the image with its background blurred onto a canvas.</p><p>An example of applying a <a class=link href=https://www.nikonusa.com/en/learn-and-explore/a/tips-and-techniques/bokeh-for-beginners.html target=_blank rel=noopener>bokeh effect</a> can be seen in this <a class=link href=https://storage.googleapis.com/tfjs-models/demos/body-pix/index.html target=_blank rel=noopener>demo</a>:</p><p><a class=link href=https://github.com/tensorflow/tfjs-models/blob/master/body-pix/images/bokeh.gif target=_blank rel=noopener><img src=https://github.com/tensorflow/tfjs-models/raw/master/body-pix/images/bokeh.gif loading=lazy alt=Bokeh></a></p><h5 id=httpsgithubcomtensorflowtfjs-modelstreemasterbody-pixinputs-4inputs><a class=link href=https://github.com/tensorflow/tfjs-models/tree/master/body-pix#inputs-4 target=_blank rel=noopener></a>Inputs</h5><ul><li><strong>canvas</strong> The canvas to draw the background-blurred image onto.</li><li><strong>image</strong> The image to blur the background of and draw.</li><li><strong>personSegmentation</strong> A personSegmentation object, containing a binary array with 1 for the pixels that are part of the person, and 0 otherwise. Must have the same dimensions as the image.</li><li><strong>backgroundBlurAmount</strong> How many pixels in the background blend into each other. Defaults to 3. Should be an integer between 1 and 20.</li><li><strong>edgeBlurAmount</strong> How many pixels to blur on the edge between the person and the background by. Defaults to 3. Should be an integer between 0 and 20.</li><li><strong>flipHorizontal</strong> If the output should be flipped horizontally. Defaults to false.</li></ul><h5 id=httpsgithubcomtensorflowtfjs-modelstreemasterbody-pixexample-usage-5example-usage><a class=link href=https://github.com/tensorflow/tfjs-models/tree/master/body-pix#example-usage-5 target=_blank rel=noopener></a>Example Usage</h5><div class=highlight><div class=chroma><table class=lntable><tr><td class=lntd><pre tabindex=0 class=chroma><code><span class=lnt>1
</span></code></pre></td><td class=lntd><pre tabindex=0 class=chroma><code class=language-fallback data-lang=fallback><span class=line><span class=cl>const imageElement \= document.getElementById(&#39;image&#39;); const net \= await bodyPix.load(); const personSegmentation \= await net.estimateSinglePersonSegmentation(imageElement); const backgroundBlurAmount \= 3; const edgeBlurAmount \= 3; const flipHorizontal \= true; const canvas \= document.getElementById(&#39;canvas&#39;); // draw the image with the background blurred onto the canvas. The edge between the person and blurred background is blurred by 3 pixels. bodyPix.drawBokehEffect( canvas, imageElement, personSegmentation, backgroundBlurAmount, edgeBlurAmount, flipHorizontal);
</span></span></code></pre></td></tr></table></div></div><p><a class=link href=https://github.com/tensorflow/tfjs-models/blob/master/body-pix/images/bokehimage.png target=_blank rel=noopener><img src=https://github.com/tensorflow/tfjs-models/raw/master/body-pix/images/bokehimage.png loading=lazy alt=bokeh></a></p><p><em>The above shows the process of applying a &lsquo;bokeh&rsquo; effect to an image (the left-most one) with <code>drawBokehEffect</code>. An <strong>inverted</strong> mask is generated from a <code>personSegmentation</code>. The original image is then drawn onto the canvas, and using the <a class=link href=https://developer.mozilla.org/en-US/docs/Web/API/CanvasRenderingContext2D/globalCompositeOperation target=_blank rel=noopener>canvas compositing</a> operation <code>destination-over</code> the mask is drawn onto the canvas, causing the background to be removed. The original image is blurred and drawn onto the canvas where it doesn&rsquo;t overlap with the existing image using the compositing operation <code>destination-over</code>. The result is seen in the right-most image.</em></p><h4 id=httpsgithubcomtensorflowtfjs-modelstreemasterbody-pixblurbodypartblurbodypart><a class=link href=https://github.com/tensorflow/tfjs-models/tree/master/body-pix#blurbodypart target=_blank rel=noopener></a><code>blurBodyPart</code></h4><p>Given a PartSegmentation (or an array of PartSegmentation) and an image, blurs some person body parts (e.g. left face and right face).</p><p>An example of applying a body part blur on <code>left_face</code> and <code>right_face</code> body parts (other body parts can be specified):</p><p><a class=link href=https://github.com/tensorflow/tfjs-models/blob/master/body-pix/images/three_people_faceblur.jpg target=_blank rel=noopener><img src=https://github.com/tensorflow/tfjs-models/raw/master/body-pix/images/three_people_faceblur.jpg loading=lazy alt=three_people_faceblur></a></p><h5 id=httpsgithubcomtensorflowtfjs-modelstreemasterbody-pixinputs-5inputs><a class=link href=https://github.com/tensorflow/tfjs-models/tree/master/body-pix#inputs-5 target=_blank rel=noopener></a>Inputs</h5><ul><li><strong>canvas</strong> The canvas to draw the body-part blurred image onto.</li><li><strong>image</strong> The image with people to blur the body-part and draw.</li><li><strong>partSegmentation</strong> A PartSegmentation object or an array of PartSegmentation object. Must have the same dimensions as the image.</li><li><strong>bodyPartIdsToBlur</strong> Default to [0, 1] (left-face and right-face). An array of body part ids to blur. Each must be one of the 24 body part ids.</li><li><strong>backgroundBlurAmount</strong> How many pixels in the background blend into each other. Defaults to 3. Should be an integer between 1 and 20.</li><li><strong>edgeBlurAmount</strong> How many pixels to blur on the edge between the person and the background by. Defaults to 3. Should be an integer between 0 and 20.</li><li><strong>flipHorizontal</strong> If the output should be flipped horizontally. Defaults to false.</li></ul><h5 id=httpsgithubcomtensorflowtfjs-modelstreemasterbody-pixexample-usage-6example-usage><a class=link href=https://github.com/tensorflow/tfjs-models/tree/master/body-pix#example-usage-6 target=_blank rel=noopener></a>Example Usage</h5><div class=highlight><div class=chroma><table class=lntable><tr><td class=lntd><pre tabindex=0 class=chroma><code><span class=lnt>1
</span></code></pre></td><td class=lntd><pre tabindex=0 class=chroma><code class=language-fallback data-lang=fallback><span class=line><span class=cl>const imageElement \= document.getElementById(&#39;image&#39;); const net \= await bodyPix.load(); const partSegmentation \= await net.estimateMultiPersonInstancePartSegmentation(imageElement); const backgroundBlurAmount \= 3; const edgeBlurAmount \= 3; const flipHorizontal \= true; const faceBodyPartIdsToBlur \= \[0, 1\]; const canvas \= document.getElementById(&#39;canvas&#39;); bodyPix.blurBodyPart( canvas, imageElement, partSegmentation, faceBodyPartIdsToBlur, backgroundBlurAmount, edgeBlurAmount, flipHorizontal);
</span></span></code></pre></td></tr></table></div></div><h2 id=httpsgithubcomtensorflowtfjs-modelstreemasterbody-pixdeveloping-the-demosdeveloping-the-demos><a class=link href=https://github.com/tensorflow/tfjs-models/tree/master/body-pix#developing-the-demos target=_blank rel=noopener></a>Developing the Demos</h2><p>Details for how to run the demos are included in the <code>demos/</code> folder.</p><p>from Hacker News <a class=link href=https://github.com/tensorflow/tfjs-models/tree/master/body-pix target=_blank rel=noopener>https://github.com/tensorflow/tfjs-models/tree/master/body-pix</a></p></section><footer class=article-footer><section class=article-copyright><svg xmlns="http://www.w3.org/2000/svg" class="icon icon-tabler icon-tabler-copyright" width="24" height="24" viewBox="0 0 24 24" stroke-width="2" stroke="currentcolor" fill="none" stroke-linecap="round" stroke-linejoin="round"><path stroke="none" d="M0 0h24v24H0z"/><circle cx="12" cy="12" r="9"/><path d="M14.5 9a3.5 4 0 100 6"/></svg><span>Licensed under CC BY-NC-SA 4.0</span></section></footer></article><footer class=site-footer><section class=copyright>&copy;
2020 -
2022 0x000216</section><section class=powerby>Built with <a href=https://gohugo.io/ target=_blank rel=noopener>Hugo</a><br>Theme <b><a href=https://github.com/CaiJimmy/hugo-theme-stack target=_blank rel=noopener data-version=3.11.0>Stack</a></b> designed by <a href=https://jimmycai.com target=_blank rel=noopener>Jimmy</a></section><script data-name=BMC-Widget src=https://cdn.buymeacoffee.com/widget/1.0.0/prod/widget.prod.min.js data-id=0x000216 data-description=coffee! data-message=coffee! data-color=#FF813F data-position=right data-x_margin=28 data-y_margin=18></script></footer><div class=pswp tabindex=-1 role=dialog aria-hidden=true><div class=pswp__bg></div><div class=pswp__scroll-wrap><div class=pswp__container><div class=pswp__item></div><div class=pswp__item></div><div class=pswp__item></div></div><div class="pswp__ui pswp__ui--hidden"><div class=pswp__top-bar><div class=pswp__counter></div><button class="pswp__button pswp__button--close" title="Close (Esc)"></button>
<button class="pswp__button pswp__button--share" title=Share></button>
<button class="pswp__button pswp__button--fs" title="Toggle fullscreen"></button>
<button class="pswp__button pswp__button--zoom" title="Zoom in/out"></button><div class=pswp__preloader><div class=pswp__preloader__icn><div class=pswp__preloader__cut><div class=pswp__preloader__donut></div></div></div></div></div><div class="pswp__share-modal pswp__share-modal--hidden pswp__single-tap"><div class=pswp__share-tooltip></div></div><button class="pswp__button pswp__button--arrow--left" title="Previous (arrow left)"></button>
<button class="pswp__button pswp__button--arrow--right" title="Next (arrow right)"></button><div class=pswp__caption><div class=pswp__caption__center></div></div></div></div></div><script src=https://cdn.jsdelivr.net/npm/photoswipe@4.1.3/dist/photoswipe.min.js integrity="sha256-ePwmChbbvXbsO02lbM3HoHbSHTHFAeChekF1xKJdleo=" crossorigin=anonymous defer></script><script src=https://cdn.jsdelivr.net/npm/photoswipe@4.1.3/dist/photoswipe-ui-default.min.js integrity="sha256-UKkzOn/w1mBxRmLLGrSeyB4e1xbrp4xylgAWb3M42pU=" crossorigin=anonymous defer></script><link rel=stylesheet href=https://cdn.jsdelivr.net/npm/photoswipe@4.1.3/dist/default-skin/default-skin.css integrity="sha256-c0uckgykQ9v5k+IqViZOZKc47Jn7KQil4/MP3ySA3F8=" crossorigin=anonymous><link rel=stylesheet href=https://cdn.jsdelivr.net/npm/photoswipe@4.1.3/dist/photoswipe.css integrity="sha256-SBLU4vv6CA6lHsZ1XyTdhyjJxCjPif/TRkjnsyGAGnE=" crossorigin=anonymous></main><aside class="sidebar right-sidebar sticky"><section class="widget archives"><div class=widget-icon><svg xmlns="http://www.w3.org/2000/svg" class="icon icon-tabler icon-tabler-hash" width="24" height="24" viewBox="0 0 24 24" stroke-width="2" stroke="currentcolor" fill="none" stroke-linecap="round" stroke-linejoin="round"><path stroke="none" d="M0 0h24v24H0z"/><line x1="5" y1="9" x2="19" y2="9"/><line x1="5" y1="15" x2="19" y2="15"/><line x1="11" y1="4" x2="7" y2="20"/><line x1="17" y1="4" x2="13" y2="20"/></svg></div><h2 class="widget-title section-title">Table of contents</h2><div class=widget--toc><nav id=TableOfContents><ol><li><a href=#httpsgithubcomtensorflowtfjs-modelstreemasterbody-pixnote-weve-just-released-version-20-with-multi-person-support-a-new-resnet-model-and-api-check-out-the-new-documentation-belownote-weve-just-released-version-20-with-multi-person-support-a-new-resnet-model-and-api-check-out-the-new-documentation-below><a href=https://github.com/tensorflow/tfjs-models/tree/master/body-pix#note-weve-just-released-version-20-with-multi-person-support-a-new-resnet-model-and-api-check-out-the-new-documentation-below></a>Note: We&rsquo;ve just released Version 2.0 with <strong>multi-person</strong> support, a <strong>new ResNet</strong> model and API. Check out the new documentation below.</a></li><li><a href=#httpsgithubcomtensorflowtfjs-modelstreemasterbody-pixcontributorscontributors><a href=https://github.com/tensorflow/tfjs-models/tree/master/body-pix#contributors></a>Contributors</a><ol><li><a href=#httpsgithubcomtensorflowtfjs-modelstreemasterbody-pixmain-contributorsmain-contributors><a href=https://github.com/tensorflow/tfjs-models/tree/master/body-pix#main-contributors></a>Main Contributors</a></li><li><a href=#httpsgithubcomtensorflowtfjs-modelstreemasterbody-pixacknowledgementacknowledgement><a href=https://github.com/tensorflow/tfjs-models/tree/master/body-pix#acknowledgement></a>Acknowledgement</a></li><li><a href=#httpsgithubcomtensorflowtfjs-modelstreemasterbody-pixspecial-thanks-for-participants-in-the-demo-videospecial-thanks-for-participants-in-the-demo-video><a href=https://github.com/tensorflow/tfjs-models/tree/master/body-pix#special-thanks-for-participants-in-the-demo-video></a>Special thanks for participants in the demo video</a></li></ol></li><li><a href=#httpsgithubcomtensorflowtfjs-modelstreemasterbody-pixtables-of-contentstables-of-contents><a href=https://github.com/tensorflow/tfjs-models/tree/master/body-pix#tables-of-contents></a>Tables of Contents</a></li><li><a href=#httpsgithubcomtensorflowtfjs-modelstreemasterbody-pixinstallationinstallation><a href=https://github.com/tensorflow/tfjs-models/tree/master/body-pix#installation></a>Installation</a></li><li><a href=#httpsgithubcomtensorflowtfjs-modelstreemasterbody-pixusageusage><a href=https://github.com/tensorflow/tfjs-models/tree/master/body-pix#usage></a>Usage</a><ol><li><a href=#httpsgithubcomtensorflowtfjs-modelstreemasterbody-pixloading-a-pre-trained-bodypix-modelloading-a-pre-trained-bodypix-model><a href=https://github.com/tensorflow/tfjs-models/tree/master/body-pix#loading-a-pre-trained-bodypix-model></a>Loading a pre-trained BodyPix Model</a><ol><li><a href=#httpsgithubcomtensorflowtfjs-modelstreemasterbody-pixmobilenet-smaller-faster-less-accuratemobilenet-smaller-faster-less-accurate><a href=https://github.com/tensorflow/tfjs-models/tree/master/body-pix#mobilenet-smaller-faster-less-accurate></a>MobileNet (smaller, faster, less accurate)</a></li><li><a href=#httpsgithubcomtensorflowtfjs-modelstreemasterbody-pixresnet-larger-slower-more-accurate-newresnet-larger-slower-more-accurate-new><a href=https://github.com/tensorflow/tfjs-models/tree/master/body-pix#resnet-larger-slower-more-accurate-new></a>ResNet (larger, slower, more accurate) **new!**</a></li><li><a href=#httpsgithubcomtensorflowtfjs-modelstreemasterbody-pixconfig-params-in-bodypixloadconfig-params-in-bodypixload><a href=https://github.com/tensorflow/tfjs-models/tree/master/body-pix#config-params-in-bodypixload></a>Config params in bodyPix.load()</a></li></ol></li><li><a href=#httpsgithubcomtensorflowtfjs-modelstreemasterbody-pixperson-segmentationperson-segmentation><a href=https://github.com/tensorflow/tfjs-models/tree/master/body-pix#person-segmentation></a>Person segmentation</a><ol><li><a href=#httpsgithubcomtensorflowtfjs-modelstreemasterbody-pixparams-in-estimatemultipersoninstancesegmentationparams-in-estimatemultipersoninstancesegmentation><a href=https://github.com/tensorflow/tfjs-models/tree/master/body-pix#params-in-estimatemultipersoninstancesegmentation></a>Params in estimateMultiPersonInstanceSegmentation()</a></li><li><a href=#httpsgithubcomtensorflowtfjs-modelstreemasterbody-pixreturnsreturns><a href=https://github.com/tensorflow/tfjs-models/tree/master/body-pix#returns></a>Returns</a></li><li><a href=#httpsgithubcomtensorflowtfjs-modelstreemasterbody-pixexample-usageexample-usage><a href=https://github.com/tensorflow/tfjs-models/tree/master/body-pix#example-usage></a>Example Usage</a></li></ol></li><li><a href=#httpsgithubcomtensorflowtfjs-modelstreemasterbody-pixmulti-person-body-part-segmentationmulti-person-body-part-segmentation><a href=https://github.com/tensorflow/tfjs-models/tree/master/body-pix#multi-person-body-part-segmentation></a>Multi-person body part segmentation</a><ol><li><a href=#httpsgithubcomtensorflowtfjs-modelstreemasterbody-pixthe-body-partsthe-body-parts><a href=https://github.com/tensorflow/tfjs-models/tree/master/body-pix#the-body-parts></a>The Body Parts</a></li><li><a href=#httpsgithubcomtensorflowtfjs-modelstreemasterbody-pixparams-in-estimatemultipersoninstancesegmentation-1params-in-estimatemultipersoninstancesegmentation><a href=https://github.com/tensorflow/tfjs-models/tree/master/body-pix#params-in-estimatemultipersoninstancesegmentation-1></a>Params in estimateMultiPersonInstanceSegmentation</a></li><li><a href=#httpsgithubcomtensorflowtfjs-modelstreemasterbody-pixreturns-1returns><a href=https://github.com/tensorflow/tfjs-models/tree/master/body-pix#returns-1></a>Returns</a></li><li><a href=#httpsgithubcomtensorflowtfjs-modelstreemasterbody-pixexample-usage-1example-usage><a href=https://github.com/tensorflow/tfjs-models/tree/master/body-pix#example-usage-1></a>Example Usage</a></li></ol></li><li><a href=#httpsgithubcomtensorflowtfjs-modelstreemasterbody-pixoutput-visualization-utility-functionsoutput-visualization-utility-functions><a href=https://github.com/tensorflow/tfjs-models/tree/master/body-pix#output-visualization-utility-functions></a>Output Visualization Utility Functions</a><ol><li><a href=#httpsgithubcomtensorflowtfjs-modelstreemasterbody-pixtomaskimagedatatomaskimagedata><a href=https://github.com/tensorflow/tfjs-models/tree/master/body-pix#tomaskimagedata></a><code>toMaskImageData</code></a></li><li><a href=#httpsgithubcomtensorflowtfjs-modelstreemasterbody-pixtocoloredpartimagedatatocoloredpartimagedata><a href=https://github.com/tensorflow/tfjs-models/tree/master/body-pix#tocoloredpartimagedata></a><code>toColoredPartImageData</code></a></li><li><a href=#httpsgithubcomtensorflowtfjs-modelstreemasterbody-pixdrawmaskdrawmask><a href=https://github.com/tensorflow/tfjs-models/tree/master/body-pix#drawmask></a><code>drawMask</code></a></li><li><a href=#httpsgithubcomtensorflowtfjs-modelstreemasterbody-pixdrawpixelatedmaskdrawpixelatedmask><a href=https://github.com/tensorflow/tfjs-models/tree/master/body-pix#drawpixelatedmask></a><code>drawPixelatedMask</code></a></li><li><a href=#httpsgithubcomtensorflowtfjs-modelstreemasterbody-pixdrawbokeheffectdrawbokeheffect><a href=https://github.com/tensorflow/tfjs-models/tree/master/body-pix#drawbokeheffect></a><code>drawBokehEffect</code></a></li><li><a href=#httpsgithubcomtensorflowtfjs-modelstreemasterbody-pixblurbodypartblurbodypart><a href=https://github.com/tensorflow/tfjs-models/tree/master/body-pix#blurbodypart></a><code>blurBodyPart</code></a></li></ol></li></ol></li><li><a href=#httpsgithubcomtensorflowtfjs-modelstreemasterbody-pixdeveloping-the-demosdeveloping-the-demos><a href=https://github.com/tensorflow/tfjs-models/tree/master/body-pix#developing-the-demos></a>Developing the Demos</a></li></ol></nav></div></section></aside></div><script src=https://cdn.jsdelivr.net/npm/node-vibrant@3.1.5/dist/vibrant.min.js integrity="sha256-5NovOZc4iwiAWTYIFiIM7DxKUXKWvpVEuMEPLzcm5/g=" crossorigin=anonymous></script><script type=text/javascript src=/ts/main.js defer></script>
<script>(function(){const e=document.createElement("link");e.href="https://fonts.googleapis.com/css2?family=Lato:wght@300;400;700&display=swap",e.type="text/css",e.rel="stylesheet",document.head.appendChild(e)})()</script></body></html>