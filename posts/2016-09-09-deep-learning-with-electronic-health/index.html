<!doctype html><html lang=en-us dir=ltr><head><meta charset=utf-8><meta name=viewport content="width=device-width,initial-scale=1"><meta name=description content="‚öïÔ∏è Introduction
Walk into any machine learning conference and ask people about the applications of ML in healthcare and most will respond with the
canonical example
of using computer vision to diagnose diseases from medical scans (followed by a prolonged
debate
about ‚Äúshould radiologists be worried about their jobs‚Äù). But there exists another source of data, beyond imaging studies, that can change the way we approach health: the electronic health record (EHR)."><title>Deep Learning with Electronic Health Record (EHR) Systems</title><link rel=canonical href=https://Nexus-Security.github.io/posts/2016-09-09-deep-learning-with-electronic-health/><link rel=stylesheet href=/scss/style.min.450926226e724574a6b936335ea06111f8aeb253d932c86cb2cc807341cd2889.css><meta property="og:title" content="Deep Learning with Electronic Health Record (EHR) Systems"><meta property="og:description" content="‚öïÔ∏è Introduction
Walk into any machine learning conference and ask people about the applications of ML in healthcare and most will respond with the
canonical example
of using computer vision to diagnose diseases from medical scans (followed by a prolonged
debate
about ‚Äúshould radiologists be worried about their jobs‚Äù). But there exists another source of data, beyond imaging studies, that can change the way we approach health: the electronic health record (EHR)."><meta property="og:url" content="https://Nexus-Security.github.io/posts/2016-09-09-deep-learning-with-electronic-health/"><meta property="og:site_name" content="ZYChimne"><meta property="og:type" content="article"><meta property="article:section" content="Posts"><meta property="article:published_time" content="2019-09-26T03:52:00+01:00"><meta property="article:modified_time" content="2019-09-26T03:52:00+01:00"><meta name=twitter:title content="Deep Learning with Electronic Health Record (EHR) Systems"><meta name=twitter:description content="‚öïÔ∏è Introduction
Walk into any machine learning conference and ask people about the applications of ML in healthcare and most will respond with the
canonical example
of using computer vision to diagnose diseases from medical scans (followed by a prolonged
debate
about ‚Äúshould radiologists be worried about their jobs‚Äù). But there exists another source of data, beyond imaging studies, that can change the way we approach health: the electronic health record (EHR)."></head><body class=article-page><script>(function(){const e="StackColorScheme";localStorage.getItem(e)||localStorage.setItem(e,"auto")})()</script><script>(function(){const t="StackColorScheme",e=localStorage.getItem(t),n=window.matchMedia("(prefers-color-scheme: dark)").matches===!0;e=="dark"||e==="auto"&&n?document.documentElement.dataset.scheme="dark":document.documentElement.dataset.scheme="light"})()</script><div class="container main-container flex on-phone--column extended"><aside class="sidebar left-sidebar sticky"><button class="hamburger hamburger--spin" type=button id=toggle-menu aria-label="Toggle Menu">
<span class=hamburger-box><span class=hamburger-inner></span></span></button><header><figure class=site-avatar><a href=/><img src=/img/avatar_hu8b78332b6420dc9affabe23720d11e63_1937019_300x0_resize_q75_box.jpg width=300 height=300 class=site-logo loading=lazy alt=Avatar></a>
<span class=emoji>üçá</span></figure><div class=site-meta><h1 class=site-name><a href=/>ZYChimne</a></h1><h2 class=site-description>Computer Science, Wuhan University</h2></div></header><ol class=social-menu><li><a href=https://github.com/ZYChimne target=_blank title=GitHub><svg xmlns="http://www.w3.org/2000/svg" class="icon icon-tabler icon-tabler-brand-github" width="24" height="24" viewBox="0 0 24 24" stroke-width="2" stroke="currentcolor" fill="none" stroke-linecap="round" stroke-linejoin="round"><path stroke="none" d="M0 0h24v24H0z" fill="none"/><path d="M9 19c-4.3 1.4-4.3-2.5-6-3m12 5v-3.5c0-1 .1-1.4-.5-2 2.8-.3 5.5-1.4 5.5-6a4.6 4.6.0 00-1.3-3.2 4.2 4.2.0 00-.1-3.2s-1.1-.3-3.5 1.3a12.3 12.3.0 00-6.2.0C6.5 2.8 5.4 3.1 5.4 3.1a4.2 4.2.0 00-.1 3.2A4.6 4.6.0 004 9.5c0 4.6 2.7 5.7 5.5 6-.6.6-.6 1.2-.5 2V21"/></svg></a></li><li><a href=https://twitter.com/ZChimne target=_blank title=Twitter><svg xmlns="http://www.w3.org/2000/svg" class="icon icon-tabler icon-tabler-brand-twitter" width="24" height="24" viewBox="0 0 24 24" stroke-width="2" stroke="currentcolor" fill="none" stroke-linecap="round" stroke-linejoin="round"><path stroke="none" d="M0 0h24v24H0z" fill="none"/><path d="M22 4.01c-1 .49-1.98.689-3 .99-1.121-1.265-2.783-1.335-4.38-.737S11.977 6.323 12 8v1c-3.245.083-6.135-1.395-8-4 0 0-4.182 7.433 4 11-1.872 1.247-3.739 2.088-6 2 3.308 1.803 6.913 2.423 10.034 1.517 3.58-1.04 6.522-3.723 7.651-7.742a13.84 13.84.0 00.497-3.753C20.18 7.773 21.692 5.25 22 4.009z"/></svg></a></li></ol><ol class=menu id=main-menu><li><a href=/><svg xmlns="http://www.w3.org/2000/svg" class="icon icon-tabler icon-tabler-home" width="24" height="24" viewBox="0 0 24 24" stroke-width="2" stroke="currentcolor" fill="none" stroke-linecap="round" stroke-linejoin="round"><path stroke="none" d="M0 0h24v24H0z"/><polyline points="5 12 3 12 12 3 21 12 19 12"/><path d="M5 12v7a2 2 0 002 2h10a2 2 0 002-2v-7"/><path d="M9 21v-6a2 2 0 012-2h2a2 2 0 012 2v6"/></svg><span>Home</span></a></li><li><a href=/about-me/><svg xmlns="http://www.w3.org/2000/svg" class="icon icon-tabler icon-tabler-user" width="24" height="24" viewBox="0 0 24 24" stroke-width="2" stroke="currentcolor" fill="none" stroke-linecap="round" stroke-linejoin="round"><path stroke="none" d="M0 0h24v24H0z"/><circle cx="12" cy="7" r="4"/><path d="M6 21v-2a4 4 0 014-4h4a4 4 0 014 4v2"/></svg><span>About Me</span></a></li><div class=menu-bottom-section><li id=dark-mode-toggle><svg xmlns="http://www.w3.org/2000/svg" class="icon icon-tabler icon-tabler-toggle-left" width="24" height="24" viewBox="0 0 24 24" stroke-width="2" stroke="currentcolor" fill="none" stroke-linecap="round" stroke-linejoin="round"><path stroke="none" d="M0 0h24v24H0z"/><circle cx="8" cy="12" r="2"/><rect x="2" y="6" width="20" height="12" rx="6"/></svg><svg xmlns="http://www.w3.org/2000/svg" class="icon icon-tabler icon-tabler-toggle-right" width="24" height="24" viewBox="0 0 24 24" stroke-width="2" stroke="currentcolor" fill="none" stroke-linecap="round" stroke-linejoin="round"><path stroke="none" d="M0 0h24v24H0z"/><circle cx="16" cy="12" r="2"/><rect x="2" y="6" width="20" height="12" rx="6"/></svg><span>Dark Mode</span></li></div></ol></aside><main class="main full-width"><article class=main-article><header class=article-header><div class=article-details><div class=article-title-wrapper><h2 class=article-title><a href=/posts/2016-09-09-deep-learning-with-electronic-health/>Deep Learning with Electronic Health Record (EHR) Systems</a></h2></div><footer class=article-time><div><svg xmlns="http://www.w3.org/2000/svg" class="icon icon-tabler icon-tabler-calendar-time" width="56" height="56" viewBox="0 0 24 24" stroke-width="2" stroke="currentcolor" fill="none" stroke-linecap="round" stroke-linejoin="round"><path stroke="none" d="M0 0h24v24H0z"/><path d="M11.795 21H5a2 2 0 01-2-2V7a2 2 0 012-2h12a2 2 0 012 2v4"/><circle cx="18" cy="18" r="4"/><path d="M15 3v4"/><path d="M7 3v4"/><path d="M3 11h16"/><path d="M18 16.496V18l1 1"/></svg><time class=article-time--published>Sep 26, 2019</time></div><div><svg xmlns="http://www.w3.org/2000/svg" class="icon icon-tabler icon-tabler-clock" width="24" height="24" viewBox="0 0 24 24" stroke-width="2" stroke="currentcolor" fill="none" stroke-linecap="round" stroke-linejoin="round"><path stroke="none" d="M0 0h24v24H0z"/><circle cx="12" cy="12" r="9"/><polyline points="12 7 12 12 15 15"/></svg><time class=article-time--reading>22 minute read</time></div></footer></div></header><section class=article-content><p>‚öïÔ∏è Introduction</p><p>Walk into any machine learning conference and ask people about the applications of ML in healthcare and most will respond with the</p><p><a class=link href=https://arxiv.org/abs/1711.05225 target=_blank rel=noopener>canonical example</a></p><p>of using computer vision to diagnose diseases from medical scans (followed by a prolonged</p><p><a class=link href=https://twitter.com/AndrewYNg/status/930938692310482944 target=_blank rel=noopener>debate</a></p><p>about ‚Äúshould radiologists be worried about their jobs‚Äù). But there exists another source of data, beyond imaging studies, that can change the way we approach health: the electronic health record (EHR).</p><p>Data in EHR systems</p><p>EHR systems can have data from a variety of different sources including billing data, patient demographics, medical history, lab results, sensor data, prescriptions, clinical notes, medical images, etc. Hospitals adopt EHR systems to store data for every patient encounter, mainly for billing and insurance-related administrative purposes, but we can leverage these records to capture trends and draw conclusions.</p><p><strong>Note</strong>: Be cautious about using data that was primarily created for insurance purposes. Often, it&rsquo;s not truly reflective of patient&rsquo;s condition but rather encompassing for billing / profit. Luckily, there are clinical reports, like radiology, diagnostic imaging, pathology reports, etc., that are intended for physician use and are more reflective of true patient conditions. Unfortunately, most of this data is not readily available in APIs because it&rsquo;s largely unstructured. This is a ripe space for ML to take raw, unstructured data and produce structured, computable data.</p><p>Types of data in EHR systems. [</p><p><a class=link href=https://goku.me target=_blank rel=noopener>source</a></p><p>]</p><p>Application themes</p><p>While the number of</p><p><a class=link href=https://arxiv.org/abs/1706.03446 target=_blank rel=noopener>potential applications</a></p><p>from leveraging EHRs is bountiful, the current goals are around increasing clinical efficiency by minimizing medical misdiagnosis and augmenting the physician‚Äôs capabilities. There are so many different ways that machine learning is aiding in fulfilling these goals but the main themes of applications are</p><p><strong>representation learning</strong></p><p>,</p><p><strong>information extraction</strong></p><p>and</p><p><strong>clinical predictions</strong></p><p>. We will also cover several emerging themes that are gaining traction.</p><p>üî¨ Representation learning</p><p>If you look inside an EHR system for a particular patient, you‚Äôll find a record for each encounter. Each encounter will have details on the patient such as diagnosis or administered medications as a list of codes (ex.</p><p><a class=link href=https://www.icd10data.com/ICD10CM/Codes/I00-I99/I10-I16/I10-/I10 target=_blank rel=noopener>I10</a></p><p>for primary hypertension). These codes were initially developed for administrative purposes where each one represents a specific diagnosis, medication, procedure, etc. In order to use these codes as inputs into our models, we need a way of representing them.</p><p>Different types of medical codes. [</p><p><a class=link href=https://arxiv.org/abs/1706.03446 target=_blank rel=noopener>source</a></p><p>]</p><p>Traditional approaches involved representing these codes via</p><p><a class=link href=https://chrisalbon.com/images/machine_learning_flashcards/One-Hot_Encoding_print.png target=_blank rel=noopener>one-hot encoding</a></p><p>. This approach failed to capture the meaningful representations between the different codes and also caused a computational dimensionality issue since there over a 100,000 different codes.</p><p>Application themes</p><p>One approach towards meaningful representations is to learn distributed embeddings via techniques like</p><p><a class=link href=https://arxiv.org/abs/1310.4546 target=_blank rel=noopener>skip-gram</a></p><p>. This is commonly employed in natural language processing to learn representations for words in a sentence. The skip-gram technique learns vector representations of words that can predict the neighboring words (context), which in turn captures the relationships between the words. However, unlike sentences, which are an ordered sequence of words, medical codes in a patient encounter do not have an intrinsic order to them. Therefore, it‚Äôs non-trivial to form (target, context) pairs required for the skip-gram technique.</p><p><a class=link href=https://arxiv.org/abs/1310.4546 target=_blank rel=noopener>Choi et al.</a></p><p>approached this issue by defining the (target, context) pairs at the patient encounter level rather than at the medical code level. Unlike a sequence of medical codes, the patient encounters (comprised of medical codes) do have an order to them. By representing each patient encounter with a binary vector for the codes present, we can feed it into a two-layer neural network that will predict the binary vector for neighboring visits.</p><p>Using skip-gram technique to learn distributed embeddings for medical codes. [</p><p><a class=link href=https://arxiv.org/abs/1602.05568 target=_blank rel=noopener>source</a></p><p>]</p><p>Once the embeddings are learned, we can represent the medical codes as inputs into our deep learning models for supervised tasks. But how do we know that the representations we learned are trustworthy?</p><p><strong>Interpretability</strong></p><p>Choi et al. applied a <strong>non-negative constraint</strong> on the code embeddings weight matrix by measuring loss for the skip-gram technique using W&rsquo;c = ReLU(Wc) instead of Wc. This allowed them to inspect every ith embedding dimension and get the top k medical codes in that dimension. These codes should be highly correlated and the clusters should confirm established code groups from knowledge bases.</p><p>Top k codes from ith dimension of the embedding weight matrix. [</p><p><a class=link href=https://arxiv.org/abs/1602.05568 target=_blank rel=noopener>source</a></p><p>]</p><p>There are also several other techniques to learn meaningful representations for the medical codes and which one you choose depends on the data. You can use techniques like</p><p><a class=link href=https://nlp.stanford.edu/pubs/glove.pdf target=_blank rel=noopener>GloVe</a></p><p>,</p><p><a class=link href=https://arxiv.org/abs/1301.3781 target=_blank rel=noopener>CBOW</a></p><p>or</p><p><a class=link href=https://www.nature.com/articles/srep26094 target=_blank rel=noopener>stacked autoencoders</a></p><p>to learn the embeddings. There are even</p><p><a class=link href=https://arxiv.org/abs/1611.07012 target=_blank rel=noopener>advanced</a></p><p>implementations where representations are learned with an attention model based on knowledge ontologies (great for infrequent codes).</p><p><strong>Tips</strong></p><p>You can use embeddings in three different ways:</p><ol><li>Completely skip learning embeddings and train the entire model for a supervised task with a randomly initialized embedding matrix end-to-end (can cause overfitting).</li><li>Freeze the learned embeddings and train the rest of the model.</li><li>Use the learned embeddings and train everything end-to-end.</li></ol><p>Contextualized embeddings</p><p><strong>[Updated 2019]</strong></p><p>There has also been quite a bit of work in 2019 dealing with text representation. The traditional method of one-hot encoded representations evolved into word embeddings, which gave way for efficient representations that accounted for the relationship of the words to each other. You can use biomedical specific</p><p><a class=link href=http://bio.nlplab.org/ target=_blank rel=noopener>word and character level embeddings</a></p><p>, which were trained on large</p><p><a class=link href=https://www.ncbi.nlm.nih.gov/pmc/tools/openftlist/ target=_blank rel=noopener>PubMed datasets</a></p><p>, to represent your text. However, these representations did not account well for context. For example, the world</p><p><em>discharge</em></p><p>would have the same embedding whether it was used in the context of an emergency room discharge or excretions. To address this limitation, researchers leveraged</p><p><a class=link href=https://arxiv.org/abs/1810.04805 target=_blank rel=noopener>BERT</a></p><p>- bidirectional encoder representations from</p><p><a class=link href=https://arxiv.org/abs/1706.03762 target=_blank rel=noopener>Transformers</a></p><p>. These representations are conditionally learned from bidirectionally looking at unlabeled text at all layers. After training, a pre-trained BERT model can be fine-tuned with one or a few output layers to produce contextualized embeddings for a myriad of applications. Researchers have since released publically available</p><p><a class=link href=https://github.com/EmilyAlsentzer/clinicalBERT target=_blank rel=noopener>clinical BERT embeddings</a></p><p>(and</p><p><a class=link href=https://github.com/dmis-lab/biobert target=_blank rel=noopener>BioBERT</a></p><p>) to represent text for downstream applications like named entity recognition (NER), relationship extraction and QA tasks.</p><p><img src=https://practicalai.me/static/img/blog/ehr/bert.png loading=lazy></p><p>Pretraining and finetunning with BERT. [</p><p><a class=link href=https://arxiv.org/abs/1810.04805 target=_blank rel=noopener>source</a></p><p>]</p><p>A small side note on all the recent representation techniques. Our focus shouldn&rsquo;t just be on the performance gain they provide. Especially in the clinical setting, having an accurate and contextualized representation for a concept is crucial. It&rsquo;s not just about overall performance, but also about the model&rsquo;s ability to adapt to the nuances of individual inputs.</p><p>‚öíÔ∏è Information extraction</p><p>EHR systems not only hold patient information and codes but also things like physician‚Äôs notes, ambulance records, admission/discharge details, medication steps, etc. The difficult part is extracting information from the text. Traditional approaches include manual extraction which is costly especially when trained physicians are involved in the process. And you might be wondering why a simple automated lookup won‚Äôt suffice? But lookups don‚Äôt fare well when entities (ie.</p><p><a class=link href=https://arxiv.org/abs/1804.04225 target=_blank rel=noopener>abbreviations</a></p><p>) could mean different things depending on the context and the same entity can be expressed with a diverse vocabulary. There are several useful non-ML techniques, like</p><p><a class=link href=https://www.ncbi.nlm.nih.gov/pmc/articles/PMC5573874/ target=_blank rel=noopener>Valx</a></p><p>for getting lab test names and the corresponding measurement values and the</p><p><a class=link href=https://code.google.com/archive/p/negex/ target=_blank rel=noopener>NegEx</a></p><p>system for negation tagging, to name a few. However, there are several aspects of information extraction that can greatly benefit from deep learning.</p><p>Entity recognition</p><p>Deep learning approaches begin with representing the words in a sentence as a sequence of tokens. Then we can apply an embedding layer, feed it into a bidirectional (to account for surrounding context) gated RNN component and use a softmax on top of that to classify each token‚Äôs entity class.</p><p>Common deep learning approach for information extraction. [</p><p><a class=link href=https://arxiv.org/abs/1606.07953 target=_blank rel=noopener>source</a></p><p>]</p><p>However, this approach requires large datasets with annotated entities. To overcome this limitation,</p><p><a class=link href=https://arxiv.org/abs/1711.07908 target=_blank rel=noopener>Xing et al.</a></p><p>use language modeling (as a transfer learning approach) to aid in biomedical named entity recognition (NER). First, they use bidirectional language modeling (on</p><p><a class=link href=https://www.ncbi.nlm.nih.gov/pmc/tools/openftlist/ target=_blank rel=noopener>PubMed abstracts</a></p><p>) as a transfer learning approach to pretrain the NER model‚Äôs weights. They believed that this auxiliary task will prevent overfitting and improve convergence speed (helped F1 score about ~2% compared to baseline of randomly initialized weights on four benchmark datasets) on the main supervised task while using less data. They then used the pre-trained embeddings and LSTM weights for the supervised NER task and trained it end-to-end.</p><p>NER model architecture using pre-trained weights from language modeling. [</p><p><a class=link href=https://arxiv.org/abs/1711.07908 target=_blank rel=noopener>source</a></p><p>]</p><p>So now that we know how to extract entities and also represent them with meaningful representations, let‚Äôs see how we can leverage everything for a supervised task.</p><p>üë©‚Äç‚öïÔ∏è Clinical predictions</p><p>Recall that EHR data for a patient is a sequence of encounters composed of medical codes, clinical notes, etc. Now that we have a way of meaningfully representing the inputs, we can leverage them for supervised tasks like predicting clinical outcomes. We‚Äôll look at different use cases where different types of data are used to make predictions.</p><p>Codes and values</p><p>A typical patient encounter record in an EHR system will include a collection of medical codes, patient demographics, lab values etc. We can use this data as inputs to our model to predict an outcome like likelihood of a disease. There are two different ways of using this data to make predictions. The simple approach is to use a set of inputs to predict a static outcome like probability of heart disease.</p><p><a class=link href=https://arxiv.org/abs/1602.03686 target=_blank rel=noopener>Choi et al.</a></p><p>concatenated learned ICD code embeddings for a particular patient encounter to create a patient representation. They used this representation as the sole input into a model to predict the probability of heart failure.</p><p>The more involved approach is to process a sequence of inputs to make a prediction. Predictions could be made after each individual input or at the end of the entire sequence. Choi et al. developed</p><p><a class=link href=https://arxiv.org/abs/1511.05942 target=_blank rel=noopener>Doctor AI</a></p><p>, which uses ICD codes from one visit and the duration since the last visit to predict the next visit‚Äôs expected ICD codes and duration. They embed the input ICD codes (using embeddings learned from skip-gram) and concatenate the duration since last visit to feed it into a gated recurrent component. A softmax layer then uses the output to predict the subsequent visit‚Äôs diagnosis codes and time until the next visit.</p><p>Doctor AI‚Äôs architecture for predicting the subsequent visit ICD codes. [</p><p><a class=link href=https://arxiv.org/abs/1511.05942 target=_blank rel=noopener>source</a></p><p>]</p><p>Using this data, either as a single set of inputs or as a sequence of inputs, is fairly straight forward. However, there are plenty of other types of data that can add a lot of signal towards making predictions.</p><p>Clinical notes</p><p>So far we‚Äôve seen examples of how structured data is used in making predictions. But there‚Äôs plenty of unstructured data that holds a lot of valuable information: physician notes, medical/procedure instructions, etc.</p><p><a class=link href=https://arxiv.org/abs/1808.04928 target=_blank rel=noopener>Liu el al.</a></p><p>explored using both CNN and RNN based structures for processing unstructured clinical notes to predict the onset of diseases within a prediction window.</p><p><strong>Tips</strong></p><ol><li>The prediction window based approach is very common for clinical prediction tasks. One tip that the authors provided was to leave a gap between the history window (where input data is collected) and the prediction window (where you see if the onset of the disease occurred or not). For example, they used a 12-month historical window, a 3-month gap and a 6-month prediction window in their study. This is done to prevent the model from ‚Äúcheating‚Äù with information that‚Äôs generated just prior to the diagnosis time. We want to catch the onset of disease earlier on so our training data needs to reflect that requirement.</li><li>The same patient‚Äôs data never appears in two separate datasets (train/val/test). Refer to this <a class=link href=http://www.fast.ai/2017/11/13/validation-sets/ target=_blank rel=noopener>blog post</a> for more details on constructing a proper validation/test set.</li></ol><p>They first applied skip-gram to learn embeddings on an auxiliary dataset (abstracts from medical journals). They applied these embeddings on the input tokens and then used a CNN to apply 1D convolutions with various kernel sizes. It‚Äôs not enough just to use the notes to make clinical predictions, so they concatenated the max-pooled values with structured numerical data (demographics, lab values, etc.) to feed into FC layers for prediction. CNNs are a great option here because they can be applied to both char-level (for understanding abbreviations based on context) and word-level embeddings to find meaningful sub-structures with varying kernel sizes.</p><p>The authors also looked at using LSTMs for processing the word level embeddings. Here the embedded words are sequentially processed by a BiLSTM and then go through a max-pooling operation before being concatenated with the structured numerical data to be fed into FC layers for prediction. Though these recurrent structures are great for processing sequential data, they have a tough time preserving the gradient across 1000s of words. As a result, the authors processed the input tokens with CNNs and fed the max-pooled output into an RNN, which significantly reduced the sequence size that needed to be processed.</p><p>Combined CNN-RNN architecture to process clinical tokens. [</p><p><a class=link href=https://arxiv.org/abs/1808.04928 target=_blank rel=noopener>source</a></p><p>]</p><p><strong>Interpretability</strong></p><p>The authors wanted to know the influence of words or phrases towards the model‚Äôs prediction. They first tried a gradient based approach by measuring the gradient of the prediction with respect to each word‚Äôs embedding and calculate the norm. This approach resulted in very noisy results and not much interpretability.</p><p>Noisy results from the gradient based approach. [</p><p><a class=link href=https://arxiv.org/abs/1808.04928 target=_blank rel=noopener>source</a></p><p>]</p><p>Next, they tried a log-odds based approach where they looked at which n-grams affects the prediction. By seeing which n-grams activate neurons in the max-pooling or FC layer, we can find the most influential n-grams for the prediction that was made. This approach resulted in much more interpretable results compared to the gradient based approach.</p><p>Interpretable results from the log-odds based approach. [</p><p><a class=link href=https://arxiv.org/abs/1808.04928 target=_blank rel=noopener>source</a></p><p>]</p><p>Time-series data</p><p>One type of data that is increasing in size and has tremendous predictive value is time-series data. This type of data can come from sensors placed on medical devices, smartphones, etc. and they have the advantage of being continuously collected prior, during and after an event of interest occurs. Traditional methods for analyzing time-series data involved manual signal processing and using specific filters to extract features. Since the advent of deep learning, specifically convolutional neural networks, this manual preprocessing step is no longer required for meaningful feature engineering.</p><p><a class=link href=https://arxiv.org/abs/1807.10707 target=_blank rel=noopener>Gotlibovych et al.</a></p><p>(@</p><p><a class=link href=https://twitter.com/jawbone target=_blank rel=noopener>Jawbone</a></p><p>) used time series data from EHR to detect atrial fibrillation (Afib is an irregular, rapid heartbeat that can increase your risk of stroke, heart failure, etc.) using raw PPG signals (a signal derived from using light to get the volumetric measurement of an organ). They used a convolutional-recurrent architecture to process the time-series inputs, which were a sequence of samples collected at regular time intervals. The inputs from a receptive field of fixed length are initially processed by a CNN. The CNN acts as digital signal filters that can extract useful signals from the raw time series data. The output from the CNN goes through a max-pool operation (for downsampling) which is then fed into an LSTM to account for previously processed signals. Finally, an FC layer with a sigmoid activation is used to determine the probability of Afib for a particular receptive field.</p><p>Convolutional-recurrent architecture to predict probability of Afib. [</p><p><a class=link href=https://arxiv.org/abs/1807.10707 target=_blank rel=noopener>source</a></p><p>]</p><p>Medical scans</p><p>Even though we slightly undermined image processing at the beginning of this post, there‚Äôs no denying that medical scans hold some of the most valuable clinical information. X-rays, CT, MRI and many other types of scans all require the expertise of a radiologist to accurately process the information. But after deep learning improved upon existing computer vision techniques, models were able to perform specific parts of a radiologist‚Äôs job really well. We‚Äôre not going to be looking at just how much of the expertise can be mapped with machine learning models but instead we‚Äôre going to focus on things to be wary of. Typically, a complex pre-trained CNN-based</p><p><a class=link href=https://arxiv.org/abs/1602.07261 target=_blank rel=noopener>architecture</a></p><p>is used to process the medical scans for diagnosis classification, tumor segmentation, etc. Great performance is achieved through a combination of complex models and large annotated datasets. But sometimes, your model may be performing really well by incorrectly focusing on confounding features (extraneous influencers in the data that aren‚Äôt accounted for).</p><p><a class=link href=https://arxiv.org/abs/1807.00431 target=_blank rel=noopener>Zech et al.</a></p><p>found that x-ray stickers, acting as confounding features, unintentionally influenced the classifications. They were using CNNs to process X-ray images to predict probability of pneumonia but found the confounding variables during the interpretability study. They found that the X-ray sticker on the scan was strongly correlated with where the x-ray was taken (poor region, wealthy region, etc.) which was strongly correlated with disease prevalence levels.</p><p><strong>Interpretability</strong></p><p>A great interpretability method when working with images is to use maximum activation. We can use activation maps to understand which regions of the input image were most influential towards the prediction. You‚Äôll have to apply some normalization to highlight the most influential regions and get vivid results like below.</p><p>Using activation maps to capture confounding variables. [</p><p><a class=link href=https://arxiv.org/abs/1807.00431 target=_blank rel=noopener>source</a></p><p>]</p><p>The interpretability study revealed that the model was using the stickers as the most influential variable for making its prediction. Many people wonder why this is a problem but this type of prediction will create false positives in the poor regions and false negatives in the other regions. Confounding variables can also assume</p><p><a class=link href=https://arxiv.org/abs/1705.08821 target=_blank rel=noopener>other forms</a></p><p>like structured numerical variables (ie. socio-economic status, etc.), so it‚Äôs very important to use domain expertise and interpretability measures to capture them.</p><p>These are the four major types of data in EHR systems and a few of the common ways of handling them. A sound approach towards a clinical prediction task may involve using all of these different types of data together and you may have to come up with your own clever architectures to process them. But besides the work we‚Äôve looked at so far, you can also draw inspiration from emerging themes.</p><p>üì£ Emerging themes</p><p>Emerging themes don‚Äôt warrant their own sections just yet but they are noteworthy because they are quickly gaining traction in the research community. We will look at these topics really quickly but you can refer to the individual papers for more information.</p><p>Relationship extraction</p><p>Relation extraction is a subset of information extraction but there‚Äôs been quite a bit of new work on extracting new relationships that expand on existing knowledge bases. Clinical notes are filled with explicit relationships like Disease A causes symptoms B or Medicine X causes symptom Y. Lv et al. applied sparse autoencoders with a conditional random field (CRF) classifier to extract these explicit relationships with remarkable results. However, Zhang et al. took it one step further by extracting novel relationships via generative discovery. They use a conditional variational autoencoder (CVAE) to learn the latent space conditioned on the relationship type. After training, they can use density-based sampling to generate two entities based on an input relationship type, allowing them to find novel entity relationship pairs that expand existing knowledge bases.</p><p>Using a CVAE to conditionally generate entity relationship pairs. [</p><p><a class=link href=https://www.kdd.org/kdd2018/accepted-papers/view/on-the-generative-discovery-of-structured-medical-knowledge target=_blank rel=noopener>source</a></p><p>]</p><p>Generative sampling</p><p>One of the issues with EHR data is the scarcity of data for particular diseases, procedures, etc. To tackle this issue, GANs are used to learn from patient records and generate samples to augment the training dataset. A GAN is composed of a generator and a discriminator (both are deep neural networks). The generator will try its best to make a sample by learning from a dataset and the discriminator will learn to predict if a given sample is generated by the generator or if it is from the original training dataset.</p><p><a class=link href=https://arxiv.org/abs/1709.01648v1 target=_blank rel=noopener>Che et al.</a></p><p>use GANs to augment their training dataset but recall that unlike VAEs, GANs generate samples based on the input and random noise. To address this limitation, the authors tweaked the generator with variation contrastive diverge in order to be able to generate samples that align with the same class as the input. With this tweak, the generated samples belong to a particular class and can be used to augment the training dataset.</p><p><img src=https://practicalai.me/static/img/blog/ehr/gan.png loading=lazy></p><p>GAN with a tweaked generator to generate samples conditioned on a class. [</p><p><a class=link href=https://arxiv.org/abs/1709.01648v1 target=_blank rel=noopener>source</a></p><p>]</p><p>Sometimes, however, GANs produce obvious outliers such as records with both male and female specific health codes. To eliminate these types of poorly simulated cases,</p><p><a class=link href=https://arxiv.org/abs/1804.08033 target=_blank rel=noopener>Ravuri et al.</a></p><p>(@</p><p><a class=link href=https://curai.com/ target=_blank rel=noopener>CurAI</a></p><p>) found a way to combine expert knowledge with EHR data to create simulated data for training. From EHR data, they generate medical cases with findings and diagnosis based on frequencies and likelihood from an established knowledge base. Generative sampling via semi-supervised learning is gaining traction because of the large data requirement for deep learning but the focus will be on incorporating existing EHR data and medical expertise.</p><p>Multitask learning</p><p>Multitask learning (MTL) has been shown to help with supervised tasks across many different domains, including</p><p><a class=link href=https://arxiv.org/abs/1704.05742 target=_blank rel=noopener>natural language processing</a></p><p>. The idea is to have your model predict for both the primary and auxiliary tasks. The auxiliary task is highly related to the primary task and the idea is that the model will learn things from the auxiliary task that will be useful for the primary task.</p><p><a class=link href=https://arxiv.org/abs/1808.03331v1 target=_blank rel=noopener>Ding et al.</a></p><p>have shown that MTL is both helpful and detrimental depending on your phenotype distribution.</p><p>Multitask learning architecture for phenotyping. [</p><p><a class=link href=https://arxiv.org/abs/1808.03331v1 target=_blank rel=noopener>source</a></p><p>]</p><p>They found that MTL is helpful for rare phenotypes but harmful for common phenotypes. The magnitude of benefit or harm increases as we add more auxiliary tasks. This is one of the very few examples of MTL in the clinical setting that I have found so there‚Äôs plenty of room for exploration and improvement here.</p><p>Recommendation systems</p><p>Recommendation systems are a great medium for delivering personalized interventions. However, an issue is that the outcome we are optimizing for is a delayed, long-term one.</p><p><a class=link href=https://arxiv.org/abs/1807.09387v1 target=_blank rel=noopener>Mann et al.</a></p><p>address this issue by factoring in intermediate signals. They use both the input state and the intermediate signals to predict the target y. They found that using intermediate signals, as opposed to just the initial state, significantly helped with performance on two recommendation based tasks.</p><p>Recommendation system that factors in intermediate signals. [</p><p><a class=link href=https://arxiv.org/abs/1807.09387v1 target=_blank rel=noopener>source</a></p><p>]</p><p>Both the input x and the intermediate signal z are used to predict y but backpropagation is only for the input channel. The trickiest aspect of this implementation is devising what the intermediate signals will be. We need to pick intermediate signals that are general enough that we see them from case to case (so we can use the model on new cases) but also specific enough for each case that they add meaningful value for the long term goal.</p><p>Counterfactual reasoning</p><p>One of the most interesting and necessary emerging ML health topics is counterfactual reasoning. All the supervised predictive modeling we‚Äôve seen so far involves predicting outcomes based on a policy. We collect data from a window of time and then use that to predict the outcome at a later point in time. But patients can receive different treatments in between which can have an effect on the prediction. When the policy changes (ie. medications of varying quantities are administered at irregular times), our supervised models don‚Äôt generalize well.</p><p><a class=link href=https://arxiv.org/abs/1703.10651 target=_blank rel=noopener>Schulam et al.</a></p><p>used counterfactual gaussian processes (CGP) to measure outcomes that are insensitive to the action policies in our training data. CGPs can then map the trajectory of the outcome if an action a is taken from a defined set of actions. This allows us to ask the ‚Äúwhat if‚Äù question which is useful for tasks like evaluating risk where we want to know how the patient will do without any treatment, or with two doses of medication X, etc.</p><p><img src=https://practicalai.me/static/img/blog/ehr/cgp.png loading=lazy></p><p>CGPs aid in mapping the trajectory of outcomes based on an action. [</p><p><a class=link href=https://arxiv.org/abs/1703.10651 target=_blank rel=noopener>source</a></p><p>]</p><p>Using CGP allows us to define the causal effect of an action since we know what would‚Äôve happened had it not occured. This type of reasoning is highly interpretable and offers great value to physicians.</p><p>üè• Industry applications</p><p>We are starting to see a massive</p><p><a class=link href=https://blog.ycombinator.com/there-are-now-141-bio-companies-funded-by-yc/ target=_blank rel=noopener>increase</a></p><p>in bio/health companies and many of them are even starting to leverage machine learning</p><p><em>properly</em></p><p>. But there are a few things to think about before machine learning is widely accepted in healthcare. As we‚Äôve seen so far, deep learning methods have offered amazing results for clinical predictions but the lack of interpretability makes them brittle and untrustworthy. The deep learning applications that are having small success are the ones that are augmenting physician‚Äôs existing capabilities instead of trying to replace them. For example, using information extraction to transfer raw, unstructured notes into structured, computable schema or offering a ranked list of diagnosis from a patient‚Äôs symptoms. All of these augmenting features provide extra, relevant information and allow the medical expert to retain complete decision making power. Applications that follow this theme of influence are the ones that are going to be widely adopted.</p><p>Approach</p><p><strong>[Updated 2019]</strong> With all the ML progress in healthcare, we need to be intentional about following a proper approach so we can separate hype from reality and, more importantly, safely transition research to the clinical setting.</p><p>Sobering tweet from Professor Saria on the need to separate hype from reality. [</p><p><a class=link href=https://twitter.com/suchisaria/status/1176932346324508673 target=_blank rel=noopener>source</a></p><p>]</p><p>When designing your deep learning models for clinical applications, there are many things to consider. One of the best papers I&rsquo;ve seen these last few years is from the Google health group on how they used</p><p><a class=link href=https://ai.googleblog.com/2019/09/using-deep-learning-to-inform.html target=_blank rel=noopener>multi-modal learning for diagnosis of skin disease</a></p><p>. You can read the specific paper</p><p><a class=link href=https://arxiv.org/abs/1909.05382 target=_blank rel=noopener>here</a></p><p>but let&rsquo;s breakdown the generalizable approach they took.</p><ol><li><p><strong>Really understand the problem and how it&rsquo;s currently addressed.</strong><br>Don&rsquo;t try to predict something just because it&rsquo;s cool or possible. This type of goal is acceptable for the purpose of publishing, but when you want to create products to help people, especially in the clinical setting, you need to consider the utility it will bring. In this paper, the researchers use patient metadata and images of their skin to predict specific skin condition. They could&rsquo;ve resorted to just identifying the most likely disorder but they took the time to understand that physicians create a differential diagnosis (list of potential disorders) which they then use to conduct more tests to identify the exact condition. The objective function was carefully crafted while keeping this in mind. The more of a decision the algorithm makes, the more interpretability it needs to offer.</p><ul><li><p><strong>Don&rsquo;t restrict yourself to how things are currently done.</strong><br>This sounds like the opposite advice of #1 but now we&rsquo;re talking about the modeling phase. Think outside the box on what information to leverage to answer your ultimate question. In this paper, dermatologist typically use the skin condition itself to create their differential diagnosis and it&rsquo;s hard for them to keep track of the other features (ex. patient&rsquo;s history, family history, etc.) even when it&rsquo;s all on file. However, with a deep learning model, we can learn to leverage all of these multimodal features to gather signals from all the available input features.</p><p>Proper approach using data, machine learning and a reference standard. [</p><p><a class=link href=https://arxiv.org/abs/1909.05382 target=_blank rel=noopener>source</a></p><p>]</p></li></ul></li><li><p><strong>Be intentional and thorough when creating your ground truths.</strong><br>One of the advantages/disadvantages of machine learning is that your model will learn to fit to your ground truth values. You must spend the time to design the proper values to optimize and be deliberate in your method for collecting them. In this paper, due to the variability in diagnosis even among professional dermatologists, the researchers aggregated ground truth labels from a group of 40 certified dermatologists. This allows our models to leverage the knowledge from an entire team of physicians to augment any one physician&rsquo;s decision.</p></li></ol><p>Even with an approach like this there are still possibilities for a tool with such high utility to fail in the clinical setting. Many groups are starting to develop the proper testing and evaluation frameworks for ML tools but we still have quite a ways to go. I look forward to sharing the progress next year.</p><p>from Hacker News <a class=link href=https://ift.tt/2mLytaN target=_blank rel=noopener>https://ift.tt/2mLytaN</a></p></section><footer class=article-footer><section class=article-copyright><svg xmlns="http://www.w3.org/2000/svg" class="icon icon-tabler icon-tabler-copyright" width="24" height="24" viewBox="0 0 24 24" stroke-width="2" stroke="currentcolor" fill="none" stroke-linecap="round" stroke-linejoin="round"><path stroke="none" d="M0 0h24v24H0z"/><circle cx="12" cy="12" r="9"/><path d="M14.5 9a3.5 4 0 100 6"/></svg><span>Licensed under CC BY-NC-SA 4.0</span></section></footer></article><footer class=site-footer><section class=copyright>&copy;
2020 -
2022 ZYChimne</section><section class=powerby>Built with <a href=https://gohugo.io/ target=_blank rel=noopener>Hugo</a><br>Theme <b><a href=https://github.com/CaiJimmy/hugo-theme-stack target=_blank rel=noopener data-version=3.11.0>Stack</a></b> designed by <a href=https://jimmycai.com target=_blank rel=noopener>Jimmy</a></section></footer><div class=pswp tabindex=-1 role=dialog aria-hidden=true><div class=pswp__bg></div><div class=pswp__scroll-wrap><div class=pswp__container><div class=pswp__item></div><div class=pswp__item></div><div class=pswp__item></div></div><div class="pswp__ui pswp__ui--hidden"><div class=pswp__top-bar><div class=pswp__counter></div><button class="pswp__button pswp__button--close" title="Close (Esc)"></button>
<button class="pswp__button pswp__button--share" title=Share></button>
<button class="pswp__button pswp__button--fs" title="Toggle fullscreen"></button>
<button class="pswp__button pswp__button--zoom" title="Zoom in/out"></button><div class=pswp__preloader><div class=pswp__preloader__icn><div class=pswp__preloader__cut><div class=pswp__preloader__donut></div></div></div></div></div><div class="pswp__share-modal pswp__share-modal--hidden pswp__single-tap"><div class=pswp__share-tooltip></div></div><button class="pswp__button pswp__button--arrow--left" title="Previous (arrow left)"></button>
<button class="pswp__button pswp__button--arrow--right" title="Next (arrow right)"></button><div class=pswp__caption><div class=pswp__caption__center></div></div></div></div></div><script src=https://cdn.jsdelivr.net/npm/photoswipe@4.1.3/dist/photoswipe.min.js integrity="sha256-ePwmChbbvXbsO02lbM3HoHbSHTHFAeChekF1xKJdleo=" crossorigin=anonymous defer></script><script src=https://cdn.jsdelivr.net/npm/photoswipe@4.1.3/dist/photoswipe-ui-default.min.js integrity="sha256-UKkzOn/w1mBxRmLLGrSeyB4e1xbrp4xylgAWb3M42pU=" crossorigin=anonymous defer></script><link rel=stylesheet href=https://cdn.jsdelivr.net/npm/photoswipe@4.1.3/dist/default-skin/default-skin.css integrity="sha256-c0uckgykQ9v5k+IqViZOZKc47Jn7KQil4/MP3ySA3F8=" crossorigin=anonymous><link rel=stylesheet href=https://cdn.jsdelivr.net/npm/photoswipe@4.1.3/dist/photoswipe.css integrity="sha256-SBLU4vv6CA6lHsZ1XyTdhyjJxCjPif/TRkjnsyGAGnE=" crossorigin=anonymous></main></div><script src=https://cdn.jsdelivr.net/npm/node-vibrant@3.1.5/dist/vibrant.min.js integrity="sha256-5NovOZc4iwiAWTYIFiIM7DxKUXKWvpVEuMEPLzcm5/g=" crossorigin=anonymous></script><script type=text/javascript src=/ts/main.js defer></script>
<script>(function(){const e=document.createElement("link");e.href="https://fonts.googleapis.com/css2?family=Lato:wght@300;400;700&display=swap",e.type="text/css",e.rel="stylesheet",document.head.appendChild(e)})()</script></body></html>